{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å‚æ•°é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/anomalib/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"Fastflow\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "\n",
    "normal_ouput_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) çš„tensorç±»å‹\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # æå– B é€šé“ï¼ˆå¼ é‡çš„ç¬¬ä¸‰ä¸ªé€šé“ï¼Œç´¢å¼•ä¸º2ï¼‰\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # ä¸èƒ½æ·»åŠ \n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1ä¹‹é—´\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),                                               # å¦‚æœresizeHWä¸ä¸€è‡´ï¼Œä¼šå¼•èµ·fastflowæŠ¥layernormé”™è¯¯\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # æ— seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx ä¸æ”¯æŒ grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # è®°å½•å¼€å§‹æ—¶é—´\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # è¯»å–å›¾åƒ\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # å›¾åƒtransform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # å›¾åƒæ¨ç†\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        predictions = inferencer.predict(image=tmp)        #! æ³¨ï¼šå¦‚æœä½¿ç”¨vinoï¼Œè¾“å…¥çš„imageå‚æ•°å¦‚æœä¸æ˜¯pathï¼Œé‚£ä¹ˆå…¶shapeåªèƒ½æ˜¯HWC\n",
    "        #predictions = torch_inferencer.predict(image=transform_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! ğŸ¯ inferencer.predictæ¥å—åŸå§‹å›¾åƒï¼Œ\n",
    "        #! å†…éƒ¨é€šè¿‡metadataå’Œmodelåœ¨forwardå‡½æ•°(épre_processå‡½æ•°)ä¸­è°ƒç”¨æ ‡å‡†åŒ–çš„transformå¯¹å›¾åƒå¤„ç†;\n",
    "        #! è§/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! æ³¨: 1. æ¨¡å‹å¯¼å‡ºæ—¶ï¼Œç”Ÿæˆçš„bin/onnxã€jsonç­‰æ–‡ä»¶ä¸­å‡ä¸åŒ…å«è®­ç»ƒæ—¶ä½¿ç”¨åˆ°çš„transformæ“ä½œï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºç›¸å…³çš„æ“ä½œå’Œæ ‡å‡†åŒ–ç›¸å…³çš„æ“ä½œ\n",
    "        #! æ³¨: 2. è€Œpredictæ¨ç†æ—¶ï¼Œä¼šè¿›è¡Œçš„é¢„å¤„ç†æ“ä½œæ˜¯æ ‡å‡†åŒ–æ“ä½œï¼Œæ¯”å¦‚normalizeï¼Œä½†æ˜¯è¿™é‡Œçš„normalizeæ˜¯é™¤ä»¥255çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯æ ‡å‡†æ­£å¤ªåˆ†å¸ƒã€‚\n",
    "        #! é€šè¿‡ä»¥ä¸Šæ€»ç»“ï¼Œå¯çŸ¥ï¼Œæˆ‘ä»¬éœ€è¦1. ä¿®æ”¹è®­ç»ƒæ—¶çš„normalizeï¼›2. å°†ExtractBChannelæ“ä½œè¦å‡ºç°åœ¨è®­ç»ƒæ—¶çš„transformä»¥å¤–ï¼Œè¿˜éœ€è¦å°è£…æˆä¸€ä¸ªæ•°æ®é¢„å¤„ç†æ“ä½œã€‚åœ¨æ¨¡å‹æ¨ç†ä¹‹å‰å¯¹å›¾åƒè¿›è¡Œé¢å¤–çš„é¢„å¤„ç†æ“ä½œã€‚\n",
    "        \n",
    "        # è®°å½•ç»“æŸæ—¶é—´\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # è®¡ç®—è€—æ—¶\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        #print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢çª—å£\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # åŸå§‹å›¾åƒ\n",
    "        axs[0].imshow(image)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # çƒ­å›¾\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! çƒ­åŠ›å›¾æ˜¯anomaly_mapä¸åŸå§‹å›¾åƒçš„åŠ æƒç»“åˆ\n",
    "        axs[3].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # å…³é—­åæ ‡è½´ \n",
    "        \n",
    "\n",
    "        # æ·»åŠ æ–‡æœ¬ä¿¡æ¯åˆ°å›¾å½¢çš„ä¸Šæ–¹ä¸­é—´ä½ç½®\n",
    "        fig_text_x = 0.1   # xåæ ‡åœ¨å›¾å½¢å®½åº¦çš„ä¸­å¿ƒä½ç½®\n",
    "        fig_text_y = 0.95  # yåæ ‡ç¨å¾®é è¿‘å›¾å½¢çš„é¡¶éƒ¨ï¼Œé¿å…ä¸å­å›¾é‡å \n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n'\n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # æ˜¾ç¤ºæ•´ä¸ªå›¾å½¢\n",
    "        plt.tight_layout()  # è°ƒæ•´å­å›¾é—´çš„é—´è·\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom æ•°æ®é›†é…ç½®\n",
    "\n",
    "Folderçš„ç†è§£:\n",
    "\n",
    "1. è®­ç»ƒé›†ä¸­å…¨éƒ¨æ˜¯æ­£å¸¸æ ·æœ¬ï¼›\n",
    "2. æ­£å¸¸æ ·æœ¬é›†ä¸­çš„éƒ¨åˆ†(normal_split_ratio)æ ·æœ¬è¢«å‡ç­‰(val_split_ratio)æ”¾å…¥åˆ°éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­ã€‚\n",
    "3. test_split_ratioä¼¼ä¹å’Œnormal_split_ratioä¸€æ ·ã€‚\n",
    "4. ğŸ¯ éœ€è¦ç¡®ä¿è®­ç»ƒé›†ä¸å­˜åœ¨æ­£å¸¸æ ·æœ¬ï¼›æµ‹è¯•é›†ä¸å­˜åœ¨æ ·æœ¬ï¼›éªŒè¯é›†æ˜¯å…¨éƒ¨æ­£æ ·æœ¬å’Œå…¨éƒ¨è´Ÿæ ·æœ¬ã€‚\n",
    "\n",
    "æ¯”å¦‚ï¼šval_split_ratio=0.5æ—¶ï¼š82 | 27, 10 | 27, 10|; val_split_ratio=0.1æ—¶ï¼š82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "WARNING:anomalib.data.utils.split:Zero subset length encountered during splitting. This means one of your subsets\n",
      "            might be empty or devoid of either normal or anomalous images.\n"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal_filtered\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.04,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleB.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleC.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> åˆ†æè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„æ•°æ®é‡ä»¥åŠç±»åˆ«åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # ç»Ÿè®¡dataloaderä¸­æ ·æœ¬çš„ç±»åˆ«æ¯”ä¾‹\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"labelé¢‘ç‡åˆ†å¸ƒï¼š{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([16, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({0: 100})\n",
      "image_paths(100)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/13__DA1479053.png', '/local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png', '/local_data/datasets/3-5-jing/normal/5__DA2951215.png', '/local_data/datasets/3-5-jing/normal/55__DA2951215.png', '/local_data/datasets/3-5-jing/normal/71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({0: 97, 1: 52})\n",
      "image_paths(149)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n"
     ]
    }
   ],
   "source": [
    "# Val images\n",
    "i, data = next(enumerate(val_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({1: 28, 0: 2})\n",
      "image_paths(30)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal_filtered/11__DA1479053 (3).png', '/local_data/datasets/3-5-jing/abnormal_filtered/11__DA1479053.png', '/local_data/datasets/3-5-jing/abnormal_filtered/11__DA2951175 (3).png', '/local_data/datasets/3-5-jing/abnormal_filtered/13__DA2951215 (2).png', '/local_data/datasets/3-5-jing/abnormal_filtered/13__DA2951215 (3).png']\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "i, data = next(enumerate(test_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> æŸ¥çœ‹å›¾åƒå†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å‰çš„æ•°æ® : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.9362553358078003, 0.06547410041093826;\n",
      "transform.2: -1.804444432258606, 2.517995834350586, -1.5134505033493042;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"å¤„ç†å‰çš„æ•°æ® : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ–å™¨é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing Fastflow model.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:anomalib.models.components.base.anomaly_module:No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | loss                  | FastflowLoss             | 0      | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "7 | model                 | FastflowModel            | 7.7 M  | train\n",
      "---------------------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.678    Total estimated model params size (MB)\n",
      "261       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601c217d049d4da2baa9c2d21a5f0f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7807119c514dea843e22d4e5f182d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:anomalib.callbacks.timer:Training took 43.99 seconds\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd633d91451c4739a1d58ea1b5985610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.callbacks.timer:Testing took 69.64969515800476 seconds\n",
      "Throughput (batch_size=8) : 2.1392771305313545 FPS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     0.981760561466217     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.9444444179534912     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    0.981760561466217    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.9444444179534912    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/projects/results/Fastflow/latest\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    model_checkpoint,\n",
    "    early_stopping,\n",
    "    graph_logger,\n",
    "]\n",
    "\n",
    "\n",
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, \n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    pixel_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    # callbacks= callbacks\n",
    "                    )\n",
    "    # engine.train(model=model, \n",
    "    #              train_dataloaders=train_loader_lst, \n",
    "    #              val_dataloaders=val_loader_lst, \n",
    "    #              test_dataloaders=test_loader_lst)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=test_loader)\n",
    "elif configs[\"model_name\"] == \"Fastflow\":\n",
    "    model = Fastflow()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"auto\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    max_epochs=1,                            #! å¸Œæœ›èµ‹å€¼ç»™Lightning Trainerçš„å‚æ•°å¿…é¡»å…¨éƒ¨æ”¾åœ¨å·²æ ‡æ˜å‚æ•°çš„æœ€åé¢\n",
    "                    )\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "    \n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹å¯¼å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/Fastflow/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "# model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# openvino_model_path = model_output_path / \"weights\" / \"openvino\" / \"model.bin\"\n",
    "# metadata_path = model_output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n",
    "# print(openvino_model_path.exists(), metadata_path.exists())\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.15258617148454573 LabelName.NORMAL\n",
      "Prediction took 1.1802 seconds.\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.13462447928461213 LabelName.NORMAL\n",
      "Prediction took 1.0889 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [2.865513e-08..1.0000001].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.4438535835224849 LabelName.NORMAL\n",
      "Prediction took 0.9160 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.574201e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.1877837405138954 LabelName.NORMAL\n",
      "Prediction took 1.1105 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4916407e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.27357514890748513 LabelName.NORMAL\n",
      "Prediction took 0.9661 seconds.\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.9015370254314499 LabelName.ABNORMAL\n",
      "Prediction took 1.0223 seconds.\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.0571 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-5.6794747e-10..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.8170319617869952 LabelName.ABNORMAL\n",
      "Prediction took 1.0010 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.673895e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.8186749472043311 LabelName.ABNORMAL\n",
      "Prediction took 1.2177 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.16941745e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.8667581616047777 LabelName.ABNORMAL\n",
      "Prediction took 1.0465 seconds.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# å¾…æµ‹è¯•å›¾åƒ\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# è¾“å‡ºè·¯å¾„ç¡®è®¤\n",
    "if os.path.exists(normal_ouput_path): \n",
    "    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): \n",
    "    shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# æ¨¡å‹æµ‹è¯•\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
