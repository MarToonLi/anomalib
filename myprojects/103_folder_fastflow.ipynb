{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å‚æ•°é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/anomalib/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"Fastflow\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path   = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "test_folder_path     = os.path.join(configs[\"dataset_root\"], \"test\")\n",
    "\n",
    "normal_ouput_path    = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")\n",
    "test_output_path     = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"test_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) çš„tensorç±»å‹\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # æå– B é€šé“ï¼ˆå¼ é‡çš„ç¬¬ä¸‰ä¸ªé€šé“ï¼Œç´¢å¼•ä¸º2ï¼‰\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # ä¸èƒ½æ·»åŠ \n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1ä¹‹é—´\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),                                               # å¦‚æœresizeHWä¸ä¸€è‡´ï¼Œä¼šå¼•èµ·fastflowæŠ¥layernormé”™è¯¯\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # æ— seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx ä¸æ”¯æŒ grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        #Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        #Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # è®°å½•å¼€å§‹æ—¶é—´\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # è¯»å–å›¾åƒ\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # å›¾åƒtransform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # å›¾åƒæ¨ç†\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        \n",
    "        tmp = cv2.resize(tmp, (256, 256))\n",
    "        predictions = inferencer.predict(image=tmp)        #! æ³¨ï¼šå¦‚æœä½¿ç”¨vinoï¼Œè¾“å…¥çš„imageå‚æ•°å¦‚æœä¸æ˜¯pathï¼Œé‚£ä¹ˆå…¶shapeåªèƒ½æ˜¯HWC\n",
    "        #predictions = torch_inferencer.predict(image=filter_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! ğŸ¯ inferencer.predictæ¥å—åŸå§‹å›¾åƒï¼Œ\n",
    "        #! å†…éƒ¨é€šè¿‡metadataå’Œmodelåœ¨forwardå‡½æ•°(épre_processå‡½æ•°)ä¸­è°ƒç”¨æ ‡å‡†åŒ–çš„transformå¯¹å›¾åƒå¤„ç†;\n",
    "        #! è§/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! æ³¨: 1. æ¨¡å‹å¯¼å‡ºæ—¶ï¼Œç”Ÿæˆçš„bin/onnxã€jsonç­‰æ–‡ä»¶ä¸­å‡ä¸åŒ…å«è®­ç»ƒæ—¶ä½¿ç”¨åˆ°çš„transformæ“ä½œï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºç›¸å…³çš„æ“ä½œå’Œæ ‡å‡†åŒ–ç›¸å…³çš„æ“ä½œ\n",
    "        #! æ³¨: 2. è€Œpredictæ¨ç†æ—¶ï¼Œä¼šè¿›è¡Œçš„é¢„å¤„ç†æ“ä½œæ˜¯æ ‡å‡†åŒ–æ“ä½œï¼Œæ¯”å¦‚normalizeï¼Œä½†æ˜¯è¿™é‡Œçš„normalizeæ˜¯é™¤ä»¥255çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯æ ‡å‡†æ­£å¤ªåˆ†å¸ƒã€‚\n",
    "        #! é€šè¿‡ä»¥ä¸Šæ€»ç»“ï¼Œå¯çŸ¥ï¼Œæˆ‘ä»¬éœ€è¦1. ä¿®æ”¹è®­ç»ƒæ—¶çš„normalizeï¼›2. å°†ExtractBChannelæ“ä½œè¦å‡ºç°åœ¨è®­ç»ƒæ—¶çš„transformä»¥å¤–ï¼Œè¿˜éœ€è¦å°è£…æˆä¸€ä¸ªæ•°æ®é¢„å¤„ç†æ“ä½œã€‚åœ¨æ¨¡å‹æ¨ç†ä¹‹å‰å¯¹å›¾åƒè¿›è¡Œé¢å¤–çš„é¢„å¤„ç†æ“ä½œã€‚\n",
    "        \n",
    "        # è®°å½•ç»“æŸæ—¶é—´\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # è®¡ç®—è€—æ—¶\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢çª—å£\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # åŸå§‹å›¾åƒ\n",
    "        tmp0 = cv2.normalize(image, None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[0].imshow(tmp0)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # çƒ­å›¾\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! çƒ­åŠ›å›¾æ˜¯anomaly_mapä¸åŸå§‹å›¾åƒçš„åŠ æƒç»“åˆ\n",
    "        axs[3].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # å…³é—­åæ ‡è½´ \n",
    "        \n",
    "\n",
    "        # æ·»åŠ æ–‡æœ¬ä¿¡æ¯åˆ°å›¾å½¢çš„ä¸Šæ–¹ä¸­é—´ä½ç½®\n",
    "        fig_text_x = 0.1   # xåæ ‡åœ¨å›¾å½¢å®½åº¦çš„ä¸­å¿ƒä½ç½®\n",
    "        fig_text_y = 0.95  # yåæ ‡ç¨å¾®é è¿‘å›¾å½¢çš„é¡¶éƒ¨ï¼Œé¿å…ä¸å­å›¾é‡å \n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n' \n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # æ˜¾ç¤ºæ•´ä¸ªå›¾å½¢\n",
    "        plt.tight_layout()  # è°ƒæ•´å­å›¾é—´çš„é—´è·\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom æ•°æ®é›†é…ç½®\n",
    "\n",
    "Folderçš„ç†è§£:\n",
    "\n",
    "1. è®­ç»ƒé›†ä¸­å…¨éƒ¨æ˜¯æ­£å¸¸æ ·æœ¬ï¼›\n",
    "2. æ­£å¸¸æ ·æœ¬é›†ä¸­çš„éƒ¨åˆ†(normal_split_ratio)æ ·æœ¬è¢«å‡ç­‰(val_split_ratio)æ”¾å…¥åˆ°éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­ã€‚\n",
    "3. test_split_ratioä¼¼ä¹å’Œnormal_split_ratioä¸€æ ·ã€‚\n",
    "4. ğŸ¯ éœ€è¦ç¡®ä¿è®­ç»ƒé›†ä¸å­˜åœ¨æ­£å¸¸æ ·æœ¬ï¼›æµ‹è¯•é›†ä¸å­˜åœ¨æ ·æœ¬ï¼›éªŒè¯é›†æ˜¯å…¨éƒ¨æ­£æ ·æœ¬å’Œå…¨éƒ¨è´Ÿæ ·æœ¬ã€‚\n",
    "\n",
    "æ¯”å¦‚ï¼šval_split_ratio=0.5æ—¶ï¼š82 | 27, 10 | 27, 10|; val_split_ratio=0.1æ—¶ï¼š82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.10\n"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"test\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.1,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.1,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleB.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleC.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> åˆ†æè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„æ•°æ®é‡ä»¥åŠç±»åˆ«åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # ç»Ÿè®¡dataloaderä¸­æ ·æœ¬çš„ç±»åˆ«æ¯”ä¾‹\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"labelé¢‘ç‡åˆ†å¸ƒï¼š{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([16, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({0: 100})\n",
      "image_paths(100)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/13__DA1479053.png', '/local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png', '/local_data/datasets/3-5-jing/normal/5__DA2951215.png', '/local_data/datasets/3-5-jing/normal/55__DA2951215.png', '/local_data/datasets/3-5-jing/normal/71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({0: 97, 1: 52})\n",
      "image_paths(149)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n"
     ]
    }
   ],
   "source": [
    "# Val images\n",
    "i, data = next(enumerate(val_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({1: 27, 0: 9})\n",
      "image_paths(36)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/11__DA1479053.png', '/local_data/datasets/3-5-jing/normal/3__DA2951175 (3).png', '/local_data/datasets/3-5-jing/normal/53__DA1479053.png', '/local_data/datasets/3-5-jing/normal/55__DA1479053.png', '/local_data/datasets/3-5-jing/normal/55__DA2951225.png']\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "i, data = next(enumerate(test_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> æŸ¥çœ‹å›¾åƒå†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å‰çš„æ•°æ® : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.9362553358078003, 0.06547410041093826;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"å¤„ç†å‰çš„æ•°æ® : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ–å™¨é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing Fastflow model.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:anomalib.models.components.base.anomaly_module:No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | loss                  | FastflowLoss             | 0      | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "7 | model                 | FastflowModel            | 7.7 M  | train\n",
      "---------------------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.678    Total estimated model params size (MB)\n",
      "261       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5816717e4b246638940bed435a54391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea978ed9cfc4fac95b2e9ab8214566d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:anomalib.callbacks.timer:Training took 45.49 seconds\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9502940c7802467084fdfa07c841ed88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.callbacks.timer:Testing took 85.0091655254364 seconds\n",
      "Throughput (batch_size=8) : 1.7527521777097823 FPS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.9483544230461121     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.8199999928474426     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.9483544230461121    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.8199999928474426    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/projects/results/Fastflow/latest\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    model_checkpoint,\n",
    "    early_stopping,\n",
    "    graph_logger,\n",
    "]\n",
    "\n",
    "\n",
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, \n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    pixel_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    # callbacks= callbacks\n",
    "                    )\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "elif configs[\"model_name\"] == \"Fastflow\":\n",
    "    model = Fastflow()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"auto\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    max_epochs=1,                            #! å¸Œæœ›èµ‹å€¼ç»™Lightning Trainerçš„å‚æ•°å¿…é¡»å…¨éƒ¨æ”¾åœ¨å·²æ ‡æ˜å‚æ•°çš„æœ€åé¢\n",
    "                    )\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "    \n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹å¯¼å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/Fastflow/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "\n",
    "#! ğŸ¯ æ¨¡å‹åœ¨å¯¼å‡ºæ—¶å¯ä»¥æŒ‡å®štransformï¼Œè€Œtransformå› ä¸ºç»§æ‰¿è‡ªtorch.nn.Moduleç±»å‹ï¼Œä¸”å®ç°åŸºäºtorchè‡ªèº«ç®—å­ï¼Œå› æ­¤å®ƒå¯ä»¥èå…¥åœ¨æ¨¡å‹æ–‡ä»¶ptæˆ–è€…onnxæ–‡ä»¶ä¸­ã€‚\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "# model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# openvino_model_path = model_output_path / \"weights\" / \"openvino\" / \"model.bin\"\n",
    "# metadata_path = model_output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n",
    "# print(openvino_model_path.exists(), metadata_path.exists())\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path       = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path     = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[input] shape[?,3,?,?] type: f32>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(inferencer.input_blob)\n",
    "print(inferencer.input_blob.partial_shape[2].is_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1__DA2951215 (3).png', '9__DA2951225 (2).png', '61__DA2951215.png', '9__DA2951225.png', '15__DA2951175 (2).png', '1__DA2951215.png', '13__DA2951215.png', '13__DA2951215 (3).png', '13__DA2951215 (2).png', '11__DA2951175 (3).png', '69__DA2951225.png', '19__DA1479053.png', '15__DA2951215 - å‰¯æœ¬.png', '1__DA1479053 (4).png', '3__DA1479053.png', '15__DA2951175.png', '9__DA2951225 (3).png', '67__DA2951215.png', '9__DA2951225 (4).png', '3__DA1479053 (2).png', '71__DA2951175.png', '3__DA2951215.png', '57__DA2951175.png', '15__DA2951215 (2).png', '65__DA1479053.png', '11__DA1479053.png', '3__DA1479053 (3).png', '13__DA2951225 (2).png', '11__DA1479053 (3).png']\n",
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/test/1__DA2951215 (3).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.8022 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5304 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/61__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5623 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5729 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951175 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6130 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/1__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6454 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5780 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951215 (3).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5860 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951215 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5770 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/11__DA2951175 (3).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5764 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/69__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5689 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/19__DA1479053.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5142 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951215 - å‰¯æœ¬.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6365 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/1__DA1479053 (4).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6199 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA1479053.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5411 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6257 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225 (3).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.4950 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/67__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5661 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225 (4).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5119 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA1479053 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5284 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/71__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5562 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5581 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/57__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5602 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951215 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5466 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/65__DA1479053.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5291 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/11__DA1479053.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5237 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA1479053 (3).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5319 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951225 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5407 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/11__DA1479053 (3).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.4970 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5744 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5361 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5210 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5452 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5361 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5450 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5228 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5283 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5436 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.5707 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# å¾…æµ‹è¯•å›¾åƒ\n",
    "test_png_files = [f for f in os.listdir(test_folder_path) if f.endswith('.png')]\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "\n",
    "print(test_png_files)\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# è¾“å‡ºè·¯å¾„ç¡®è®¤\n",
    "if os.path.exists(test_output_path):     shutil.rmtree(test_output_path)\n",
    "if os.path.exists(normal_ouput_path):    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(test_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# æ¨¡å‹æµ‹è¯•\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
