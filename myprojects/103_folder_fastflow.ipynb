{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/anomalib/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"Fastflow\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "\n",
    "normal_ouput_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) 的tensor类型\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # 提取 B 通道（张量的第三个通道，索引为2）\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # 不能添加\n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1之间\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),                                               # 如果resizeHW不一致，会引起fastflow报layernorm错误\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # 无seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx 不支持 grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 读取图像\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # 图像transform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # 图像推理\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        predictions = inferencer.predict(image=tmp)        #! 注：如果使用vino，输入的image参数如果不是path，那么其shape只能是HWC\n",
    "        #predictions = torch_inferencer.predict(image=transform_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! 🎯 inferencer.predict接受原始图像，\n",
    "        #! 内部通过metadata和model在forward函数(非pre_process函数)中调用标准化的transform对图像处理;\n",
    "        #! 见/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! 注: 1. 模型导出时，生成的bin/onnx、json等文件中均不包含训练时使用到的transform操作，包括数据增强相关的操作和标准化相关的操作\n",
    "        #! 注: 2. 而predict推理时，会进行的预处理操作是标准化操作，比如normalize，但是这里的normalize是除以255的方式，而不是标准正太分布。\n",
    "        #! 通过以上总结，可知，我们需要1. 修改训练时的normalize；2. 将ExtractBChannel操作要出现在训练时的transform以外，还需要封装成一个数据预处理操作。在模型推理之前对图像进行额外的预处理操作。\n",
    "        \n",
    "        # 记录结束时间\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # 计算耗时\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # 可视化\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        #print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 创建一个新的图形窗口\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # 原始图像\n",
    "        axs[0].imshow(image)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 热图\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! 热力图是anomaly_map与原始图像的加权结合\n",
    "        axs[3].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 预测掩模\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 预测掩模\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # 关闭坐标轴 \n",
    "        \n",
    "\n",
    "        # 添加文本信息到图形的上方中间位置\n",
    "        fig_text_x = 0.1   # x坐标在图形宽度的中心位置\n",
    "        fig_text_y = 0.95  # y坐标稍微靠近图形的顶部，避免与子图重叠\n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n'\n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # 显示整个图形\n",
    "        plt.tight_layout()  # 调整子图间的间距\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom 数据集配置\n",
    "\n",
    "Folder的理解:\n",
    "\n",
    "1. 训练集中全部是正常样本；\n",
    "2. 正常样本集中的部分(normal_split_ratio)样本被均等(val_split_ratio)放入到验证集和测试集中。\n",
    "3. test_split_ratio似乎和normal_split_ratio一样。\n",
    "4. 🎯 需要确保训练集不存在正常样本；测试集不存在样本；验证集是全部正样本和全部负样本。\n",
    "\n",
    "比如：val_split_ratio=0.5时：82 | 27, 10 | 27, 10|; val_split_ratio=0.1时：82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "WARNING:anomalib.data.utils.split:Zero subset length encountered during splitting. This means one of your subsets\n",
      "            might be empty or devoid of either normal or anomalous images.\n"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.98,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.98,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal_filtered\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.04,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # 进行数据集分割\n",
    "folder_datamoduleB.setup()    # 进行数据集分割\n",
    "folder_datamoduleC.setup()    # 进行数据集分割\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> 分析训练集、验证集、测试集的数据量以及类别分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # 统计dataloader中样本的类别比例\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"label频率分布：{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([16, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({0: 100})\n",
      "image_paths(100)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/13__DA1479053.png', '/local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png', '/local_data/datasets/3-5-jing/normal/5__DA2951215.png', '/local_data/datasets/3-5-jing/normal/55__DA2951215.png', '/local_data/datasets/3-5-jing/normal/71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({0: 97, 1: 52})\n",
      "image_paths(149)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n"
     ]
    }
   ],
   "source": [
    "# Val images\n",
    "i, data = next(enumerate(val_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({1: 28, 0: 2})\n",
      "image_paths(30)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal_filtered/11__DA1479053 (3).png', '/local_data/datasets/3-5-jing/abnormal_filtered/11__DA1479053.png', '/local_data/datasets/3-5-jing/abnormal_filtered/11__DA2951175 (3).png', '/local_data/datasets/3-5-jing/abnormal_filtered/13__DA2951215 (2).png', '/local_data/datasets/3-5-jing/abnormal_filtered/13__DA2951215 (3).png']\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "i, data = next(enumerate(test_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> 查看图像内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAACBCAYAAACma0xyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWYUlEQVR4nO3dz28baR3H8c+M7dhxkrYJSXcbWrZNBBILC3SFgAUJVCEhdcUKCSFxgwMc4MKBP4ADF24IrqiCA6KA0EpICAEHqNqlpVq0rLSq0G5pmnZDk5IfbZz4Z2yPOVTP7OPJjJO2iX88fr8ky449ntr9juf5zvd55hmv1Wq1BAAAgIHm9/oDAAAA4OmR1AEAADiApA4AAMABJHUAAAAOIKkDAABwAEkdAACAA0jqAAAAHJDez0JBEGh5eVkTExPyPO+wPxMOSKvV0vb2tmZnZ+X7j5e/E/PBRMyHDzEfPsR8+Ow35vtK6paXl3Xq1KkD+3DorqWlJZ08efKx3kPMBxsxHz7EfPgQ8+GzV8z3ldRNTEwc2AfqlpGREX3961/X888/r/e9732anJzUxMSEfN9XKpVSKpWS53lqtVqyL6oRBIEkhc+1Wi01m00FQaBWq6UgCBQEgRqNRnjb2tpSoVDQw4cP9fe//11Xr17tyXdO8iTxI+bEvB998pOf1Llz5zQ5Oanx8XGNjo4qn88rm83K932l02n5vi/f98MqhOd5sY9brVa4nNkmms2m6vW6arWayuWytra2tL6+rtdff11/+tOf1M8X4HE15nE8z9MXv/hFvfTSSzp27Fi4LeRyufC3bmJrx9tm4t1sNpVKpcJtwPzmzW+9VqupWq2qXC7rwYMHunbtml577bVefO1dhinmH/zgB/XVr35Vx48f19GjRzU2NqZcLqdMJtMWb0mxFUg7/nasU6mU6vV6+NtvNBoql8sqlUra3NzU2tqafve732l5eblr37WTveK3r6RuEEu0nucpk8kol8tpbGxMo6OjGhkZUTabDTcCSbsaePtHbe/oJbU18o1GQ/V6XZLk+752dnZULpeVyWS6/2X38CTxI+bEvB9ls1nlcjnl8/kwxiapS6VSSqfT4Q7eMA273cAnMQ15Op2W53lqNBrhdmSSv37laszjeJ6nkZGRcFswN7uR9zxPvu8rm81qZ2cnPHjzPE9BEIT35vlmsxk+Zyf45kCh1WppdHQ0PFA07+ulYYp5Lpdr++2bpG5kZKRjUheXzJt7z/OUy+VUKpXC/Xu9Xg/fX6/Xtb293Vf/Z3t9ln0ldYPObrCjO3bzt/mRm+Xto3fzulne3ijq9XrYoHiep9HR0b7f+Q8DYu4ez/OUzWbbdtx2Vc48tpeXFD4XNw7F3i7seJt47jcZRO9EY2Q/TqfTyuVyajQaajabbdU7czOVOns7sRM2U+k321cul2N76KG4fXn0tbjnkvbP1Wo1XC66fZjYZ7PZA/8eh8XZpM4EMPoDtgNrjr5MN4z06GjNPFev19uCbK/TPA6CQNVqNVzH4w5axcEh5u4zFTTz/26qr/YO2b6X1HYEb5aNe2yWDYJg13v6oSqD95iDNiPuN+j7flhxN9uNqczZ+4eRkZG2Azzz2Gxj0YbedPWbdaM7Go2GpPj9vL0f3usgzD5gM+sy7YIdcxP3VCqlXC53yN/u4Dib1NlVF7tR36ssa78et6HYy8Qd0ZtGBt1HzN1m/q+jSbvd6CY1xOb95j4ad+m9ioxprKPrRX+JVuHjKjie56larSqfz6ter6tSqbQd2JllTXztdcZtR6Y714zDQvcl9bw8TVU9Kakzt5GRkXC5fj/AczapizbedmMcfS0aKLt0b3fH2e+PbjjmB2+OCOmK6z5i7jZTJYmOn7FPhLHHUsXt5Dt13dgVGru7jcS9P0Ub17hkX3pUiS+VSruSevuxadQltY29jXbve57XNj4X3ZWUxNuvRV/vtF+2D/DsCn1S1X8Q9vHOJnVxko7ozH20JGs2EHtsll2ujVZv7Ean37P5YUHM3RFN1uyzHKMVuuiZj0nxN/fRwdNx60B/sROxTpVaSSqVSmo2mxofH28bemGPozT3ZhuIHjiYZJ+krjfsGHX6vXdK+iSFyVtcYh+X2EntiX6/czqpizvLMW5HHdddZz9n39s7g7hKwPj4uPL5vAqFwqF+N8Qj5u7yPC88CzV6ckTcAHgzTUVctS7pKD+6vZhbJpMhueszcY14XOwkKZ/Pt71PevTbtqcysfcBpoGPbmvpdDo82xLdZc5CjsY87mbYv2upfX8edzAXt19Jp9MaHR3t8rd9cs4OFIkOpDXP2aKNfPS16DiauI0lunHlcjlNTU0dyHfA4yHmbjNJXVxlzr7FPRet7EV33HbcTUUm+l6Suv4X17jHVd2Sthl7HUnJ/fHjxweqkXdR3H446XcffS3ucdxwDnvfMEi/f+crddLuxtg8Zz+2y7FxU1rYy8YF1zxXKpV0//79g/8y2Bdi7q6kHXGnHXRcoi6913VnV206Ne6mSoD+FNeIm1hGx8h2apzNdmHmrIsePJiTaEjqui/ugDwpltG4R5e1H9u/ebv71d4fDFK8h2IvldStZsRtLNGB8XE7BDMFhnmdMTj9g5i7x8w5Zs8ptleSlzTw2RZ3FqS0ewwl+kv0RCc7VnZ3qh3XaMIefS2pymNXc7e3t5nOpAfS6Uc1qOi+3Tw2EwfHJfBmW4iTyWSUz+cT9xme54Unww0CZ5O6uDMe4wZDRwNlH7nHbQTRxMAea2PGaHBE3xvE3G3pdDrcsUd3wHHVtbjHZhlJbTvvuAqd/ZgY95+48bPS7nFUhl29sS8NGP19xx0gRJMDxtR1XzTW5mQ2I51OJ57E0ukEh0ajIc/zwsROak/wzZVqBsXgfNInEHfKu31vxP2o4zaCuCPDaMPCjr+3iLm7ome7xiVvSY2xHaNoFcdIWk/0/egv0e7VuCEV9kGY/diwEz7zd1LV3nTHo7ui+2KT2JlL+2WzWaXT6bZLPJpr+0rxY65N0latVsMJhpOmuhqUSp2zW2ZcJi/Fd6NEf+T22U/283GVnuiRXdK/gcNHzN0WnYdur26zpEqLiXHcSTFm8uGk96J/RH/nphFPpVLh1QfM89GEzbw/eiakvU77SjPSe9OakNT1holHvV7X2tqa1tfXFQSBarWaCoWCMpmM0um0SqWSSqWSKpWKCoWCyuVymNiZ7cKMnTPXDx4fH9f4+LgmJiY0PT2tmZkZHTlyJDwgGKTKrNNb5uPuhOOqNdGj+ehr0R89O/7eIubuirtod1IXq/26abjtkyPiErVoxSeuCxf9odVqqdFohAdy9XpdDx8+1NbWljY2NrS2tqaHDx+qXC6rWq2Gy0rt242Z6sQkakEQhJeFyufzOnLkiI4dO6YjR46Er3veoyEYZnvq1LWHg7Ozs6ObN2/qj3/8o+7evatCoRD+/5uET3rvd2wfoI2MjKher2tkZETHjh3T//73v3C90Yqtif/09LSee+45zc3NqVwud/37Pilnkzp7x510pkxcg24fscWtI+7fsHf85mgB3UfM3Radpyo69UD0+WisooPmpd3z08VtN9FkEf1hZ2dHlUpFb7/9tv7zn//o5s2bWl1dVblc3lWdi+P7vs6cOaPp6WnV63XduXNHxWKx7UQokxBMTEzoxIkTmpub0zPPPKMgCJTJZMKB+Th8i4uLWlxc1M7Ozp7Lzs7O6itf+Ypee+015fN5rays6N69e/roRz+q73//+/rxj3+s6elpbW5u6vr16+H7zMFCsVhUsVjUnTt3dO3aNWWz2YE5OcbZlsjuP7f7383fSTvppHEZ9mudunfMjOPoPmLuNjupS5prKmlcXVySFk3iTfd79P0GjXd/WVpa0q9//WstLy+rWq0+9vvz+by+853v6Pnnn1e5XNYbb7yh+/fv69VXX9X29nYY70qlokqlotXVVb311luamZlRo9FQrVY76K+EDvaTzE1MTOj48eP62te+pvPnz2tnZ0dXr17VvXv3FASBbty4oV/+8peamZnRl770JY2Ojuqdd97Rw4cPE9dpzqodFM4mdZLaxkeZMmxSlcZ+HB1IHS3P2vfRBCCVSoUX/0X3EXN3RROtaMU0rmqX1NW6VwXXXm/cBMXovaWlpad6f6VS0cLCgt599129+OKL+uY3v6m1tTUtLCzo7bff1tra2q73tFotra6uPtW/i8ORyWT0ve99T1NTU7px44Z+8IMf6K233tLm5ma4TLVa1V/+8hd5nqd//vOf+shHPqJPfOIT4XbgAmeTOnvsk2l4pfidt1k+7v1J65UU241jGhd0HzF3mz2mzr7Zs70nVemi3arRBM9+PW5OM8bUuafZbOrChQtqtVo6d+6cRkdHVavV9N3vflcXLlzQpUuXqM4OkDNnzqhQKOjy5cu6fv1628kyNnPgv76+rsuXL2tycjI889UFziZ1ktr6wKM7873GTRnRLrm4SoH9Hhr43iLm7rLPfI12jUZPoEg62cV+3h7oHr3Zy3bqusdgMw3/pUuX9I9//EPZbFbnz5/Xhz70IV25ciUxMUB/yWQympub089//vPwpAZ7NoNOOnW9DiJnkzp7PFW0ApO004/rqouu0yxn/o4ewXueR1dcjxBzt0WnMTH3dmzMzSzXqRvdsLcVz9t9qTiqNe5rNpvhVBi/+c1v9OyzzxL3ARIEgf72t7+1jbvLZDLa2dkZujg6m9RJuyei7TS2Ku71vZ6PvmYqNgya7x1i7q64pMxMZ2EqKnZCFzeJaNJcY6YCaA4M7Jt9sAD3NZtN3bt3r9cfA4/BnmTYGNYTWZxN6pK6TOxqTdyYqqQB03aXjHnefj06xgfdR8yHQ6PRUKFQ0LvvvqulpSWtrKxodXVVxWJR1Wo17Haxu84879HcYmYesrGxMZ0+fVrPPPOMbt++rVqtplwup5mZGb3//e/XBz7wAU1OTqrRaOxregwA6AfOJnVS8rVAo91w+63cdBpEbycU2Wz2ID4+ngAxd1ez2dTS0pKuXbumhYUFra2tqVKp7LuKlk6n1Wg05Pu+pqam5Pu+VlZWtLi4qGKxqAcPHoTLjY2N6dSpU3rhhRc0Pz8/MHNUAdib2Re4yNmkLm58VXT8TVJVx36PfdmpuPFXcQOpmfqgN4i52xYXF3X37l3VarUnqpqZnfjZs2d19uxZnThxQsViUfPz87py5UqY1JlKYKFQ0L///W9NTU1pdnZ2oOaqApDM5ROfnE3qpN2Vlr3GV9nPmfvoGXL2eqNdetHl0H3E3F2dJh/NZrP62Mc+pps3b6pQKHRcz8mTJ/Wtb31LR48eValU0q9+9SttbGzELhsEgdbX17W+vv5Unx1A/3D5AM3p8kJ0LExSQy21n+kWrdLENdxxA+6TLhSO7iHmwyeVSukLX/iCzp07t2dy7XmeSqWSzpw5o7m5Ob3wwgv68pe/rA9/+MNd+rQAcHicbomiY22ik5HudYZj3Ngse91JA+kzmYzT5d1+RsyHz+nTp/Xyyy/rxo0be154e2pqSp/97Ge1sbERXl7qU5/6lF566SXiB2DgOZ3UNRqNjgPn7Qbanrog2nAnic5nFZ1DC91HzIdPq9XS/Py8bt26tefg53K5rPv37+vKlStKpVIaHR3V+Pi4Pve5zzk1qzyA4eT0mDpbp+43+3W7i83zvPD6oXa3nhlblclklE6n2/rn7clL0VvEfDiUSiVVq1Vtb2/vuWylUtEvfvELXbx4UW+++aZefvllfeYzn1GlUmFaGgADz+mkLm7QfFxDbV6zl5Peq8rYlRz7b9/3lc/n1Wg0tLW1Fa7PntQU3UXMh8/GxoZ+9KMf7ftkhnq9rnq9rgsXLujixYt65ZVX9NxzzxFDAAPP6b2YXT2JOwrvdKakfTak1F7NMe8zM86nUqlweSai7S1iPnwajYb+9a9/Pfb7giBQsVjUb3/7W01NTWlra+sQPh0AdI+zSV2r1QrHVwVBoGazmXhdUKn9rEc7MTANt33JIPN3s9nUgwcPVKlU2tZpBs3vNT4LB4uY40mYaUsAYNA5faKE3YXWbDZjp6gwogPl45Y1y9sD7M1lhOz1pdNpBs73CDEHAAwrZyt1knaNpUpqvPea5iJumoyk6TE8zwurReg+Yg4AGFZDl9SZLjX74uzmOWl3g24v32q1wuqPLTphLRWb3iHmAIBh5XRSV6/XVSwWw0a50WioUqloc3NTpVJJ5XJZ1WpV1Wq1rdJipq6QHs1Dlk6nlUqllMlklEqlwsfRKxKYWyqVku/7THPRA8QcANCJ2cd3uvTgoHI6qZOkXC6nWq2m27dva2FhQe+8847++9//anNzU9VqNRxMn06nwyTAruiYm+/7SqVSYWM/MjKiXC6no0eP6vjx43r22Wc1PT2tVCpF1abHiDkAoBNX99lOJ3U7Ozu6e/eu/vznP4cX+rYrKb7v68SJEzp58qQ+/elPa21tTX/9619VLpdVLBYf698y85fNzMzI9301m82D/jrYB2IOAOgkCALVarVef4xD4XRSt7CwoNu3b6tSqex6LZVK6fOf/7y+8Y1vaH5+XqdOndLKyorefPNNra6uxq7P933Nz88rCAItLi7uGodVLBYfOzHAwSLmAIBh5XRSV61WE187e/asfvazn+n06dMKgkDXr1/X73//exUKhcT3jI2N6fz585qbm9NPfvIT3blz5xA+NZ4GMQcADCun56nrZHZ2VpOTk/I8T5cuXdK3v/1t/fSnP9XKykrie4rFov7whz8olUrp4x//eBc/LQ4CMQcAuGwokzrP83Ts2DFtb2+rVqvp4sWLunXr1p5nLrZaLS0uLuqHP/yhNjY2ND4+3qVPjKdFzAEArnO6+zWJ7z/KZd944w29+OKLqtfrjzVx7Nramra2ttRoNA7rI+KAEXMAgOuGMqlrNpt69dVXVS6XdfnyZV29evWx1+HqmTOuIuYAANcNZferJJXLZVUqFb3++uu6f/9+rz8OuoCYAwBcNpSVOunRWKmrV6+qVCqpXq/3+uOgC4g5AMBlQ1upkxROGovhQcwBAK4a2kqdJN26deuxBstj8BFzAICrhrpkQeM+fIg5AMBVQ53UAQAAuIKkDgAAwAEkdQAAAA4gqQMAAHAASR0AAIADSOoAAAAcQFIHAADgAJI6AAAAB5DUAQAAOICkDgAAwAEkdQAAAA4gqQMAAHAASR0AAIADSOoAAAAcQFIHAADgAJI6AAAAB5DUAQAAOICkDgAAwAEkdQAAAA4gqQMAAHAASR0AAIADSOoAAAAcQFIHAADgAJI6AAAAB5DUAQAAOICkDgAAwAEkdQAAAA4gqQMAAHAASR0AAIADSOoAAAAcQFIHAADgAJI6AAAAB5DUAQAAOICkDgAAwAEkdQAAAA4gqQMAAHAASR0AAIADSOoAAAAcQFIHAADgAJI6AAAAB5DUAQAAOICkDgAAwAEkdQAAAA4gqQMAAHAASR0AAIADSOoAAAAcQFIHAADgAJI6AAAAB5DUAQAAOICkDgAAwAEkdQAAAA4gqQMAAHAASR0AAIADSOoAAAAcQFIHAADgAJI6AAAAB5DUAQAAOICkDgAAwAH7SupardZhfw4coieJHzEfbMR8+BDz4UPMh89e8dtXUre9vX0gHwa98STxI+aDjZgPH2I+fIj58Nkrfl5rH2l7EARaXl7WxMSEPM87sA+Hw9VqtbS9va3Z2Vn5/uP1tBPzwUTMhw8xHz7EfPjsN+b7SuoAAADQ3zhRAgAAwAEkdQAAAA4gqQMAAHAASR0AAIADSOoAAAAcQFIHAADgAJI6AAAAB/wfZQ3UEWP+sPcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前的数据 : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.9362553358078003, 0.06547410041093826;\n",
      "transform.2: -1.804444432258606, 2.517995834350586, -1.5134505033493042;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"处理前的数据 : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型选择和优化器配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing Fastflow model.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:anomalib.models.components.base.anomaly_module:No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | loss                  | FastflowLoss             | 0      | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "7 | model                 | FastflowModel            | 7.7 M  | train\n",
      "---------------------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.678    Total estimated model params size (MB)\n",
      "261       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601c217d049d4da2baa9c2d21a5f0f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7807119c514dea843e22d4e5f182d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:anomalib.callbacks.timer:Training took 43.99 seconds\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd633d91451c4739a1d58ea1b5985610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.callbacks.timer:Testing took 69.64969515800476 seconds\n",
      "Throughput (batch_size=8) : 2.1392771305313545 FPS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.981760561466217     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9444444179534912     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.981760561466217    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9444444179534912    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/projects/results/Fastflow/latest\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    model_checkpoint,\n",
    "    early_stopping,\n",
    "    graph_logger,\n",
    "]\n",
    "\n",
    "\n",
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, \n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    pixel_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    # callbacks= callbacks\n",
    "                    )\n",
    "    # engine.train(model=model, \n",
    "    #              train_dataloaders=train_loader_lst, \n",
    "    #              val_dataloaders=val_loader_lst, \n",
    "    #              test_dataloaders=test_loader_lst)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=test_loader)\n",
    "elif configs[\"model_name\"] == \"Fastflow\":\n",
    "    model = Fastflow()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"auto\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    max_epochs=1,                            #! 希望赋值给Lightning Trainer的参数必须全部放在已标明参数的最后面\n",
    "                    )\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "    \n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/Fastflow/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "# model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# openvino_model_path = model_output_path / \"weights\" / \"openvino\" / \"model.bin\"\n",
    "# metadata_path = model_output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n",
    "# print(openvino_model_path.exists(), metadata_path.exists())\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.15258617148454573 LabelName.NORMAL\n",
      "Prediction took 1.1802 seconds.\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.13462447928461213 LabelName.NORMAL\n",
      "Prediction took 1.0889 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [2.865513e-08..1.0000001].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.4438535835224849 LabelName.NORMAL\n",
      "Prediction took 0.9160 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.574201e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.1877837405138954 LabelName.NORMAL\n",
      "Prediction took 1.1105 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4916407e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.27357514890748513 LabelName.NORMAL\n",
      "Prediction took 0.9661 seconds.\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.9015370254314499 LabelName.ABNORMAL\n",
      "Prediction took 1.0223 seconds.\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.0571 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-5.6794747e-10..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.8170319617869952 LabelName.ABNORMAL\n",
      "Prediction took 1.0010 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.673895e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.8186749472043311 LabelName.ABNORMAL\n",
      "Prediction took 1.2177 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.16941745e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "torch.Size([3, 2048, 2448]) --> torch.Size([1, 3, 2048, 2448]) --> torch.Size([1, 2048, 2448]) -- torch.Size([3, 2048, 2448]);\n",
      "0.8667581616047777 LabelName.ABNORMAL\n",
      "Prediction took 1.0465 seconds.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 待测试图像\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# 输出路径确认\n",
    "if os.path.exists(normal_ouput_path): \n",
    "    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): \n",
    "    shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# 模型测试\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
