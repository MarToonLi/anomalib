{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÂèÇÊï∞ÈÖçÁΩÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/anomalib/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"Patchcore\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path   = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "test_folder_path     = os.path.join(configs[\"dataset_root\"], \"test\")\n",
    "\n",
    "normal_ouput_path    = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")\n",
    "test_output_path     = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"test_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) ÁöÑtensorÁ±ªÂûã\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # ÊèêÂèñ B ÈÄöÈÅìÔºàÂº†ÈáèÁöÑÁ¨¨‰∏â‰∏™ÈÄöÈÅìÔºåÁ¥¢Âºï‰∏∫2Ôºâ\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # ‰∏çËÉΩÊ∑ªÂä†\n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1‰πãÈó¥\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),                                               # Â¶ÇÊûúresizeHW‰∏ç‰∏ÄËá¥Ôºå‰ºöÂºïËµ∑fastflowÊä•layernormÈîôËØØ\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # Êó†seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx ‰∏çÊîØÊåÅ grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # ËÆ∞ÂΩïÂºÄÂßãÊó∂Èó¥\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ËØªÂèñÂõæÂÉè\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # ÂõæÂÉètransform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # ÂõæÂÉèÊé®ÁêÜ\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        predictions = inferencer.predict(image=tmp)        #! Ê≥®ÔºöÂ¶ÇÊûú‰ΩøÁî®vinoÔºåËæìÂÖ•ÁöÑimageÂèÇÊï∞Â¶ÇÊûú‰∏çÊòØpathÔºåÈÇ£‰πàÂÖ∂shapeÂè™ËÉΩÊòØHWC\n",
    "        #predictions = torch_inferencer.predict(image=transform_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! üéØ inferencer.predictÊé•ÂèóÂéüÂßãÂõæÂÉèÔºå\n",
    "        #! ÂÜÖÈÉ®ÈÄöËøámetadataÂíåmodelÂú®forwardÂáΩÊï∞(Èùûpre_processÂáΩÊï∞)‰∏≠Ë∞ÉÁî®Ê†áÂáÜÂåñÁöÑtransformÂØπÂõæÂÉèÂ§ÑÁêÜ;\n",
    "        #! ËßÅ/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! Ê≥®: 1. Ê®°ÂûãÂØºÂá∫Êó∂ÔºåÁîüÊàêÁöÑbin/onnx„ÄÅjsonÁ≠âÊñá‰ª∂‰∏≠Âùá‰∏çÂåÖÂê´ËÆ≠ÁªÉÊó∂‰ΩøÁî®Âà∞ÁöÑtransformÊìç‰ΩúÔºåÂåÖÊã¨Êï∞ÊçÆÂ¢ûÂº∫Áõ∏ÂÖ≥ÁöÑÊìç‰ΩúÂíåÊ†áÂáÜÂåñÁõ∏ÂÖ≥ÁöÑÊìç‰Ωú\n",
    "        #! Ê≥®: 2. ËÄåpredictÊé®ÁêÜÊó∂Ôºå‰ºöËøõË°åÁöÑÈ¢ÑÂ§ÑÁêÜÊìç‰ΩúÊòØÊ†áÂáÜÂåñÊìç‰ΩúÔºåÊØîÂ¶ÇnormalizeÔºå‰ΩÜÊòØËøôÈáåÁöÑnormalizeÊòØÈô§‰ª•255ÁöÑÊñπÂºèÔºåËÄå‰∏çÊòØÊ†áÂáÜÊ≠£Â§™ÂàÜÂ∏É„ÄÇ\n",
    "        #! ÈÄöËøá‰ª•‰∏äÊÄªÁªìÔºåÂèØÁü•ÔºåÊàë‰ª¨ÈúÄË¶Å1. ‰øÆÊîπËÆ≠ÁªÉÊó∂ÁöÑnormalizeÔºõ2. Â∞ÜExtractBChannelÊìç‰ΩúË¶ÅÂá∫Áé∞Âú®ËÆ≠ÁªÉÊó∂ÁöÑtransform‰ª•Â§ñÔºåËøòÈúÄË¶ÅÂ∞ÅË£ÖÊàê‰∏Ä‰∏™Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊìç‰Ωú„ÄÇÂú®Ê®°ÂûãÊé®ÁêÜ‰πãÂâçÂØπÂõæÂÉèËøõË°åÈ¢ùÂ§ñÁöÑÈ¢ÑÂ§ÑÁêÜÊìç‰Ωú„ÄÇ\n",
    "        \n",
    "        # ËÆ∞ÂΩïÁªìÊùüÊó∂Èó¥\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # ËÆ°ÁÆóËÄóÊó∂\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # ÂèØËßÜÂåñ\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        #print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÂõæÂΩ¢Á™óÂè£\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # ÂéüÂßãÂõæÂÉè\n",
    "        tmp0 = cv2.normalize(image.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[0].imshow(tmp0)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "        \n",
    "        # ËÆ≠ÁªÉÁî®ÂõæÂÉè\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "        \n",
    "        # ËÆ≠ÁªÉÁî®ÂõæÂÉè\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "\n",
    "        # ÁÉ≠Âõæ\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! ÁÉ≠ÂäõÂõæÊòØanomaly_map‰∏éÂéüÂßãÂõæÂÉèÁöÑÂä†ÊùÉÁªìÂêà\n",
    "        axs[3].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "\n",
    "        # È¢ÑÊµãÊé©Ê®°\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "\n",
    "        # È¢ÑÊµãÊé©Ê®°\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥ \n",
    "        \n",
    "\n",
    "        # Ê∑ªÂä†ÊñáÊú¨‰ø°ÊÅØÂà∞ÂõæÂΩ¢ÁöÑ‰∏äÊñπ‰∏≠Èó¥‰ΩçÁΩÆ\n",
    "        fig_text_x = 0.1   # xÂùêÊ†áÂú®ÂõæÂΩ¢ÂÆΩÂ∫¶ÁöÑ‰∏≠ÂøÉ‰ΩçÁΩÆ\n",
    "        fig_text_y = 0.95  # yÂùêÊ†áÁ®çÂæÆÈù†ËøëÂõæÂΩ¢ÁöÑÈ°∂ÈÉ®ÔºåÈÅøÂÖç‰∏éÂ≠êÂõæÈáçÂè†\n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n' \n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # ÊòæÁ§∫Êï¥‰∏™ÂõæÂΩ¢\n",
    "        plt.tight_layout()  # Ë∞ÉÊï¥Â≠êÂõæÈó¥ÁöÑÈó¥Ë∑ù\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Êï∞ÊçÆÈõÜÈÖçÁΩÆ\n",
    "\n",
    "FolderÁöÑÁêÜËß£:\n",
    "\n",
    "1. ËÆ≠ÁªÉÈõÜ‰∏≠ÂÖ®ÈÉ®ÊòØÊ≠£Â∏∏Ê†∑Êú¨Ôºõ\n",
    "2. Ê≠£Â∏∏Ê†∑Êú¨ÈõÜ‰∏≠ÁöÑÈÉ®ÂàÜ(normal_split_ratio)Ê†∑Êú¨Ë¢´ÂùáÁ≠â(val_split_ratio)ÊîæÂÖ•Âà∞È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏≠„ÄÇ\n",
    "3. test_split_ratio‰ºº‰πéÂíånormal_split_ratio‰∏ÄÊ†∑„ÄÇ\n",
    "4. üéØ ÈúÄË¶ÅÁ°Æ‰øùËÆ≠ÁªÉÈõÜ‰∏çÂ≠òÂú®Ê≠£Â∏∏Ê†∑Êú¨ÔºõÊµãËØïÈõÜ‰∏çÂ≠òÂú®Ê†∑Êú¨ÔºõÈ™åËØÅÈõÜÊòØÂÖ®ÈÉ®Ê≠£Ê†∑Êú¨ÂíåÂÖ®ÈÉ®Ë¥üÊ†∑Êú¨„ÄÇ\n",
    "\n",
    "ÊØîÂ¶ÇÔºöval_split_ratio=0.5Êó∂Ôºö82 | 27, 10 | 27, 10|; val_split_ratio=0.1Êó∂Ôºö82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Path does not exist: /local_data/datasets/3-5-jing/abnormal_filtered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m folder_datamoduleA\u001b[38;5;241m.\u001b[39msetup()    \u001b[38;5;66;03m# ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\u001b[39;00m\n\u001b[1;32m     41\u001b[0m folder_datamoduleB\u001b[38;5;241m.\u001b[39msetup()    \u001b[38;5;66;03m# ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mfolder_datamoduleC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\u001b[39;00m\n\u001b[1;32m     44\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m folder_datamoduleA\u001b[38;5;241m.\u001b[39mtrain_dataloader()\n\u001b[1;32m     45\u001b[0m val_loader   \u001b[38;5;241m=\u001b[39m folder_datamoduleB\u001b[38;5;241m.\u001b[39mval_dataloader()\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/base/datamodule.py:138\u001b[0m, in \u001b[0;36mAnomalibDataModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    136\u001b[0m has_subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, subset) \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_data\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_subset \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_setup:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_test_split()\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_val_split()\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/image/folder.py:446\u001b[0m, in \u001b[0;36mFolder._setup\u001b[0;34m(self, _stage)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup\u001b[39m(\u001b[38;5;28mself\u001b[39m, _stage: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data \u001b[38;5;241m=\u001b[39m \u001b[43mFolderDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabnormal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabnormal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormal_test_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_test_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_data \u001b[38;5;241m=\u001b[39m FolderDataset(\n\u001b[1;32m    460\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    461\u001b[0m         task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         extensions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/image/folder.py:261\u001b[0m, in \u001b[0;36mFolderDataset.__init__\u001b[0;34m(self, name, task, normal_dir, transform, root, abnormal_dir, normal_test_dir, mask_dir, split, extensions)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_dir \u001b[38;5;241m=\u001b[39m mask_dir\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m \u001b[43mmake_folder_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mabnormal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabnormal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormal_test_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_test_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/image/folder.py:102\u001b[0m, in \u001b[0;36mmake_folder_dataset\u001b[0;34m(normal_dir, root, abnormal_dir, normal_test_dir, mask_dir, split, extensions)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# All paths are changed to the List[Path] type and used.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m normal_dir \u001b[38;5;241m=\u001b[39m _resolve_path_and_convert_to_list(normal_dir)\n\u001b[0;32m--> 102\u001b[0m abnormal_dir \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_path_and_convert_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabnormal_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m normal_test_dir \u001b[38;5;241m=\u001b[39m _resolve_path_and_convert_to_list(normal_test_dir)\n\u001b[1;32m    104\u001b[0m mask_dir \u001b[38;5;241m=\u001b[39m _resolve_path_and_convert_to_list(mask_dir)\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/image/folder.py:98\u001b[0m, in \u001b[0;36mmake_folder_dataset.<locals>._resolve_path_and_convert_to_list\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [validate_and_resolve_path(dir_path, root) \u001b[38;5;28;01mfor\u001b[39;00m dir_path \u001b[38;5;129;01min\u001b[39;00m path]\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mvalidate_and_resolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/utils/path.py:246\u001b[0m, in \u001b[0;36mvalidate_and_resolve_path\u001b[0;34m(folder, root, base_dir)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_and_resolve_path\u001b[39m(\n\u001b[1;32m    232\u001b[0m     folder: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path,\n\u001b[1;32m    233\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    234\u001b[0m     base_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Path:\n\u001b[1;32m    236\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate and resolve the path.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m        Path: Validated and resolved path.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidate_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/utils/path.py:216\u001b[0m, in \u001b[0;36mvalidate_path\u001b[0;34m(path, base_dir, should_exist, extensions)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    215\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Check the read and execute permissions\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (os\u001b[38;5;241m.\u001b[39maccess(path, os\u001b[38;5;241m.\u001b[39mR_OK) \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39maccess(path, os\u001b[38;5;241m.\u001b[39mX_OK)):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Path does not exist: /local_data/datasets/3-5-jing/abnormal_filtered"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! ËÆ°ÁÆóÁöÑÊó∂ÂÄô‰ºö‰ΩøÁî®cudaÔºåÂõ†Ê≠§ÈúÄË¶ÅÈôêÂà∂BS‰∏çÈÄÇÁî®ÈªòËÆ§ÂÄº32Ôºõ\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! ÊéßÂà∂Ê≠£Â∏∏Ê†∑Êú¨ÔºåÂú®ÈùûËÆ≠ÁªÉÈõÜÂíåËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    "    val_split_ratio=0.98,     #! ÊéßÂà∂Ââ©‰ΩôÊ≠£Â∏∏Ê†∑Êú¨ÂíåÂºÇÂ∏∏Ê†∑Êú¨ÔºåÂú®È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! ËÆ°ÁÆóÁöÑÊó∂ÂÄô‰ºö‰ΩøÁî®cudaÔºåÂõ†Ê≠§ÈúÄË¶ÅÈôêÂà∂BS‰∏çÈÄÇÁî®ÈªòËÆ§ÂÄº32Ôºõ\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! ÊéßÂà∂Ê≠£Â∏∏Ê†∑Êú¨ÔºåÂú®ÈùûËÆ≠ÁªÉÈõÜÂíåËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    "    val_split_ratio=0.98,     #! ÊéßÂà∂Ââ©‰ΩôÊ≠£Â∏∏Ê†∑Êú¨ÂíåÂºÇÂ∏∏Ê†∑Êú¨ÔºåÂú®È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal_filtered\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! ËÆ°ÁÆóÁöÑÊó∂ÂÄô‰ºö‰ΩøÁî®cudaÔºåÂõ†Ê≠§ÈúÄË¶ÅÈôêÂà∂BS‰∏çÈÄÇÁî®ÈªòËÆ§ÂÄº32Ôºõ\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! ÊéßÂà∂Ê≠£Â∏∏Ê†∑Êú¨ÔºåÂú®ÈùûËÆ≠ÁªÉÈõÜÂíåËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    "    val_split_ratio=0.04,     #! ÊéßÂà∂Ââ©‰ΩôÊ≠£Â∏∏Ê†∑Êú¨ÂíåÂºÇÂ∏∏Ê†∑Êú¨ÔºåÂú®È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\n",
    "folder_datamoduleB.setup()    # ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\n",
    "folder_datamoduleC.setup()    # ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> ÂàÜÊûêËÆ≠ÁªÉÈõÜ„ÄÅÈ™åËØÅÈõÜ„ÄÅÊµãËØïÈõÜÁöÑÊï∞ÊçÆÈáè‰ª•ÂèäÁ±ªÂà´ÂàÜÂ∏É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # ÁªüËÆ°dataloader‰∏≠Ê†∑Êú¨ÁöÑÁ±ªÂà´ÊØî‰æã\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"labelÈ¢ëÁéáÂàÜÂ∏ÉÔºö{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([16, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelÈ¢ëÁéáÂàÜÂ∏ÉÔºöCounter({0: 100})\n",
      "image_paths(100)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/13__DA1479053.png', '/local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png', '/local_data/datasets/3-5-jing/normal/5__DA2951215.png', '/local_data/datasets/3-5-jing/normal/55__DA2951215.png', '/local_data/datasets/3-5-jing/normal/71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelÈ¢ëÁéáÂàÜÂ∏ÉÔºöCounter({0: 97, 1: 52})\n",
      "image_paths(149)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n"
     ]
    }
   ],
   "source": [
    "# Val images\n",
    "i, data = next(enumerate(val_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelÈ¢ëÁéáÂàÜÂ∏ÉÔºöCounter({1: 28, 0: 2})\n",
      "image_paths(30)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal_filtered/11__DA1479053 (3).png', '/local_data/datasets/3-5-jing/abnormal_filtered/11__DA1479053.png', '/local_data/datasets/3-5-jing/abnormal_filtered/11__DA2951175 (3).png', '/local_data/datasets/3-5-jing/abnormal_filtered/13__DA2951215 (2).png', '/local_data/datasets/3-5-jing/abnormal_filtered/13__DA2951215 (3).png']\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "i, data = next(enumerate(test_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> Êü•ÁúãÂõæÂÉèÂÜÖÂÆπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â§ÑÁêÜÂâçÁöÑÊï∞ÊçÆ : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.9362553358078003, 0.06547410041093826;\n",
      "transform.2: -1.804444432258606, 2.517995834350586, -1.5134505033493042;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"Â§ÑÁêÜÂâçÁöÑÊï∞ÊçÆ : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÂûãÈÄâÊã©Âíå‰ºòÂåñÂô®ÈÖçÁΩÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing Fastflow model.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:anomalib.models.components.base.anomaly_module:No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | loss                  | FastflowLoss             | 0      | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "7 | model                 | FastflowModel            | 7.7 M  | train\n",
      "---------------------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.678    Total estimated model params size (MB)\n",
      "261       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601c217d049d4da2baa9c2d21a5f0f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7807119c514dea843e22d4e5f182d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:anomalib.callbacks.timer:Training took 43.99 seconds\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd633d91451c4739a1d58ea1b5985610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.callbacks.timer:Testing took 69.64969515800476 seconds\n",
      "Throughput (batch_size=8) : 2.1392771305313545 FPS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\">        Test metric        </span>‚îÉ<span style=\"font-weight: bold\">       DataLoader 0        </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">     0.981760561466217     </span>‚îÇ\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.9444444179534912     </span>‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m    0.981760561466217    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
       "‚îÇ\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m   0.9444444179534912    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/projects/results/Fastflow/latest\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    model_checkpoint,\n",
    "    early_stopping,\n",
    "    graph_logger,\n",
    "]\n",
    "\n",
    "\n",
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, \n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    pixel_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    # callbacks= callbacks\n",
    "                    )\n",
    "    # engine.train(model=model, \n",
    "    #              train_dataloaders=train_loader_lst, \n",
    "    #              val_dataloaders=val_loader_lst, \n",
    "    #              test_dataloaders=test_loader_lst)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=test_loader)\n",
    "elif configs[\"model_name\"] == \"Fastflow\":\n",
    "    model = Fastflow()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"auto\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    max_epochs=1,                            #! Â∏åÊúõËµãÂÄºÁªôLightning TrainerÁöÑÂèÇÊï∞ÂøÖÈ°ªÂÖ®ÈÉ®ÊîæÂú®Â∑≤Ê†áÊòéÂèÇÊï∞ÁöÑÊúÄÂêéÈù¢\n",
    "                    )\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "    \n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÂûãÂØºÂá∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/Fastflow/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "# model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# openvino_model_path = model_output_path / \"weights\" / \"openvino\" / \"model.bin\"\n",
    "# metadata_path = model_output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n",
    "# print(openvino_model_path.exists(), metadata_path.exists())\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÂûãÊµãËØï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1__DA2951215 (3).png', '9__DA2951225 (2).png', '61__DA2951215.png', '9__DA2951225.png', '15__DA2951175 (2).png', '1__DA2951215.png', '13__DA2951215.png', '13__DA2951215 (3).png', '13__DA2951215 (2).png', '11__DA2951175 (3).png', '69__DA2951225.png', '19__DA1479053.png', '15__DA2951215 - ÂâØÊú¨.png', '1__DA1479053 (4).png', '3__DA1479053.png', '15__DA2951175.png', '9__DA2951225 (3).png', '67__DA2951215.png', '9__DA2951225 (4).png', '3__DA1479053 (2).png', '71__DA2951175.png', '3__DA2951215.png', '57__DA2951175.png', '15__DA2951215 (2).png', '65__DA1479053.png', '11__DA1479053.png', '3__DA1479053 (3).png', '13__DA2951225 (2).png', '11__DA1479053 (3).png']\n",
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/1__DA2951215 (3).png;\n",
      "0.5971561961839659 LabelName.ABNORMAL\n",
      "Prediction took 0.9854 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225 (2).png;\n",
      "0.8132488163764002 LabelName.ABNORMAL\n",
      "Prediction took 0.8697 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.987191e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/61__DA2951215.png;\n",
      "0.6961009147738344 LabelName.ABNORMAL\n",
      "Prediction took 0.9624 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225.png;\n",
      "0.7041936876918945 LabelName.ABNORMAL\n",
      "Prediction took 0.8351 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.222311e-08..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951175 (2).png;\n",
      "0.6754931721112475 LabelName.ABNORMAL\n",
      "Prediction took 0.8895 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/1__DA2951215.png;\n",
      "0.5421393700331256 LabelName.ABNORMAL\n",
      "Prediction took 0.9354 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.6752104e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951215.png;\n",
      "0.6030783166476199 LabelName.ABNORMAL\n",
      "Prediction took 1.1184 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-9.630957e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951215 (3).png;\n",
      "0.6124779469462774 LabelName.ABNORMAL\n",
      "Prediction took 1.0415 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.2045656e-10..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951215 (2).png;\n",
      "0.6030783166476199 LabelName.ABNORMAL\n",
      "Prediction took 1.0608 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-9.630957e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/11__DA2951175 (3).png;\n",
      "0.9077558285891838 LabelName.ABNORMAL\n",
      "Prediction took 1.0704 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/69__DA2951225.png;\n",
      "0.4352237430417678 LabelName.NORMAL\n",
      "Prediction took 1.0536 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/19__DA1479053.png;\n",
      "0.37838906063207345 LabelName.NORMAL\n",
      "Prediction took 1.0144 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-6.590362e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951215 - ÂâØÊú¨.png;\n",
      "0.7580850576407577 LabelName.ABNORMAL\n",
      "Prediction took 1.0715 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8904487e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/1__DA1479053 (4).png;\n",
      "0.596735094768884 LabelName.ABNORMAL\n",
      "Prediction took 0.7887 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.691003e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA1479053.png;\n",
      "0.8268994854726331 LabelName.ABNORMAL\n",
      "Prediction took 0.8108 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2885391e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951175.png;\n",
      "0.6754931721112475 LabelName.ABNORMAL\n",
      "Prediction took 0.9233 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225 (3).png;\n",
      "0.7041936876918945 LabelName.ABNORMAL\n",
      "Prediction took 0.8357 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.222311e-08..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/67__DA2951215.png;\n",
      "0.34311165296524704 LabelName.NORMAL\n",
      "Prediction took 0.9088 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225 (4).png;\n",
      "0.6334254777942206 LabelName.ABNORMAL\n",
      "Prediction took 0.8374 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.800192e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA1479053 (2).png;\n",
      "0.8268994854726331 LabelName.ABNORMAL\n",
      "Prediction took 0.9790 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2885391e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/71__DA2951175.png;\n",
      "0.7027006630398719 LabelName.ABNORMAL\n",
      "Prediction took 0.9685 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA2951215.png;\n",
      "0.6157166255009664 LabelName.ABNORMAL\n",
      "Prediction took 1.0508 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/57__DA2951175.png;\n",
      "0.5729443920730134 LabelName.ABNORMAL\n",
      "Prediction took 1.1356 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0350673e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951215 (2).png;\n",
      "0.7580850576407577 LabelName.ABNORMAL\n",
      "Prediction took 1.0783 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8904487e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/65__DA1479053.png;\n",
      "0.6047196958126847 LabelName.ABNORMAL\n",
      "Prediction took 0.8735 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.4684586e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/11__DA1479053.png;\n",
      "0.9443510609164592 LabelName.ABNORMAL\n",
      "Prediction took 1.0117 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.684697e-11..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA1479053 (3).png;\n",
      "0.8268994854726331 LabelName.ABNORMAL\n",
      "Prediction took 1.0946 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2885391e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951225 (2).png;\n",
      "0.7813827198854031 LabelName.ABNORMAL\n",
      "Prediction took 0.8409 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.491868e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/11__DA1479053 (3).png;\n",
      "0.9443510609164592 LabelName.ABNORMAL\n",
      "Prediction took 0.7612 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.684697e-11..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "0.15258617148454573 LabelName.NORMAL\n",
      "Prediction took 0.8689 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "0.13462447928461213 LabelName.NORMAL\n",
      "Prediction took 0.7791 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [2.865513e-08..1.0000001].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "0.4438535835224849 LabelName.NORMAL\n",
      "Prediction took 0.9043 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.574201e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "0.1877837405138954 LabelName.NORMAL\n",
      "Prediction took 0.9576 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4916407e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "0.27357514890748513 LabelName.NORMAL\n",
      "Prediction took 0.9334 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "0.9015370254314499 LabelName.ABNORMAL\n",
      "Prediction took 0.8949 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.9404 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-5.6794747e-10..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "0.8170319617869952 LabelName.ABNORMAL\n",
      "Prediction took 0.9141 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.673895e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "0.8186749472043311 LabelName.ABNORMAL\n",
      "Prediction took 0.9514 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.16941745e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "0.8667581616047777 LabelName.ABNORMAL\n",
      "Prediction took 1.0324 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# ÂæÖÊµãËØïÂõæÂÉè\n",
    "test_png_files = [f for f in os.listdir(test_folder_path) if f.endswith('.png')]\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "\n",
    "print(test_png_files)\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# ËæìÂá∫Ë∑ØÂæÑÁ°ÆËÆ§\n",
    "if os.path.exists(test_output_path):     shutil.rmtree(test_output_path)\n",
    "if os.path.exists(normal_ouput_path):    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(test_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# Ê®°ÂûãÊµãËØï\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
