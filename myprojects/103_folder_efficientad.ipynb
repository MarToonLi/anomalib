{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/anomalib/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"Patchcore\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path   = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "test_folder_path     = os.path.join(configs[\"dataset_root\"], \"test\")\n",
    "\n",
    "normal_ouput_path    = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")\n",
    "test_output_path     = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"test_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) 的tensor类型\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # 提取 B 通道（张量的第三个通道，索引为2）\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # 不能添加\n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1之间\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),                                               # 如果resizeHW不一致，会引起fastflow报layernorm错误\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # 无seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx 不支持 grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 读取图像\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # 图像transform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # 图像推理\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        predictions = inferencer.predict(image=tmp)        #! 注：如果使用vino，输入的image参数如果不是path，那么其shape只能是HWC\n",
    "        #predictions = torch_inferencer.predict(image=transform_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! 🎯 inferencer.predict接受原始图像，\n",
    "        #! 内部通过metadata和model在forward函数(非pre_process函数)中调用标准化的transform对图像处理;\n",
    "        #! 见/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! 注: 1. 模型导出时，生成的bin/onnx、json等文件中均不包含训练时使用到的transform操作，包括数据增强相关的操作和标准化相关的操作\n",
    "        #! 注: 2. 而predict推理时，会进行的预处理操作是标准化操作，比如normalize，但是这里的normalize是除以255的方式，而不是标准正太分布。\n",
    "        #! 通过以上总结，可知，我们需要1. 修改训练时的normalize；2. 将ExtractBChannel操作要出现在训练时的transform以外，还需要封装成一个数据预处理操作。在模型推理之前对图像进行额外的预处理操作。\n",
    "        \n",
    "        # 记录结束时间\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # 计算耗时\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # 可视化\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        #print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 创建一个新的图形窗口\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # 原始图像\n",
    "        tmp0 = cv2.normalize(image.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[0].imshow(tmp0)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 热图\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! 热力图是anomaly_map与原始图像的加权结合\n",
    "        axs[3].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 预测掩模\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 预测掩模\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # 关闭坐标轴 \n",
    "        \n",
    "\n",
    "        # 添加文本信息到图形的上方中间位置\n",
    "        fig_text_x = 0.1   # x坐标在图形宽度的中心位置\n",
    "        fig_text_y = 0.95  # y坐标稍微靠近图形的顶部，避免与子图重叠\n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n' \n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # 显示整个图形\n",
    "        plt.tight_layout()  # 调整子图间的间距\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom 数据集配置\n",
    "\n",
    "Folder的理解:\n",
    "\n",
    "1. 训练集中全部是正常样本；\n",
    "2. 正常样本集中的部分(normal_split_ratio)样本被均等(val_split_ratio)放入到验证集和测试集中。\n",
    "3. test_split_ratio似乎和normal_split_ratio一样。\n",
    "4. 🎯 需要确保训练集不存在正常样本；测试集不存在样本；验证集是全部正样本和全部负样本。\n",
    "\n",
    "比如：val_split_ratio=0.5时：82 | 27, 10 | 27, 10|; val_split_ratio=0.1时：82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Path does not exist: /local_data/datasets/3-5-jing/abnormal_filtered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m folder_datamoduleA\u001b[38;5;241m.\u001b[39msetup()    \u001b[38;5;66;03m# 进行数据集分割\u001b[39;00m\n\u001b[1;32m     41\u001b[0m folder_datamoduleB\u001b[38;5;241m.\u001b[39msetup()    \u001b[38;5;66;03m# 进行数据集分割\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mfolder_datamoduleC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# 进行数据集分割\u001b[39;00m\n\u001b[1;32m     44\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m folder_datamoduleA\u001b[38;5;241m.\u001b[39mtrain_dataloader()\n\u001b[1;32m     45\u001b[0m val_loader   \u001b[38;5;241m=\u001b[39m folder_datamoduleB\u001b[38;5;241m.\u001b[39mval_dataloader()\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/base/datamodule.py:138\u001b[0m, in \u001b[0;36mAnomalibDataModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    136\u001b[0m has_subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, subset) \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_data\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_subset \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_setup:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_test_split()\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_val_split()\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/image/folder.py:446\u001b[0m, in \u001b[0;36mFolder._setup\u001b[0;34m(self, _stage)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup\u001b[39m(\u001b[38;5;28mself\u001b[39m, _stage: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data \u001b[38;5;241m=\u001b[39m \u001b[43mFolderDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabnormal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabnormal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormal_test_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_test_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_data \u001b[38;5;241m=\u001b[39m FolderDataset(\n\u001b[1;32m    460\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    461\u001b[0m         task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         extensions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/image/folder.py:261\u001b[0m, in \u001b[0;36mFolderDataset.__init__\u001b[0;34m(self, name, task, normal_dir, transform, root, abnormal_dir, normal_test_dir, mask_dir, split, extensions)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_dir \u001b[38;5;241m=\u001b[39m mask_dir\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m \u001b[43mmake_folder_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mabnormal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabnormal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormal_test_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_test_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/image/folder.py:102\u001b[0m, in \u001b[0;36mmake_folder_dataset\u001b[0;34m(normal_dir, root, abnormal_dir, normal_test_dir, mask_dir, split, extensions)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# All paths are changed to the List[Path] type and used.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m normal_dir \u001b[38;5;241m=\u001b[39m _resolve_path_and_convert_to_list(normal_dir)\n\u001b[0;32m--> 102\u001b[0m abnormal_dir \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_path_and_convert_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabnormal_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m normal_test_dir \u001b[38;5;241m=\u001b[39m _resolve_path_and_convert_to_list(normal_test_dir)\n\u001b[1;32m    104\u001b[0m mask_dir \u001b[38;5;241m=\u001b[39m _resolve_path_and_convert_to_list(mask_dir)\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/image/folder.py:98\u001b[0m, in \u001b[0;36mmake_folder_dataset.<locals>._resolve_path_and_convert_to_list\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [validate_and_resolve_path(dir_path, root) \u001b[38;5;28;01mfor\u001b[39;00m dir_path \u001b[38;5;129;01min\u001b[39;00m path]\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mvalidate_and_resolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/utils/path.py:246\u001b[0m, in \u001b[0;36mvalidate_and_resolve_path\u001b[0;34m(folder, root, base_dir)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_and_resolve_path\u001b[39m(\n\u001b[1;32m    232\u001b[0m     folder: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path,\n\u001b[1;32m    233\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    234\u001b[0m     base_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Path:\n\u001b[1;32m    236\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate and resolve the path.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m        Path: Validated and resolved path.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidate_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/data/utils/path.py:216\u001b[0m, in \u001b[0;36mvalidate_path\u001b[0;34m(path, base_dir, should_exist, extensions)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    215\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Check the read and execute permissions\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (os\u001b[38;5;241m.\u001b[39maccess(path, os\u001b[38;5;241m.\u001b[39mR_OK) \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39maccess(path, os\u001b[38;5;241m.\u001b[39mX_OK)):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Path does not exist: /local_data/datasets/3-5-jing/abnormal_filtered"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.98,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.98,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal_filtered\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.04,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # 进行数据集分割\n",
    "folder_datamoduleB.setup()    # 进行数据集分割\n",
    "folder_datamoduleC.setup()    # 进行数据集分割\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> 分析训练集、验证集、测试集的数据量以及类别分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # 统计dataloader中样本的类别比例\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"label频率分布：{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([16, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({0: 100})\n",
      "image_paths(100)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/13__DA1479053.png', '/local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png', '/local_data/datasets/3-5-jing/normal/5__DA2951215.png', '/local_data/datasets/3-5-jing/normal/55__DA2951215.png', '/local_data/datasets/3-5-jing/normal/71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({0: 97, 1: 52})\n",
      "image_paths(149)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n"
     ]
    }
   ],
   "source": [
    "# Val images\n",
    "i, data = next(enumerate(val_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({1: 28, 0: 2})\n",
      "image_paths(30)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal_filtered/11__DA1479053 (3).png', '/local_data/datasets/3-5-jing/abnormal_filtered/11__DA1479053.png', '/local_data/datasets/3-5-jing/abnormal_filtered/11__DA2951175 (3).png', '/local_data/datasets/3-5-jing/abnormal_filtered/13__DA2951215 (2).png', '/local_data/datasets/3-5-jing/abnormal_filtered/13__DA2951215 (3).png']\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "i, data = next(enumerate(test_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> 查看图像内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前的数据 : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.9362553358078003, 0.06547410041093826;\n",
      "transform.2: -1.804444432258606, 2.517995834350586, -1.5134505033493042;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"处理前的数据 : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型选择和优化器配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing Fastflow model.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:anomalib.models.components.base.anomaly_module:No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | loss                  | FastflowLoss             | 0      | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "7 | model                 | FastflowModel            | 7.7 M  | train\n",
      "---------------------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.678    Total estimated model params size (MB)\n",
      "261       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601c217d049d4da2baa9c2d21a5f0f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7807119c514dea843e22d4e5f182d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:anomalib.callbacks.timer:Training took 43.99 seconds\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd633d91451c4739a1d58ea1b5985610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.callbacks.timer:Testing took 69.64969515800476 seconds\n",
      "Throughput (batch_size=8) : 2.1392771305313545 FPS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.981760561466217     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9444444179534912     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.981760561466217    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9444444179534912    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/projects/results/Fastflow/latest\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    model_checkpoint,\n",
    "    early_stopping,\n",
    "    graph_logger,\n",
    "]\n",
    "\n",
    "\n",
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, \n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    pixel_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    # callbacks= callbacks\n",
    "                    )\n",
    "    # engine.train(model=model, \n",
    "    #              train_dataloaders=train_loader_lst, \n",
    "    #              val_dataloaders=val_loader_lst, \n",
    "    #              test_dataloaders=test_loader_lst)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=test_loader)\n",
    "elif configs[\"model_name\"] == \"Fastflow\":\n",
    "    model = Fastflow()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"auto\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    max_epochs=1,                            #! 希望赋值给Lightning Trainer的参数必须全部放在已标明参数的最后面\n",
    "                    )\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "    \n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/Fastflow/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/Fastflow/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "# model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# openvino_model_path = model_output_path / \"weights\" / \"openvino\" / \"model.bin\"\n",
    "# metadata_path = model_output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n",
    "# print(openvino_model_path.exists(), metadata_path.exists())\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1__DA2951215 (3).png', '9__DA2951225 (2).png', '61__DA2951215.png', '9__DA2951225.png', '15__DA2951175 (2).png', '1__DA2951215.png', '13__DA2951215.png', '13__DA2951215 (3).png', '13__DA2951215 (2).png', '11__DA2951175 (3).png', '69__DA2951225.png', '19__DA1479053.png', '15__DA2951215 - 副本.png', '1__DA1479053 (4).png', '3__DA1479053.png', '15__DA2951175.png', '9__DA2951225 (3).png', '67__DA2951215.png', '9__DA2951225 (4).png', '3__DA1479053 (2).png', '71__DA2951175.png', '3__DA2951215.png', '57__DA2951175.png', '15__DA2951215 (2).png', '65__DA1479053.png', '11__DA1479053.png', '3__DA1479053 (3).png', '13__DA2951225 (2).png', '11__DA1479053 (3).png']\n",
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/1__DA2951215 (3).png;\n",
      "0.5971561961839659 LabelName.ABNORMAL\n",
      "Prediction took 0.9854 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225 (2).png;\n",
      "0.8132488163764002 LabelName.ABNORMAL\n",
      "Prediction took 0.8697 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.987191e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/61__DA2951215.png;\n",
      "0.6961009147738344 LabelName.ABNORMAL\n",
      "Prediction took 0.9624 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225.png;\n",
      "0.7041936876918945 LabelName.ABNORMAL\n",
      "Prediction took 0.8351 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.222311e-08..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951175 (2).png;\n",
      "0.6754931721112475 LabelName.ABNORMAL\n",
      "Prediction took 0.8895 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/1__DA2951215.png;\n",
      "0.5421393700331256 LabelName.ABNORMAL\n",
      "Prediction took 0.9354 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.6752104e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951215.png;\n",
      "0.6030783166476199 LabelName.ABNORMAL\n",
      "Prediction took 1.1184 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-9.630957e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951215 (3).png;\n",
      "0.6124779469462774 LabelName.ABNORMAL\n",
      "Prediction took 1.0415 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.2045656e-10..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951215 (2).png;\n",
      "0.6030783166476199 LabelName.ABNORMAL\n",
      "Prediction took 1.0608 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-9.630957e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/11__DA2951175 (3).png;\n",
      "0.9077558285891838 LabelName.ABNORMAL\n",
      "Prediction took 1.0704 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/69__DA2951225.png;\n",
      "0.4352237430417678 LabelName.NORMAL\n",
      "Prediction took 1.0536 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/19__DA1479053.png;\n",
      "0.37838906063207345 LabelName.NORMAL\n",
      "Prediction took 1.0144 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-6.590362e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951215 - 副本.png;\n",
      "0.7580850576407577 LabelName.ABNORMAL\n",
      "Prediction took 1.0715 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8904487e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/1__DA1479053 (4).png;\n",
      "0.596735094768884 LabelName.ABNORMAL\n",
      "Prediction took 0.7887 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.691003e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA1479053.png;\n",
      "0.8268994854726331 LabelName.ABNORMAL\n",
      "Prediction took 0.8108 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2885391e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951175.png;\n",
      "0.6754931721112475 LabelName.ABNORMAL\n",
      "Prediction took 0.9233 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225 (3).png;\n",
      "0.7041936876918945 LabelName.ABNORMAL\n",
      "Prediction took 0.8357 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.222311e-08..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/67__DA2951215.png;\n",
      "0.34311165296524704 LabelName.NORMAL\n",
      "Prediction took 0.9088 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/9__DA2951225 (4).png;\n",
      "0.6334254777942206 LabelName.ABNORMAL\n",
      "Prediction took 0.8374 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.800192e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA1479053 (2).png;\n",
      "0.8268994854726331 LabelName.ABNORMAL\n",
      "Prediction took 0.9790 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2885391e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/71__DA2951175.png;\n",
      "0.7027006630398719 LabelName.ABNORMAL\n",
      "Prediction took 0.9685 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA2951215.png;\n",
      "0.6157166255009664 LabelName.ABNORMAL\n",
      "Prediction took 1.0508 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/57__DA2951175.png;\n",
      "0.5729443920730134 LabelName.ABNORMAL\n",
      "Prediction took 1.1356 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0350673e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/15__DA2951215 (2).png;\n",
      "0.7580850576407577 LabelName.ABNORMAL\n",
      "Prediction took 1.0783 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8904487e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/65__DA1479053.png;\n",
      "0.6047196958126847 LabelName.ABNORMAL\n",
      "Prediction took 0.8735 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.4684586e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/11__DA1479053.png;\n",
      "0.9443510609164592 LabelName.ABNORMAL\n",
      "Prediction took 1.0117 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.684697e-11..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/3__DA1479053 (3).png;\n",
      "0.8268994854726331 LabelName.ABNORMAL\n",
      "Prediction took 1.0946 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2885391e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/13__DA2951225 (2).png;\n",
      "0.7813827198854031 LabelName.ABNORMAL\n",
      "Prediction took 0.8409 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.491868e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/test/11__DA1479053 (3).png;\n",
      "0.9443510609164592 LabelName.ABNORMAL\n",
      "Prediction took 0.7612 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.684697e-11..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "0.15258617148454573 LabelName.NORMAL\n",
      "Prediction took 0.8689 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "0.13462447928461213 LabelName.NORMAL\n",
      "Prediction took 0.7791 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [2.865513e-08..1.0000001].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "0.4438535835224849 LabelName.NORMAL\n",
      "Prediction took 0.9043 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.574201e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "0.1877837405138954 LabelName.NORMAL\n",
      "Prediction took 0.9576 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4916407e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "0.27357514890748513 LabelName.NORMAL\n",
      "Prediction took 0.9334 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "0.9015370254314499 LabelName.ABNORMAL\n",
      "Prediction took 0.8949 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.9404 seconds.\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-5.6794747e-10..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "0.8170319617869952 LabelName.ABNORMAL\n",
      "Prediction took 0.9141 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.673895e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "0.8186749472043311 LabelName.ABNORMAL\n",
      "Prediction took 0.9514 seconds.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.16941745e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "0.8667581616047777 LabelName.ABNORMAL\n",
      "Prediction took 1.0324 seconds.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 待测试图像\n",
    "test_png_files = [f for f in os.listdir(test_folder_path) if f.endswith('.png')]\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "\n",
    "print(test_png_files)\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# 输出路径确认\n",
    "if os.path.exists(test_output_path):     shutil.rmtree(test_output_path)\n",
    "if os.path.exists(normal_ouput_path):    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(test_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# 模型测试\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
