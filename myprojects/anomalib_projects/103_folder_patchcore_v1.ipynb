{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/anomalib/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"Patchcore\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path   = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "test_folder_path     = os.path.join(configs[\"dataset_root\"], \"test\")\n",
    "\n",
    "normal_ouput_path    = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")\n",
    "test_output_path     = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"test_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) 的tensor类型\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # 提取 B 通道（张量的第三个通道，索引为2）\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # 不能添加\n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1之间\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),                                               # 如果resizeHW不一致，会引起fastflow报layernorm错误\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # 无seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx 不支持 grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        #Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        #Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 读取图像\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # 图像transform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # 图像推理\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        tmp = cv2.resize(tmp, (256, 256))\n",
    "        predictions = inferencer.predict(image=tmp)        #! 注：如果使用vino，输入的image参数如果不是path，那么其shape只能是HWC\n",
    "        #predictions = torch_inferencer.predict(image=transform_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! 🎯 inferencer.predict接受原始图像，\n",
    "        #! 内部通过metadata和model在forward函数(非pre_process函数)中调用标准化的transform对图像处理;\n",
    "        #! 见/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! 注: 1. 模型导出时，生成的bin/onnx、json等文件中均不包含训练时使用到的transform操作，包括数据增强相关的操作和标准化相关的操作\n",
    "        #! 注: 2. 而predict推理时，会进行的预处理操作是标准化操作，比如normalize，但是这里的normalize是除以255的方式，而不是标准正太分布。\n",
    "        #! 通过以上总结，可知，我们需要1. 修改训练时的normalize；2. 将ExtractBChannel操作要出现在训练时的transform以外，还需要封装成一个数据预处理操作。在模型推理之前对图像进行额外的预处理操作。\n",
    "        \n",
    "        # 记录结束时间\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # 计算耗时\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # 可视化\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 创建一个新的图形窗口\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # 原始图像\n",
    "        tmp0 = cv2.normalize(image, None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[0].imshow(tmp0)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 热图\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! 热力图是anomaly_map与原始图像的加权结合\n",
    "        axs[3].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 预测掩模\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 预测掩模\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # 关闭坐标轴 \n",
    "        \n",
    "\n",
    "        # 添加文本信息到图形的上方中间位置\n",
    "        fig_text_x = 0.1   # x坐标在图形宽度的中心位置\n",
    "        fig_text_y = 0.95  # y坐标稍微靠近图形的顶部，避免与子图重叠\n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n' \n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # 显示整个图形\n",
    "        plt.tight_layout()  # 调整子图间的间距\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom 数据集配置\n",
    "\n",
    "Folder的理解:\n",
    "\n",
    "1. 训练集中全部是正常样本；\n",
    "2. 正常样本集中的部分(normal_split_ratio)样本被均等(val_split_ratio)放入到验证集和测试集中。\n",
    "3. test_split_ratio似乎和normal_split_ratio一样。\n",
    "4. 🎯 需要确保训练集不存在正常样本；测试集不存在样本；验证集是全部正样本和全部负样本。\n",
    "\n",
    "比如：val_split_ratio=0.5时：82 | 27, 10 | 27, 10|; val_split_ratio=0.1时：82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "WARNING:anomalib.data.utils.split:Zero subset length encountered during splitting. This means one of your subsets\n",
      "            might be empty or devoid of either normal or anomalous images.\n"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.98,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.98,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"test\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 8,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.04,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # 进行数据集分割\n",
    "folder_datamoduleB.setup()    # 进行数据集分割\n",
    "folder_datamoduleC.setup()    # 进行数据集分割\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> 分析训练集、验证集、测试集的数据量以及类别分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # 统计dataloader中样本的类别比例\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"label频率分布：{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([16, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({0: 100})\n",
      "image_paths(100)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/13__DA1479053.png', '/local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png', '/local_data/datasets/3-5-jing/normal/5__DA2951215.png', '/local_data/datasets/3-5-jing/normal/55__DA2951215.png', '/local_data/datasets/3-5-jing/normal/71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({0: 97, 1: 52})\n",
      "image_paths(149)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n"
     ]
    }
   ],
   "source": [
    "# Val images\n",
    "i, data = next(enumerate(val_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([8, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({1: 28, 0: 2})\n",
      "image_paths(30)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/53__DA1479053.png', '/local_data/datasets/3-5-jing/normal/5__DA1479053 (2).png', '/local_data/datasets/3-5-jing/test/11__DA1479053 (3).png', '/local_data/datasets/3-5-jing/test/11__DA1479053.png', '/local_data/datasets/3-5-jing/test/11__DA2951175 (3).png']\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "i, data = next(enumerate(test_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> 查看图像内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前的数据 : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.9362553358078003, 0.06547410041093826;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"处理前的数据 : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型选择和优化器配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/wide_resnet50_2.racm_in1k)\n",
      "INFO:timm.models._hub:[timm/wide_resnet50_2.racm_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "---------------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "174       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fc51d7f59b4b268ac6708838ecdfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5371eb19144c1f9bc7f154c124637c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.patchcore.lightning_model:Aggregating the embedding extracted from the training set.\n",
      "INFO:anomalib.models.image.patchcore.lightning_model:Applying core-set subsampling to get the embedding.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Selecting Coreset Indices.: 100%|██████████| 10240/10240 [00:13<00:00, 751.00it/s]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:anomalib.callbacks.timer:Training took 67.52 seconds\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56409166c3eb43c0bf45481967081043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.callbacks.timer:Testing took 17.742172479629517 seconds\n",
      "Throughput (batch_size=8) : 1.690886504143964 FPS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8214285373687744     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9056603908538818     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8214285373687744    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9056603908538818    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/projects/results/Patchcore/latest\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    model_checkpoint,\n",
    "    early_stopping,\n",
    "    graph_logger,\n",
    "]\n",
    "\n",
    "\n",
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, \n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"auto\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    )\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=test_loader)\n",
    "elif configs[\"model_name\"] == \"Fastflow\":\n",
    "    model = Fastflow()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"auto\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    max_epochs=1,                            #! 希望赋值给Lightning Trainer的参数必须全部放在已标明参数的最后面\n",
    "                    )\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=test_loader)\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)\n",
    "    engine.train(model=model, \n",
    "                 train_dataloaders=train_loader, \n",
    "                 val_dataloaders=val_loader, \n",
    "                 test_dataloaders=val_loader)\n",
    "    \n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Patchcore/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Patchcore/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/Patchcore/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/Patchcore/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO, transform=train_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX, transform=train_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH, transform=train_transform)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "# model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# openvino_model_path = model_output_path / \"weights\" / \"openvino\" / \"model.bin\"\n",
    "# metadata_path = model_output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n",
    "# print(openvino_model_path.exists(), metadata_path.exists())\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      ExtractBChannel()\n",
       "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1__DA2951215 (3).png', '9__DA2951225 (2).png', '61__DA2951215.png', '9__DA2951225.png', '15__DA2951175 (2).png', '1__DA2951215.png', '13__DA2951215.png', '13__DA2951215 (3).png', '13__DA2951215 (2).png', '11__DA2951175 (3).png', '69__DA2951225.png', '19__DA1479053.png', '15__DA2951215 - 副本.png', '1__DA1479053 (4).png', '3__DA1479053.png', '15__DA2951175.png', '9__DA2951225 (3).png', '67__DA2951215.png', '9__DA2951225 (4).png', '3__DA1479053 (2).png', '71__DA2951175.png', '3__DA2951215.png', '57__DA2951175.png', '15__DA2951215 (2).png', '65__DA1479053.png', '11__DA1479053.png', '3__DA1479053 (3).png', '13__DA2951225 (2).png', '11__DA1479053 (3).png']\n",
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "0.6238592445009843 LabelName.ABNORMAL\n",
      "Prediction took 1.0045 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "0.5972995804663459 LabelName.ABNORMAL\n",
      "Prediction took 0.7383 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "0.5929760769177883 LabelName.ABNORMAL\n",
      "Prediction took 0.6830 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "0.6767593438841826 LabelName.ABNORMAL\n",
      "Prediction took 0.6599 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "0.6620750869594123 LabelName.ABNORMAL\n",
      "Prediction took 0.6857 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6921 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "0.8809683597655236 LabelName.ABNORMAL\n",
      "Prediction took 0.6670 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6295 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6468 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "0.996506887729979 LabelName.ABNORMAL\n",
      "Prediction took 0.6433 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 待测试图像\n",
    "test_png_files = [f for f in os.listdir(test_folder_path) if f.endswith('.png')]\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "\n",
    "print(test_png_files)\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# 输出路径确认\n",
    "if os.path.exists(test_output_path):     shutil.rmtree(test_output_path)\n",
    "if os.path.exists(normal_ouput_path):    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(test_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# 模型测试\n",
    "# draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
