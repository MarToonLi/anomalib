{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数：批量图像可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [确定] 数据集目录\\输出目录\\模型选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/myprojects/anomalib_projects/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"Patchcore\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path   = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "test_folder_path     = os.path.join(configs[\"dataset_root\"], \"test\")\n",
    "\n",
    "normal_ouput_path    = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")\n",
    "test_output_path     = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"test_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [确定] 数据预处理操作Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) 的tensor类型\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # 提取 B 通道（张量的第三个通道，索引为2）\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # 不能添加\n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1之间\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),                                               # 如果resizeHW不一致，会引起fastflow报layernorm错误\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # 无seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx 不支持 grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集\n",
    "\n",
    "Folder的理解:\n",
    "\n",
    "1. 训练集中全部是正常样本；\n",
    "2. 正常样本集中的部分(normal_split_ratio)样本被均等(val_split_ratio)放入到验证集和测试集中。\n",
    "3. test_split_ratio似乎和normal_split_ratio一样。\n",
    "4. 🎯 需要确保训练集不存在正常样本；测试集不存在样本；验证集是全部正样本和全部负样本。\n",
    "\n",
    "比如：val_split_ratio=0.5时：82 | 27, 10 | 27, 10|; val_split_ratio=0.1时：82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [确定] 构建训练集、测试集和验证集\n",
    "\n",
    "**理念是：训练集只包含正常图像，测试集包含几乎所有的正常图像和缺陷图像。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.10\n"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 32, eval_batch_size = 32,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.98,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 32, eval_batch_size = 32,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.98,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"test\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 32, eval_batch_size = 32,              #! 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.1,    #! 控制正常样本，在非训练集和训练集中的数量比例\n",
    "    val_split_ratio=0.1,     #! 控制剩余正常样本和异常样本，在验证集和测试集中的数量比例\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # 进行数据集分割\n",
    "folder_datamoduleB.setup()    # 进行数据集分割\n",
    "folder_datamoduleC.setup()    # 进行数据集分割\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析：训练集、验证集、测试集的数据量以及类别分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # 统计dataloader中样本的类别比例\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"label频率分布：{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([32, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({0: 100})\n",
      "image_paths(100)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/13__DA1479053.png', '/local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png', '/local_data/datasets/3-5-jing/normal/5__DA2951215.png', '/local_data/datasets/3-5-jing/normal/55__DA2951215.png', '/local_data/datasets/3-5-jing/normal/71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([32, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({0: 97, 1: 52})\n",
      "image_paths(149)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n"
     ]
    }
   ],
   "source": [
    "# Val images\n",
    "i, data = next(enumerate(val_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([32, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "label频率分布：Counter({1: 27, 0: 9})\n",
      "image_paths(36)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/11__DA1479053.png', '/local_data/datasets/3-5-jing/normal/3__DA2951175 (3).png', '/local_data/datasets/3-5-jing/normal/53__DA1479053.png', '/local_data/datasets/3-5-jing/normal/55__DA1479053.png', '/local_data/datasets/3-5-jing/normal/55__DA2951225.png']\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "i, data = next(enumerate(test_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析：图像内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n",
      "image.0: -1.804444432258606, 1.9634921550750732, -1.5363503694534302;\n",
      "image.1: -1.804444432258606, 2.1835505962371826, -1.584610939025879;\n",
      "image.2: -1.804444432258606, 2.422900676727295, -1.494896411895752;\n",
      "image.3: -1.804444432258606, 2.142817974090576, -1.5176782608032227;\n",
      "image.4: -1.804444432258606, 2.2089078426361084, -1.5385369062423706;\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    print(\"image.{}: {}, {}, {};\".format(i, test_img.min(), test_img.max(), test_img.mean()))\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析：图像经过tranform后的数据范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前的数据 : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.9362553358078003, 0.06547410041093826;\n",
      "transform.2: -1.804444432258606, 2.517995834350586, -1.5134505033493042;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"处理前的数据 : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型与优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "\n",
    "# ['train_loss', 'train_loss_step', 'image_AUROC', 'train_loss_epoch', 'epoch', 'step']\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [确定] 模型、优化器和callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/wide_resnet50_2.racm_in1k)\n",
      "INFO:timm.models._hub:[timm/wide_resnet50_2.racm_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    callbacks = [ model_checkpoint, \n",
    "                 #early_stopping, \n",
    "                 graph_logger]\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, \n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    pixel_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    # callbacks= callbacks\n",
    "                    )\n",
    "elif configs[\"model_name\"] == \"Fastflow\":\n",
    "    callbacks = [ model_checkpoint, \n",
    "                 #early_stopping, \n",
    "                 graph_logger]\n",
    "    model = Fastflow()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"gpu\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    max_epochs=50,                            #! 希望赋值给Lightning Trainer的参数必须全部放在已标明参数的最后面\n",
    "                    )\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "---------------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "174       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f960aa391f1b49b7a032f2678406942a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5838268b628e457ebc470fa87eae94de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.patchcore.lightning_model:Aggregating the embedding extracted from the training set.\n",
      "INFO:anomalib.models.image.patchcore.lightning_model:Applying core-set subsampling to get the embedding.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Selecting Coreset Indices.: 100%|██████████| 10240/10240 [00:13<00:00, 755.67it/s]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:anomalib.callbacks.timer:Training took 70.01 seconds\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acef3c987074a23b16c9a0059520827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.callbacks.timer:Testing took 94.52934694290161 seconds\n",
      "Throughput (batch_size=32) : 1.576230079003933 FPS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.999801754951477     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9807692170143127     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.999801754951477    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9807692170143127    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/projects/results/Patchcore/latest\n"
     ]
    }
   ],
   "source": [
    "engine.train(model=model, \n",
    "                train_dataloaders=train_loader, \n",
    "                val_dataloaders=val_loader, \n",
    "                test_dataloaders=val_loader)\n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型导出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导出openvino模型\\ONNX模型\\Torch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Patchcore/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Patchcore/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/Patchcore/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/Patchcore/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "\n",
    "#! 🎯 模型在导出时可以指定transform，而transform因为继承自torch.nn.Module类型，且实现基于torch自身算子，因此它可以融入在模型文件pt或者onnx文件中。\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH, transform=eval_transform)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成导出模型的保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "from pathlib import Path\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "#model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path       = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path     = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 执行模型推理与可视化推理结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer: TorchInferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 读取图像\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # 图像transform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # 图像推理\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        \n",
    "        tmp = cv2.resize(tmp, (256, 256))\n",
    "        predictions = inferencer.predict(image=tmp)        #! 注：如果使用vino，输入的image参数如果不是path，那么其shape只能是HWC\n",
    "        #predictions = torch_inferencer.predict(image=filter_image)\n",
    "        #predictions = torch_inferencer.predict(image=CHW_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! 🎯 inferencer.predict接受原始图像，\n",
    "        #! 内部通过metadata和model在forward函数(非pre_process函数)中调用标准化的transform对图像处理;\n",
    "        #! 见/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! 注: 1. 模型导出时，生成的bin/onnx、json等文件中均不包含训练时使用到的transform操作，包括数据增强相关的操作和标准化相关的操作\n",
    "        #! 注: 2. 而predict推理时，会进行的预处理操作是标准化操作，比如normalize，但是这里的normalize是除以255的方式，而不是标准正太分布。\n",
    "        #! 通过以上总结，可知，我们需要1. 修改训练时的normalize；2. 将ExtractBChannel操作要出现在训练时的transform以外，还需要封装成一个数据预处理操作。在模型推理之前对图像进行额外的预处理操作。\n",
    "        \n",
    "        # 记录结束时间\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # 计算耗时\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # 可视化\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 创建一个新的图形窗口\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # 原始图像\n",
    "        tmp0 = cv2.normalize(image, None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[0].imshow(tmp0)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 热图\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! 热力图是anomaly_map与原始图像的加权结合\n",
    "        axs[3].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 预测掩模\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 预测掩模\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # 关闭坐标轴 \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # 添加文本信息到图形的上方中间位置\n",
    "        fig_text_x = 0.1   # x坐标在图形宽度的中心位置\n",
    "        fig_text_y = 0.95  # y坐标稍微靠近图形的顶部，避免与子图重叠\n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n' \n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # 显示整个图形\n",
    "        plt.tight_layout()  # 调整子图间的间距\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载openvino模型\\torch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[input] shape[?,3,?,?] type: f32>\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(inferencer.input_blob)\n",
    "print(inferencer.input_blob.partial_shape[2].is_static)\n",
    "print(inferencer.input_blob.partial_shape[3].is_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      ExtractBChannel()\n",
       "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       "      Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225], inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 执行模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['71__DA2951175 - 副本 (2) (2).png', '71__DA2951175 - 副本 (2).png', '69__DA2951225.png', '71__DA2951175 - 副本.png', '69__DA2951225 - 副本.png', '69__DA2951225 - 副本 (2).png', '71__DA2951175.png', '71__DA2951175 - 副本 (2) (2) 2.png']\n",
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - 副本 (2) (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2950 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - 副本 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.8282 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2755395e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225.png;\n",
      "0.9964976658722143 LabelName.ABNORMAL\n",
      "Prediction took 0.7232 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.01338e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - 副本.png;\n",
      "0.6976840267723647 LabelName.ABNORMAL\n",
      "Prediction took 0.7511 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225 - 副本.png;\n",
      "0.6859768710099958 LabelName.ABNORMAL\n",
      "Prediction took 0.7583 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225 - 副本 (2).png;\n",
      "0.9330881567889115 LabelName.ABNORMAL\n",
      "Prediction took 0.7451 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.8406 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2755395e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - 副本 (2) (2) 2.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.8100 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "0.5744920981974643 LabelName.ABNORMAL\n",
      "Prediction took 0.7451 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "0.7498829924195373 LabelName.ABNORMAL\n",
      "Prediction took 0.7326 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [2.865513e-08..1.0000001].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "0.6069605904842698 LabelName.ABNORMAL\n",
      "Prediction took 0.7114 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.574201e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "0.5140061791967044 LabelName.ABNORMAL\n",
      "Prediction took 0.7309 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4916407e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "0.6181693890887368 LabelName.ABNORMAL\n",
      "Prediction took 0.7150 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.7167 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "0.8263131000714965 LabelName.ABNORMAL\n",
      "Prediction took 0.7284 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-5.6794747e-10..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6987 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.673895e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "0.9873399584314628 LabelName.ABNORMAL\n",
      "Prediction took 0.6896 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.16941745e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "0.9416600993141565 LabelName.ABNORMAL\n",
      "Prediction took 0.7104 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 待测试图像\n",
    "test_folder_path = r\"/home/projects/myprojects/anomalib_projects/datasets/test_1122\"\n",
    "test_output_path = r\"/home/projects/myprojects/anomalib_projects/datasets/test_1122/{}_output\".format(configs[\"model_name\"])\n",
    "\n",
    "test_png_files = [f for f in os.listdir(test_folder_path) if f.endswith('.png')]\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "\n",
    "print(test_png_files)\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# 输出路径确认\n",
    "if os.path.exists(test_output_path):     shutil.rmtree(test_output_path)\n",
    "if os.path.exists(normal_ouput_path):    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(test_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# 模型测试\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
