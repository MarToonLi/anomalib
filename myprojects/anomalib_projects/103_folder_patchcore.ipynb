{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸºæœ¬é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‡½æ•°ï¼šæ‰¹é‡å›¾åƒå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ•°æ®é›†ç›®å½•\\è¾“å‡ºç›®å½•\\æ¨¡å‹é€‰æ‹©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/myprojects/anomalib_projects/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"Patchcore\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path   = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "test_folder_path     = os.path.join(configs[\"dataset_root\"], \"test\")\n",
    "\n",
    "normal_ouput_path    = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")\n",
    "test_output_path     = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"test_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ•°æ®é¢„å¤„ç†æ“ä½œTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) çš„tensorç±»å‹\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # æå– B é€šé“ï¼ˆå¼ é‡çš„ç¬¬ä¸‰ä¸ªé€šé“ï¼Œç´¢å¼•ä¸º2ï¼‰\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # ä¸èƒ½æ·»åŠ \n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1ä¹‹é—´\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),                                               # å¦‚æœresizeHWä¸ä¸€è‡´ï¼Œä¼šå¼•èµ·fastflowæŠ¥layernormé”™è¯¯\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # æ— seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx ä¸æ”¯æŒ grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ•°æ®é›†\n",
    "\n",
    "Folderçš„ç†è§£:\n",
    "\n",
    "1. è®­ç»ƒé›†ä¸­å…¨éƒ¨æ˜¯æ­£å¸¸æ ·æœ¬ï¼›\n",
    "2. æ­£å¸¸æ ·æœ¬é›†ä¸­çš„éƒ¨åˆ†(normal_split_ratio)æ ·æœ¬è¢«å‡ç­‰(val_split_ratio)æ”¾å…¥åˆ°éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­ã€‚\n",
    "3. test_split_ratioä¼¼ä¹å’Œnormal_split_ratioä¸€æ ·ã€‚\n",
    "4. ğŸ¯ éœ€è¦ç¡®ä¿è®­ç»ƒé›†ä¸å­˜åœ¨æ­£å¸¸æ ·æœ¬ï¼›æµ‹è¯•é›†ä¸å­˜åœ¨æ ·æœ¬ï¼›éªŒè¯é›†æ˜¯å…¨éƒ¨æ­£æ ·æœ¬å’Œå…¨éƒ¨è´Ÿæ ·æœ¬ã€‚\n",
    "\n",
    "æ¯”å¦‚ï¼šval_split_ratio=0.5æ—¶ï¼š82 | 27, 10 | 27, 10|; val_split_ratio=0.1æ—¶ï¼š82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ„å»ºè®­ç»ƒé›†ã€æµ‹è¯•é›†å’ŒéªŒè¯é›†\n",
    "\n",
    "**ç†å¿µæ˜¯ï¼šè®­ç»ƒé›†åªåŒ…å«æ­£å¸¸å›¾åƒï¼Œæµ‹è¯•é›†åŒ…å«å‡ ä¹æ‰€æœ‰çš„æ­£å¸¸å›¾åƒå’Œç¼ºé™·å›¾åƒã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.10\n"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 32, eval_batch_size = 32,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 32, eval_batch_size = 32,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"test\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 32, eval_batch_size = 32,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.1,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.1,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleB.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleC.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„æ•°æ®é‡ä»¥åŠç±»åˆ«åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # ç»Ÿè®¡dataloaderä¸­æ ·æœ¬çš„ç±»åˆ«æ¯”ä¾‹\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"labelé¢‘ç‡åˆ†å¸ƒï¼š{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([32, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({0: 100})\n",
      "image_paths(100)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/13__DA1479053.png', '/local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png', '/local_data/datasets/3-5-jing/normal/5__DA2951215.png', '/local_data/datasets/3-5-jing/normal/55__DA2951215.png', '/local_data/datasets/3-5-jing/normal/71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([32, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({0: 97, 1: 52})\n",
      "image_paths(149)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n"
     ]
    }
   ],
   "source": [
    "# Val images\n",
    "i, data = next(enumerate(val_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([32, 3, 256, 256]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({1: 27, 0: 9})\n",
      "image_paths(36)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/11__DA1479053.png', '/local_data/datasets/3-5-jing/normal/3__DA2951175 (3).png', '/local_data/datasets/3-5-jing/normal/53__DA1479053.png', '/local_data/datasets/3-5-jing/normal/55__DA1479053.png', '/local_data/datasets/3-5-jing/normal/55__DA2951225.png']\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "i, data = next(enumerate(test_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æï¼šå›¾åƒå†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n",
      "image.0: -1.804444432258606, 1.9634921550750732, -1.5363503694534302;\n",
      "image.1: -1.804444432258606, 2.1835505962371826, -1.584610939025879;\n",
      "image.2: -1.804444432258606, 2.422900676727295, -1.494896411895752;\n",
      "image.3: -1.804444432258606, 2.142817974090576, -1.5176782608032227;\n",
      "image.4: -1.804444432258606, 2.2089078426361084, -1.5385369062423706;\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    print(\"image.{}: {}, {}, {};\".format(i, test_img.min(), test_img.max(), test_img.mean()))\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æï¼šå›¾åƒç»è¿‡tranformåçš„æ•°æ®èŒƒå›´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å‰çš„æ•°æ® : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.9362553358078003, 0.06547410041093826;\n",
      "transform.2: -1.804444432258606, 2.517995834350586, -1.5134505033493042;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"å¤„ç†å‰çš„æ•°æ® : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹ä¸ä¼˜åŒ–å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "\n",
    "# ['train_loss', 'train_loss_step', 'image_AUROC', 'train_loss_epoch', 'epoch', 'step']\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œcallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/wide_resnet50_2.racm_in1k)\n",
      "INFO:timm.models._hub:[timm/wide_resnet50_2.racm_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    callbacks = [ model_checkpoint, \n",
    "                 #early_stopping, \n",
    "                 graph_logger]\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, \n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    pixel_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    # callbacks= callbacks\n",
    "                    )\n",
    "elif configs[\"model_name\"] == \"Fastflow\":\n",
    "    callbacks = [ model_checkpoint, \n",
    "                 #early_stopping, \n",
    "                 graph_logger]\n",
    "    model = Fastflow()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"gpu\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    max_epochs=50,                            #! å¸Œæœ›èµ‹å€¼ç»™Lightning Trainerçš„å‚æ•°å¿…é¡»å…¨éƒ¨æ”¾åœ¨å·²æ ‡æ˜å‚æ•°çš„æœ€åé¢\n",
    "                    )\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "---------------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "174       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f960aa391f1b49b7a032f2678406942a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5838268b628e457ebc470fa87eae94de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.patchcore.lightning_model:Aggregating the embedding extracted from the training set.\n",
      "INFO:anomalib.models.image.patchcore.lightning_model:Applying core-set subsampling to get the embedding.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Selecting Coreset Indices.: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10240/10240 [00:13<00:00, 755.67it/s]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:anomalib.callbacks.timer:Training took 70.01 seconds\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acef3c987074a23b16c9a0059520827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.callbacks.timer:Testing took 94.52934694290161 seconds\n",
      "Throughput (batch_size=32) : 1.576230079003933 FPS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     0.999801754951477     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.9807692170143127     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    0.999801754951477    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.9807692170143127    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/projects/results/Patchcore/latest\n"
     ]
    }
   ],
   "source": [
    "engine.train(model=model, \n",
    "                train_dataloaders=train_loader, \n",
    "                val_dataloaders=val_loader, \n",
    "                test_dataloaders=val_loader)\n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹å¯¼å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯¼å‡ºopenvinoæ¨¡å‹\\ONNXæ¨¡å‹\\Torchæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Patchcore/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/Patchcore/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/Patchcore/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/Patchcore/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "\n",
    "#! ğŸ¯ æ¨¡å‹åœ¨å¯¼å‡ºæ—¶å¯ä»¥æŒ‡å®štransformï¼Œè€Œtransformå› ä¸ºç»§æ‰¿è‡ªtorch.nn.Moduleç±»å‹ï¼Œä¸”å®ç°åŸºäºtorchè‡ªèº«ç®—å­ï¼Œå› æ­¤å®ƒå¯ä»¥èå…¥åœ¨æ¨¡å‹æ–‡ä»¶ptæˆ–è€…onnxæ–‡ä»¶ä¸­ã€‚\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH, transform=eval_transform)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç”Ÿæˆå¯¼å‡ºæ¨¡å‹çš„ä¿å­˜è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "from pathlib import Path\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "#model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path       = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path     = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰§è¡Œæ¨¡å‹æ¨ç†ä¸å¯è§†åŒ–æ¨ç†ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer: TorchInferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # è®°å½•å¼€å§‹æ—¶é—´\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # è¯»å–å›¾åƒ\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # å›¾åƒtransform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # å›¾åƒæ¨ç†\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        \n",
    "        tmp = cv2.resize(tmp, (256, 256))\n",
    "        predictions = inferencer.predict(image=tmp)        #! æ³¨ï¼šå¦‚æœä½¿ç”¨vinoï¼Œè¾“å…¥çš„imageå‚æ•°å¦‚æœä¸æ˜¯pathï¼Œé‚£ä¹ˆå…¶shapeåªèƒ½æ˜¯HWC\n",
    "        #predictions = torch_inferencer.predict(image=filter_image)\n",
    "        #predictions = torch_inferencer.predict(image=CHW_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! ğŸ¯ inferencer.predictæ¥å—åŸå§‹å›¾åƒï¼Œ\n",
    "        #! å†…éƒ¨é€šè¿‡metadataå’Œmodelåœ¨forwardå‡½æ•°(épre_processå‡½æ•°)ä¸­è°ƒç”¨æ ‡å‡†åŒ–çš„transformå¯¹å›¾åƒå¤„ç†;\n",
    "        #! è§/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! æ³¨: 1. æ¨¡å‹å¯¼å‡ºæ—¶ï¼Œç”Ÿæˆçš„bin/onnxã€jsonç­‰æ–‡ä»¶ä¸­å‡ä¸åŒ…å«è®­ç»ƒæ—¶ä½¿ç”¨åˆ°çš„transformæ“ä½œï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºç›¸å…³çš„æ“ä½œå’Œæ ‡å‡†åŒ–ç›¸å…³çš„æ“ä½œ\n",
    "        #! æ³¨: 2. è€Œpredictæ¨ç†æ—¶ï¼Œä¼šè¿›è¡Œçš„é¢„å¤„ç†æ“ä½œæ˜¯æ ‡å‡†åŒ–æ“ä½œï¼Œæ¯”å¦‚normalizeï¼Œä½†æ˜¯è¿™é‡Œçš„normalizeæ˜¯é™¤ä»¥255çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯æ ‡å‡†æ­£å¤ªåˆ†å¸ƒã€‚\n",
    "        #! é€šè¿‡ä»¥ä¸Šæ€»ç»“ï¼Œå¯çŸ¥ï¼Œæˆ‘ä»¬éœ€è¦1. ä¿®æ”¹è®­ç»ƒæ—¶çš„normalizeï¼›2. å°†ExtractBChannelæ“ä½œè¦å‡ºç°åœ¨è®­ç»ƒæ—¶çš„transformä»¥å¤–ï¼Œè¿˜éœ€è¦å°è£…æˆä¸€ä¸ªæ•°æ®é¢„å¤„ç†æ“ä½œã€‚åœ¨æ¨¡å‹æ¨ç†ä¹‹å‰å¯¹å›¾åƒè¿›è¡Œé¢å¤–çš„é¢„å¤„ç†æ“ä½œã€‚\n",
    "        \n",
    "        # è®°å½•ç»“æŸæ—¶é—´\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # è®¡ç®—è€—æ—¶\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢çª—å£\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # åŸå§‹å›¾åƒ\n",
    "        tmp0 = cv2.normalize(image, None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[0].imshow(tmp0)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # çƒ­å›¾\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! çƒ­åŠ›å›¾æ˜¯anomaly_mapä¸åŸå§‹å›¾åƒçš„åŠ æƒç»“åˆ\n",
    "        axs[3].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # å…³é—­åæ ‡è½´ \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # æ·»åŠ æ–‡æœ¬ä¿¡æ¯åˆ°å›¾å½¢çš„ä¸Šæ–¹ä¸­é—´ä½ç½®\n",
    "        fig_text_x = 0.1   # xåæ ‡åœ¨å›¾å½¢å®½åº¦çš„ä¸­å¿ƒä½ç½®\n",
    "        fig_text_y = 0.95  # yåæ ‡ç¨å¾®é è¿‘å›¾å½¢çš„é¡¶éƒ¨ï¼Œé¿å…ä¸å­å›¾é‡å \n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n' \n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # æ˜¾ç¤ºæ•´ä¸ªå›¾å½¢\n",
    "        plt.tight_layout()  # è°ƒæ•´å­å›¾é—´çš„é—´è·\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è½½openvinoæ¨¡å‹\\torchæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[input] shape[?,3,?,?] type: f32>\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(inferencer.input_blob)\n",
    "print(inferencer.input_blob.partial_shape[2].is_static)\n",
    "print(inferencer.input_blob.partial_shape[3].is_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      ExtractBChannel()\n",
       "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       "      Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225], inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰§è¡Œæ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['71__DA2951175 - å‰¯æœ¬ (2) (2).png', '71__DA2951175 - å‰¯æœ¬ (2).png', '69__DA2951225.png', '71__DA2951175 - å‰¯æœ¬.png', '69__DA2951225 - å‰¯æœ¬.png', '69__DA2951225 - å‰¯æœ¬ (2).png', '71__DA2951175.png', '71__DA2951175 - å‰¯æœ¬ (2) (2) 2.png']\n",
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - å‰¯æœ¬ (2) (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2950 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - å‰¯æœ¬ (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.8282 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2755395e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225.png;\n",
      "0.9964976658722143 LabelName.ABNORMAL\n",
      "Prediction took 0.7232 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.01338e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - å‰¯æœ¬.png;\n",
      "0.6976840267723647 LabelName.ABNORMAL\n",
      "Prediction took 0.7511 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225 - å‰¯æœ¬.png;\n",
      "0.6859768710099958 LabelName.ABNORMAL\n",
      "Prediction took 0.7583 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225 - å‰¯æœ¬ (2).png;\n",
      "0.9330881567889115 LabelName.ABNORMAL\n",
      "Prediction took 0.7451 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.8406 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2755395e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /home/projects/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - å‰¯æœ¬ (2) (2) 2.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.8100 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "0.5744920981974643 LabelName.ABNORMAL\n",
      "Prediction took 0.7451 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "0.7498829924195373 LabelName.ABNORMAL\n",
      "Prediction took 0.7326 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [2.865513e-08..1.0000001].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "0.6069605904842698 LabelName.ABNORMAL\n",
      "Prediction took 0.7114 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.574201e-09..0.99999994].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "0.5140061791967044 LabelName.ABNORMAL\n",
      "Prediction took 0.7309 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4916407e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "0.6181693890887368 LabelName.ABNORMAL\n",
      "Prediction took 0.7150 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.7167 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "0.8263131000714965 LabelName.ABNORMAL\n",
      "Prediction took 0.7284 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-5.6794747e-10..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 0.6987 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.673895e-09..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "0.9873399584314628 LabelName.ABNORMAL\n",
      "Prediction took 0.6896 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.16941745e-08..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "0.9416600993141565 LabelName.ABNORMAL\n",
      "Prediction took 0.7104 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 256, 256]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨å½“å‰å•å…ƒæ ¼æˆ–ä¸Šä¸€ä¸ªå•å…ƒæ ¼ä¸­æ‰§è¡Œä»£ç æ—¶ Kernel å´©æºƒã€‚\n",
      "\u001b[1;31mè¯·æŸ¥çœ‹å•å…ƒæ ¼ä¸­çš„ä»£ç ï¼Œä»¥ç¡®å®šæ•…éšœçš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må•å‡»<a href='https://aka.ms/vscodeJupyterKernelCrash'>æ­¤å¤„</a>äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚\n",
      "\u001b[1;31mæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ Jupyter <a href='command:jupyter.viewOutput'>log</a>ã€‚"
     ]
    }
   ],
   "source": [
    "# å¾…æµ‹è¯•å›¾åƒ\n",
    "test_folder_path = r\"/home/projects/myprojects/anomalib_projects/datasets/test_1122\"\n",
    "test_output_path = r\"/home/projects/myprojects/anomalib_projects/datasets/test_1122/{}_output\".format(configs[\"model_name\"])\n",
    "\n",
    "test_png_files = [f for f in os.listdir(test_folder_path) if f.endswith('.png')]\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "\n",
    "print(test_png_files)\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# è¾“å‡ºè·¯å¾„ç¡®è®¤\n",
    "if os.path.exists(test_output_path):     shutil.rmtree(test_output_path)\n",
    "if os.path.exists(normal_ouput_path):    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(test_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# æ¨¡å‹æµ‹è¯•\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
