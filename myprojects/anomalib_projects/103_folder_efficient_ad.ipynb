{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Âü∫Êú¨ÈÖçÁΩÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÂáΩÊï∞ÔºöÊâπÈáèÂõæÂÉèÂèØËßÜÂåñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Á°ÆÂÆö] Êï∞ÊçÆÈõÜÁõÆÂΩï\\ËæìÂá∫ÁõÆÂΩï\\Ê®°ÂûãÈÄâÊã©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/anomalib/myprojects/anomalib_projects/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"EfficientAd\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path   = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "test_folder_path     = os.path.join(configs[\"dataset_root\"], \"test\")\n",
    "\n",
    "normal_ouput_path    = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")\n",
    "test_output_path     = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"test_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Á°ÆÂÆö] Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊìç‰ΩúTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) ÁöÑtensorÁ±ªÂûã\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # ÊèêÂèñ B ÈÄöÈÅìÔºàÂº†ÈáèÁöÑÁ¨¨‰∏â‰∏™ÈÄöÈÅìÔºåÁ¥¢Âºï‰∏∫2Ôºâ\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # ‰∏çËÉΩÊ∑ªÂä†\n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1‰πãÈó¥\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((448, 448)),                                               # Â¶ÇÊûúresizeHW‰∏ç‰∏ÄËá¥Ôºå‰ºöÂºïËµ∑fastflowÊä•layernormÈîôËØØ\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # Êó†seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx ‰∏çÊîØÊåÅ grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        #Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((448, 448)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        #Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Êï∞ÊçÆÈõÜ\n",
    "\n",
    "FolderÁöÑÁêÜËß£:\n",
    "\n",
    "1. ËÆ≠ÁªÉÈõÜ‰∏≠ÂÖ®ÈÉ®ÊòØÊ≠£Â∏∏Ê†∑Êú¨Ôºõ\n",
    "2. Ê≠£Â∏∏Ê†∑Êú¨ÈõÜ‰∏≠ÁöÑÈÉ®ÂàÜ(normal_split_ratio)Ê†∑Êú¨Ë¢´ÂùáÁ≠â(val_split_ratio)ÊîæÂÖ•Âà∞È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏≠„ÄÇ\n",
    "3. test_split_ratio‰ºº‰πéÂíånormal_split_ratio‰∏ÄÊ†∑„ÄÇ\n",
    "4. üéØ ÈúÄË¶ÅÁ°Æ‰øùËÆ≠ÁªÉÈõÜ‰∏çÂ≠òÂú®Ê≠£Â∏∏Ê†∑Êú¨ÔºõÊµãËØïÈõÜ‰∏çÂ≠òÂú®Ê†∑Êú¨ÔºõÈ™åËØÅÈõÜÊòØÂÖ®ÈÉ®Ê≠£Ê†∑Êú¨ÂíåÂÖ®ÈÉ®Ë¥üÊ†∑Êú¨„ÄÇ\n",
    "\n",
    "ÊØîÂ¶ÇÔºöval_split_ratio=0.5Êó∂Ôºö82 | 27, 10 | 27, 10|; val_split_ratio=0.1Êó∂Ôºö82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Á°ÆÂÆö] ÊûÑÂª∫ËÆ≠ÁªÉÈõÜ„ÄÅÊµãËØïÈõÜÂíåÈ™åËØÅÈõÜ\n",
    "\n",
    "**ÁêÜÂøµÊòØÔºöËÆ≠ÁªÉÈõÜÂè™ÂåÖÂê´Ê≠£Â∏∏ÂõæÂÉèÔºåÊµãËØïÈõÜÂåÖÂê´Âá†‰πéÊâÄÊúâÁöÑÊ≠£Â∏∏ÂõæÂÉèÂíåÁº∫Èô∑ÂõæÂÉè„ÄÇ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.10\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.25\n"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 1, eval_batch_size = 16,              #! ËÆ°ÁÆóÁöÑÊó∂ÂÄô‰ºö‰ΩøÁî®cudaÔºåÂõ†Ê≠§ÈúÄË¶ÅÈôêÂà∂BS‰∏çÈÄÇÁî®ÈªòËÆ§ÂÄº32Ôºõ\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! ÊéßÂà∂Ê≠£Â∏∏Ê†∑Êú¨ÔºåÂú®ÈùûËÆ≠ÁªÉÈõÜÂíåËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    "    val_split_ratio=0.98,     #! ÊéßÂà∂Ââ©‰ΩôÊ≠£Â∏∏Ê†∑Êú¨ÂíåÂºÇÂ∏∏Ê†∑Êú¨ÔºåÂú®È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 16,              #! ËÆ°ÁÆóÁöÑÊó∂ÂÄô‰ºö‰ΩøÁî®cudaÔºåÂõ†Ê≠§ÈúÄË¶ÅÈôêÂà∂BS‰∏çÈÄÇÁî®ÈªòËÆ§ÂÄº32Ôºõ\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! ÊéßÂà∂Ê≠£Â∏∏Ê†∑Êú¨ÔºåÂú®ÈùûËÆ≠ÁªÉÈõÜÂíåËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    "    val_split_ratio=0.98,     #! ÊéßÂà∂Ââ©‰ΩôÊ≠£Â∏∏Ê†∑Êú¨ÂíåÂºÇÂ∏∏Ê†∑Êú¨ÔºåÂú®È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"test\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 16,              #! ËÆ°ÁÆóÁöÑÊó∂ÂÄô‰ºö‰ΩøÁî®cudaÔºåÂõ†Ê≠§ÈúÄË¶ÅÈôêÂà∂BS‰∏çÈÄÇÁî®ÈªòËÆ§ÂÄº32Ôºõ\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.1,    #! ÊéßÂà∂Ê≠£Â∏∏Ê†∑Êú¨ÔºåÂú®ÈùûËÆ≠ÁªÉÈõÜÂíåËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    "    val_split_ratio=0.1,     #! ÊéßÂà∂Ââ©‰ΩôÊ≠£Â∏∏Ê†∑Êú¨ÂíåÂºÇÂ∏∏Ê†∑Êú¨ÔºåÂú®È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    ")\n",
    "\n",
    "folder_datamoduleD = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 1, eval_batch_size = 16,              #! ËÆ°ÁÆóÁöÑÊó∂ÂÄô‰ºö‰ΩøÁî®cudaÔºåÂõ†Ê≠§ÈúÄË¶ÅÈôêÂà∂BS‰∏çÈÄÇÁî®ÈªòËÆ§ÂÄº32Ôºõ\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.25,     #! ÊéßÂà∂Ê≠£Â∏∏Ê†∑Êú¨ÔºåÂú®ÈùûËÆ≠ÁªÉÈõÜÂíåËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    "    val_split_ratio=0.98,     #! ÊéßÂà∂Ââ©‰ΩôÊ≠£Â∏∏Ê†∑Êú¨ÂíåÂºÇÂ∏∏Ê†∑Êú¨ÔºåÂú®È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏≠ÁöÑÊï∞ÈáèÊØî‰æã\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\n",
    "folder_datamoduleB.setup()    # ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\n",
    "folder_datamoduleC.setup()    # ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\n",
    "folder_datamoduleD.setup()    # ËøõË°åÊï∞ÊçÆÈõÜÂàÜÂâ≤\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÂàÜÊûêÔºöËÆ≠ÁªÉÈõÜ„ÄÅÈ™åËØÅÈõÜ„ÄÅÊµãËØïÈõÜÁöÑÊï∞ÊçÆÈáè‰ª•ÂèäÁ±ªÂà´ÂàÜÂ∏É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # ÁªüËÆ°dataloader‰∏≠Ê†∑Êú¨ÁöÑÁ±ªÂà´ÊØî‰æã\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"labelÈ¢ëÁéáÂàÜÂ∏ÉÔºö{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([1, 3, 448, 448]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "54\n",
      "labelÈ¢ëÁéáÂàÜÂ∏ÉÔºöCounter({0: 77})\n",
      "image_paths(77)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/67__DA1479053.png', '/local_data/datasets/3-5-jing/normal/9__DA1479053 (3).png', '/local_data/datasets/3-5-jing/normal/11__DA2951215 (3).png', '/local_data/datasets/3-5-jing/normal/19__DA2951215.png', '/local_data/datasets/3-5-jing/normal/1__DA2951175 (3).png']\n",
      "labelÈ¢ëÁéáÂàÜÂ∏ÉÔºöCounter({1: 52, 0: 24})\n",
      "image_paths(76)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n",
      "labelÈ¢ëÁéáÂàÜÂ∏ÉÔºöCounter({1: 2, 0: 1})\n",
      "image_paths(3)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/13__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/19__DA2951225.png', '/local_data/datasets/3-5-jing/normal/71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# ======= efficient_adÊ®°ÂûãÊâßË°åÊó∂ÂøÖÈ°ª‰ΩøÁî®datamoduleÂèÇÊï∞ÔºåÂõ†Ê≠§ÊûÑÂª∫‰∫ÜËØ•DÁ±ªÂûãÁöÑmodule\n",
    "print(len(os.listdir(normal_folder_path)))\n",
    "print(len(os.listdir(abnormal_folder_path)))\n",
    "ana_dataloader(folder_datamoduleD.train_dataloader())\n",
    "ana_dataloader(folder_datamoduleD.val_dataloader())\n",
    "ana_dataloader(folder_datamoduleD.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÂàÜÊûêÔºöÂõæÂÉèÂÜÖÂÆπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 448, 448])\n",
      "image.0: 0.0, 0.9121046662330627, 0.06085366755723953;\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    print(\"image.{}: {}, {}, {};\".format(i, test_img.min(), test_img.max(), test_img.mean()))\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÂàÜÊûêÔºöÂõæÂÉèÁªèËøátranformÂêéÁöÑÊï∞ÊçÆËåÉÂõ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â§ÑÁêÜÂâçÁöÑÊï∞ÊçÆ : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.940784215927124, 0.06547383964061737;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"Â§ÑÁêÜÂâçÁöÑÊï∞ÊçÆ : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°Âûã‰∏é‰ºòÂåñÂô®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow, Uflow, EfficientAd\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "\n",
    "# ['train_loss', 'train_loss_step', 'image_AUROC', 'train_loss_epoch', 'epoch', 'step']\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=20)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Á°ÆÂÆö] Ê®°Âûã„ÄÅ‰ºòÂåñÂô®Âíåcallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing EfficientAd model.\n"
     ]
    }
   ],
   "source": [
    "if configs[\"model_name\"] == \"EfficientAd\":\n",
    "    model = EfficientAd()   # official: mcait, other extractors tested: resnet18, wide_resnet50_2. Could use others...\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    max_epochs=1000,                            #! Â∏åÊúõËµãÂÄºÁªôLightning TrainerÁöÑÂèÇÊï∞ÂøÖÈ°ªÂÖ®ÈÉ®ÊîæÂú®Â∑≤Ê†áÊòéÂèÇÊï∞ÁöÑÊúÄÂêéÈù¢\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÂûãËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.25\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | model                 | EfficientAdModel         | 8.1 M  | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "---------------------------------------------------------------------------\n",
      "8.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.1 M     Total params\n",
      "32.235    Total estimated model params size (MB)\n",
      "44        Modules in train mode\n",
      "7         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b64e80618141abaac4d870e4e07a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.efficient_ad.lightning_model:Load pretrained teacher model from pre_trained/efficientad_pretrained_weights/pretrained_teacher_small.pth\n",
      "Calculate teacher channel mean & std: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:15<00:00,  5.03it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e272f7404f45929916e889930ceb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.efficient_ad.lightning_model:Calculate Validation Dataset Quantiles\n",
      "Calculate Validation Dataset Quantiles: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  2.83s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914d0ac0bc5c4ca095fe8868270aae8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.efficient_ad.lightning_model:Calculate Validation Dataset Quantiles\n",
      "Calculate Validation Dataset Quantiles: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  2.83s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a491427646745c583fc877d6e4af7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.efficient_ad.lightning_model:Calculate Validation Dataset Quantiles\n",
      "Calculate Validation Dataset Quantiles: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.78s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4333627ff34f493d947ed262d85c2cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.efficient_ad.lightning_model:Calculate Validation Dataset Quantiles\n",
      "Calculate Validation Dataset Quantiles: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1019b64e9f547a993e1c71cf73897da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.efficient_ad.lightning_model:Calculate Validation Dataset Quantiles\n",
      "Calculate Validation Dataset Quantiles: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  2.85s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24bdfbdca224c11a5c1e32399a8da45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.image.efficient_ad.lightning_model:Calculate Validation Dataset Quantiles\n",
      "Calculate Validation Dataset Quantiles: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "engine.fit(datamodule=folder_datamoduleD, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.train(model=model, \n",
    "#                 train_dataloaders=train_loader, \n",
    "#                 val_dataloaders=val_loader, \n",
    "#                 test_dataloaders=val_loader)\n",
    "\n",
    "# print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÂûãÂØºÂá∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÂØºÂá∫openvinoÊ®°Âûã\\ONNXÊ®°Âûã\\TorchÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/EfficientAd/3-5/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/EfficientAd/3-5/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/EfficientAd/3-5/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/EfficientAd/3-5/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "\n",
    "#! üéØ Ê®°ÂûãÂú®ÂØºÂá∫Êó∂ÂèØ‰ª•ÊåáÂÆötransformÔºåËÄåtransformÂõ†‰∏∫ÁªßÊâøËá™torch.nn.ModuleÁ±ªÂûãÔºå‰∏îÂÆûÁé∞Âü∫‰∫étorchËá™Ë∫´ÁÆóÂ≠êÔºåÂõ†Ê≠§ÂÆÉÂèØ‰ª•ËûçÂÖ•Âú®Ê®°ÂûãÊñá‰ª∂ptÊàñËÄÖonnxÊñá‰ª∂‰∏≠„ÄÇ\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH, transform=eval_transform)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÁîüÊàêÂØºÂá∫Ê®°ÂûãÁöÑ‰øùÂ≠òË∑ØÂæÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "from pathlib import Path\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "#model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path       = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path     = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÂûãÊé®ÁêÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÊâßË°åÊ®°ÂûãÊé®ÁêÜ‰∏éÂèØËßÜÂåñÊé®ÁêÜÁªìÊûú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer: TorchInferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # ËÆ∞ÂΩïÂºÄÂßãÊó∂Èó¥\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ËØªÂèñÂõæÂÉè\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # ÂõæÂÉètransform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # ÂõæÂÉèÊé®ÁêÜ\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        \n",
    "        tmp = cv2.resize(tmp, (256, 256))\n",
    "        predictions = inferencer.predict(image=tmp)        #! Ê≥®ÔºöÂ¶ÇÊûú‰ΩøÁî®vinoÔºåËæìÂÖ•ÁöÑimageÂèÇÊï∞Â¶ÇÊûú‰∏çÊòØpathÔºåÈÇ£‰πàÂÖ∂shapeÂè™ËÉΩÊòØHWC\n",
    "        #predictions = torch_inferencer.predict(image=filter_image)\n",
    "        #predictions = torch_inferencer.predict(image=CHW_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! üéØ inferencer.predictÊé•ÂèóÂéüÂßãÂõæÂÉèÔºå\n",
    "        #! ÂÜÖÈÉ®ÈÄöËøámetadataÂíåmodelÂú®forwardÂáΩÊï∞(Èùûpre_processÂáΩÊï∞)‰∏≠Ë∞ÉÁî®Ê†áÂáÜÂåñÁöÑtransformÂØπÂõæÂÉèÂ§ÑÁêÜ;\n",
    "        #! ËßÅ/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! Ê≥®: 1. Ê®°ÂûãÂØºÂá∫Êó∂ÔºåÁîüÊàêÁöÑbin/onnx„ÄÅjsonÁ≠âÊñá‰ª∂‰∏≠Âùá‰∏çÂåÖÂê´ËÆ≠ÁªÉÊó∂‰ΩøÁî®Âà∞ÁöÑtransformÊìç‰ΩúÔºåÂåÖÊã¨Êï∞ÊçÆÂ¢ûÂº∫Áõ∏ÂÖ≥ÁöÑÊìç‰ΩúÂíåÊ†áÂáÜÂåñÁõ∏ÂÖ≥ÁöÑÊìç‰Ωú\n",
    "        #! Ê≥®: 2. ËÄåpredictÊé®ÁêÜÊó∂Ôºå‰ºöËøõË°åÁöÑÈ¢ÑÂ§ÑÁêÜÊìç‰ΩúÊòØÊ†áÂáÜÂåñÊìç‰ΩúÔºåÊØîÂ¶ÇnormalizeÔºå‰ΩÜÊòØËøôÈáåÁöÑnormalizeÊòØÈô§‰ª•255ÁöÑÊñπÂºèÔºåËÄå‰∏çÊòØÊ†áÂáÜÊ≠£Â§™ÂàÜÂ∏É„ÄÇ\n",
    "        #! ÈÄöËøá‰ª•‰∏äÊÄªÁªìÔºåÂèØÁü•ÔºåÊàë‰ª¨ÈúÄË¶Å1. ‰øÆÊîπËÆ≠ÁªÉÊó∂ÁöÑnormalizeÔºõ2. Â∞ÜExtractBChannelÊìç‰ΩúË¶ÅÂá∫Áé∞Âú®ËÆ≠ÁªÉÊó∂ÁöÑtransform‰ª•Â§ñÔºåËøòÈúÄË¶ÅÂ∞ÅË£ÖÊàê‰∏Ä‰∏™Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊìç‰Ωú„ÄÇÂú®Ê®°ÂûãÊé®ÁêÜ‰πãÂâçÂØπÂõæÂÉèËøõË°åÈ¢ùÂ§ñÁöÑÈ¢ÑÂ§ÑÁêÜÊìç‰Ωú„ÄÇ\n",
    "        \n",
    "        # ËÆ∞ÂΩïÁªìÊùüÊó∂Èó¥\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # ËÆ°ÁÆóËÄóÊó∂\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # ÂèØËßÜÂåñ\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÂõæÂΩ¢Á™óÂè£\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # ÂéüÂßãÂõæÂÉè\n",
    "        tmp0 = cv2.normalize(image, None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[0].imshow(tmp0)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "        \n",
    "        # ËÆ≠ÁªÉÁî®ÂõæÂÉè\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "        \n",
    "        # ËÆ≠ÁªÉÁî®ÂõæÂÉè\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "\n",
    "        # ÁÉ≠Âõæ\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! ÁÉ≠ÂäõÂõæÊòØanomaly_map‰∏éÂéüÂßãÂõæÂÉèÁöÑÂä†ÊùÉÁªìÂêà\n",
    "        axs[3].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "\n",
    "        # È¢ÑÊµãÊé©Ê®°\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥\n",
    "\n",
    "        # È¢ÑÊµãÊé©Ê®°\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # ÂÖ≥Èó≠ÂùêÊ†áËΩ¥ \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Ê∑ªÂä†ÊñáÊú¨‰ø°ÊÅØÂà∞ÂõæÂΩ¢ÁöÑ‰∏äÊñπ‰∏≠Èó¥‰ΩçÁΩÆ\n",
    "        fig_text_x = 0.1   # xÂùêÊ†áÂú®ÂõæÂΩ¢ÂÆΩÂ∫¶ÁöÑ‰∏≠ÂøÉ‰ΩçÁΩÆ\n",
    "        fig_text_y = 0.95  # yÂùêÊ†áÁ®çÂæÆÈù†ËøëÂõæÂΩ¢ÁöÑÈ°∂ÈÉ®ÔºåÈÅøÂÖç‰∏éÂ≠êÂõæÈáçÂè†\n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n' \n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # ÊòæÁ§∫Êï¥‰∏™ÂõæÂΩ¢\n",
    "        plt.tight_layout()  # Ë∞ÉÊï¥Â≠êÂõæÈó¥ÁöÑÈó¥Ë∑ù\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Âä†ËΩΩopenvinoÊ®°Âûã\\torchÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[input] shape[?,3,?,?] type: f32>\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(inferencer.input_blob)\n",
    "print(inferencer.input_blob.partial_shape[2].is_static)\n",
    "print(inferencer.input_blob.partial_shape[3].is_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      ExtractBChannel()\n",
       "      Resize(size=[448, 448], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÊâßË°åÊ®°ÂûãÊé®ÁêÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['71__DA2951175 - ÂâØÊú¨ (2) (2).png', '71__DA2951175 - ÂâØÊú¨ (2).png', '69__DA2951225.png', '71__DA2951175 - ÂâØÊú¨.png', '69__DA2951225 - ÂâØÊú¨.png', '69__DA2951225 - ÂâØÊú¨ (2).png', '71__DA2951175.png', '71__DA2951175 - ÂâØÊú¨ (2) (2) 2.png']\n",
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - ÂâØÊú¨ (2) (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.5286 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - ÂâØÊú¨ (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.3104 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2618 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - ÂâØÊú¨.png;\n",
      "0.613671562128611 LabelName.ABNORMAL\n",
      "Prediction took 1.2537 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225 - ÂâØÊú¨.png;\n",
      "0.6118520193079384 LabelName.ABNORMAL\n",
      "Prediction took 1.2151 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225 - ÂâØÊú¨ (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2653 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2493 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - ÂâØÊú¨ (2) (2) 2.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.4610 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "0.5029286703376449 LabelName.ABNORMAL\n",
      "Prediction took 1.2650 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "0.4632706303388293 LabelName.NORMAL\n",
      "Prediction took 1.2026 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "0.48646007337364777 LabelName.NORMAL\n",
      "Prediction took 1.2223 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "0.4668515854114335 LabelName.NORMAL\n",
      "Prediction took 1.2731 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "0.55547889839168 LabelName.ABNORMAL\n",
      "Prediction took 1.2192 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2436 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2172 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2146 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.3101 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "0.9743003814762914 LabelName.ABNORMAL\n",
      "Prediction took 1.2224 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# ÂæÖÊµãËØïÂõæÂÉè\n",
    "test_folder_path = r\"/home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122\"\n",
    "test_output_path = r\"/home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/{}_output\".format(configs[\"model_name\"])\n",
    "\n",
    "test_png_files = [f for f in os.listdir(test_folder_path) if f.endswith('.png')]\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "\n",
    "print(test_png_files)\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# ËæìÂá∫Ë∑ØÂæÑÁ°ÆËÆ§\n",
    "if os.path.exists(test_output_path):     shutil.rmtree(test_output_path)\n",
    "if os.path.exists(normal_ouput_path):    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(test_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# Ê®°ÂûãÊµãËØï\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
