{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸºæœ¬é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‡½æ•°ï¼šæ‰¹é‡å›¾åƒå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ•°æ®é›†ç›®å½•\\è¾“å‡ºç›®å½•\\æ¨¡å‹é€‰æ‹©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: /local_data/datasets/3-5-jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/anomalib/myprojects/anomalib_projects/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"EfficientAd\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path   = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "test_folder_path     = os.path.join(configs[\"dataset_root\"], \"test\")\n",
    "\n",
    "normal_ouput_path    = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")\n",
    "test_output_path     = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"test_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ•°æ®é¢„å¤„ç†æ“ä½œTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) çš„tensorç±»å‹\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # æå– B é€šé“ï¼ˆå¼ é‡çš„ç¬¬ä¸‰ä¸ªé€šé“ï¼Œç´¢å¼•ä¸º2ï¼‰\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # ä¸èƒ½æ·»åŠ \n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1ä¹‹é—´\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((448, 448)),                                               # å¦‚æœresizeHWä¸ä¸€è‡´ï¼Œä¼šå¼•èµ·fastflowæŠ¥layernormé”™è¯¯\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # æ— seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx ä¸æ”¯æŒ grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        #Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((448, 448)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        #Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ•°æ®é›†\n",
    "\n",
    "Folderçš„ç†è§£:\n",
    "\n",
    "1. è®­ç»ƒé›†ä¸­å…¨éƒ¨æ˜¯æ­£å¸¸æ ·æœ¬ï¼›\n",
    "2. æ­£å¸¸æ ·æœ¬é›†ä¸­çš„éƒ¨åˆ†(normal_split_ratio)æ ·æœ¬è¢«å‡ç­‰(val_split_ratio)æ”¾å…¥åˆ°éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­ã€‚\n",
    "3. test_split_ratioä¼¼ä¹å’Œnormal_split_ratioä¸€æ ·ã€‚\n",
    "4. ğŸ¯ éœ€è¦ç¡®ä¿è®­ç»ƒé›†ä¸å­˜åœ¨æ­£å¸¸æ ·æœ¬ï¼›æµ‹è¯•é›†ä¸å­˜åœ¨æ ·æœ¬ï¼›éªŒè¯é›†æ˜¯å…¨éƒ¨æ­£æ ·æœ¬å’Œå…¨éƒ¨è´Ÿæ ·æœ¬ã€‚\n",
    "\n",
    "æ¯”å¦‚ï¼šval_split_ratio=0.5æ—¶ï¼š82 | 27, 10 | 27, 10|; val_split_ratio=0.1æ—¶ï¼š82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ„å»ºè®­ç»ƒé›†ã€æµ‹è¯•é›†å’ŒéªŒè¯é›†\n",
    "\n",
    "**ç†å¿µæ˜¯ï¼šè®­ç»ƒé›†åªåŒ…å«æ­£å¸¸å›¾åƒï¼Œæµ‹è¯•é›†åŒ…å«å‡ ä¹æ‰€æœ‰çš„æ­£å¸¸å›¾åƒå’Œç¼ºé™·å›¾åƒã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.10\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.50\n"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 1, eval_batch_size = 16,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 16,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"test\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 16, eval_batch_size = 16,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.1,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.1,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleD = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 1, eval_batch_size = 16,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    "    test_split_ratio=0.25,     #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleB.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleC.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleD.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„æ•°æ®é‡ä»¥åŠç±»åˆ«åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # ç»Ÿè®¡dataloaderä¸­æ ·æœ¬çš„ç±»åˆ«æ¯”ä¾‹\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"labelé¢‘ç‡åˆ†å¸ƒï¼š{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([1, 3, 448, 448]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "54\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({0: 51})\n",
      "image_paths(51)[:5]: \n",
      "['/local_data/datasets/3-5-jing/normal/7__DA2951225 (2).png', '/local_data/datasets/3-5-jing/normal/11__DA2951225 (2).png', '/local_data/datasets/3-5-jing/normal/59__DA2951175.png', '/local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png', '/local_data/datasets/3-5-jing/normal/59__DA2951215.png']\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({1: 52, 0: 49})\n",
      "image_paths(101)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/11__DA1479053 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175 (2).png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/11__DA2951225.png', '/local_data/datasets/3-5-jing/abnormal/13__DA1479053 (2).png']\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({1: 2, 0: 2})\n",
      "image_paths(4)[:5]: \n",
      "['/local_data/datasets/3-5-jing/abnormal/13__DA2951175.png', '/local_data/datasets/3-5-jing/abnormal/19__DA2951225.png', '/local_data/datasets/3-5-jing/normal/1__DA2951225.png', '/local_data/datasets/3-5-jing/normal/53__DA2951215.png']\n"
     ]
    }
   ],
   "source": [
    "# ======= efficient_adæ¨¡å‹æ‰§è¡Œæ—¶å¿…é¡»ä½¿ç”¨datamoduleå‚æ•°ï¼Œå› æ­¤æ„å»ºäº†è¯¥Dç±»å‹çš„module\n",
    "print(len(os.listdir(normal_folder_path)))\n",
    "print(len(os.listdir(abnormal_folder_path)))\n",
    "ana_dataloader(folder_datamoduleD.train_dataloader())\n",
    "ana_dataloader(folder_datamoduleD.val_dataloader())\n",
    "ana_dataloader(folder_datamoduleD.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æï¼šå›¾åƒå†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 448, 448])\n",
      "image.0: 0.0, 0.8970963358879089, 0.060321144759655;\n",
      "image.1: 0.0, 0.906277596950531, 0.049462344497442245;\n",
      "image.2: 0.0, 0.9757779240608215, 0.06964854896068573;\n",
      "image.3: 0.0, 0.9570558667182922, 0.06452272087335587;\n",
      "image.4: 0.0, 0.9264544248580933, 0.05982942134141922;\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    print(\"image.{}: {}, {}, {};\".format(i, test_img.min(), test_img.max(), test_img.mean()))\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æï¼šå›¾åƒç»è¿‡tranformåçš„æ•°æ®èŒƒå›´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å‰çš„æ•°æ® : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.940784215927124, 0.06547383964061737;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"å¤„ç†å‰çš„æ•°æ® : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹ä¸ä¼˜åŒ–å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow, Uflow, EfficientAd\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "\n",
    "# ['train_loss', 'train_loss_step', 'image_AUROC', 'train_loss_epoch', 'epoch', 'step']\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=20)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œcallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing EfficientAd model.\n"
     ]
    }
   ],
   "source": [
    "if configs[\"model_name\"] == \"EfficientAd\":\n",
    "    model = EfficientAd()   # official: mcait, other extractors tested: resnet18, wide_resnet50_2. Could use others...\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    max_epochs=1000,                            #! å¸Œæœ›èµ‹å€¼ç»™Lightning Trainerçš„å‚æ•°å¿…é¡»å…¨éƒ¨æ”¾åœ¨å·²æ ‡æ˜å‚æ•°çš„æœ€åé¢\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'train_dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/engine/engine.py:549\u001b[0m, in \u001b[0;36mEngine.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mvalidate(model, val_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 549\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/anoma/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/anoma/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/anoma/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/anoma/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:957\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/anaconda3/envs/anoma/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:158\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_precision_plugin()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/anaconda3/envs/anoma/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:138\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates optimizers and schedulers.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    trainer: the Trainer, these optimizers should be connected to\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler_configs \u001b[38;5;241m=\u001b[39m \u001b[43m_init_optimizers_and_lr_schedulers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/anoma/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:179\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call\n\u001b[0;32m--> 179\u001b[0m optim_conf \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigure_optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optim_conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     rank_zero_warn(\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    184\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/anoma/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/home/projects/anomalib/src/anomalib/models/image/efficient_ad/lightning_model.py:226\u001b[0m, in \u001b[0;36mEfficientAd.configure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m     num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_steps\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# max_steps not set -> determine steps as 'max_epochs' * 'steps in a single training epoch'\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m())\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_steps,\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39mtrain_dataloader()),\n\u001b[1;32m    231\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'train_dataloader'"
     ]
    }
   ],
   "source": [
    "engine.fit(train_dataloaders=train_loader, val_dataloaders=val_loader, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.train(model=model, \n",
    "#                 train_dataloaders=train_loader, \n",
    "#                 val_dataloaders=val_loader, \n",
    "#                 test_dataloaders=val_loader)\n",
    "\n",
    "# print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹å¯¼å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯¼å‡ºopenvinoæ¨¡å‹\\ONNXæ¨¡å‹\\Torchæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/EfficientAd/3-5/latest/weights/openvino/model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to /home/projects/results/EfficientAd/3-5/latest/weights/onnx/model.onnx\n",
      "INFO:root:Exported model to /home/projects/results/EfficientAd/3-5/latest/weights/torch/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to /home/projects/results/EfficientAd/3-5/latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "\n",
    "#! ğŸ¯ æ¨¡å‹åœ¨å¯¼å‡ºæ—¶å¯ä»¥æŒ‡å®štransformï¼Œè€Œtransformå› ä¸ºç»§æ‰¿è‡ªtorch.nn.Moduleç±»å‹ï¼Œä¸”å®ç°åŸºäºtorchè‡ªèº«ç®—å­ï¼Œå› æ­¤å®ƒå¯ä»¥èå…¥åœ¨æ¨¡å‹æ–‡ä»¶ptæˆ–è€…onnxæ–‡ä»¶ä¸­ã€‚\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH, transform=eval_transform)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç”Ÿæˆå¯¼å‡ºæ¨¡å‹çš„ä¿å­˜è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "from pathlib import Path\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "#model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path       = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path     = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰§è¡Œæ¨¡å‹æ¨ç†ä¸å¯è§†åŒ–æ¨ç†ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer: TorchInferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # è®°å½•å¼€å§‹æ—¶é—´\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # è¯»å–å›¾åƒ\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # å›¾åƒtransform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # å›¾åƒæ¨ç†\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        \n",
    "        tmp = cv2.resize(tmp, (256, 256))\n",
    "        predictions = inferencer.predict(image=tmp)        #! æ³¨ï¼šå¦‚æœä½¿ç”¨vinoï¼Œè¾“å…¥çš„imageå‚æ•°å¦‚æœä¸æ˜¯pathï¼Œé‚£ä¹ˆå…¶shapeåªèƒ½æ˜¯HWC\n",
    "        #predictions = torch_inferencer.predict(image=filter_image)\n",
    "        #predictions = torch_inferencer.predict(image=CHW_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! ğŸ¯ inferencer.predictæ¥å—åŸå§‹å›¾åƒï¼Œ\n",
    "        #! å†…éƒ¨é€šè¿‡metadataå’Œmodelåœ¨forwardå‡½æ•°(épre_processå‡½æ•°)ä¸­è°ƒç”¨æ ‡å‡†åŒ–çš„transformå¯¹å›¾åƒå¤„ç†;\n",
    "        #! è§/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! æ³¨: 1. æ¨¡å‹å¯¼å‡ºæ—¶ï¼Œç”Ÿæˆçš„bin/onnxã€jsonç­‰æ–‡ä»¶ä¸­å‡ä¸åŒ…å«è®­ç»ƒæ—¶ä½¿ç”¨åˆ°çš„transformæ“ä½œï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºç›¸å…³çš„æ“ä½œå’Œæ ‡å‡†åŒ–ç›¸å…³çš„æ“ä½œ\n",
    "        #! æ³¨: 2. è€Œpredictæ¨ç†æ—¶ï¼Œä¼šè¿›è¡Œçš„é¢„å¤„ç†æ“ä½œæ˜¯æ ‡å‡†åŒ–æ“ä½œï¼Œæ¯”å¦‚normalizeï¼Œä½†æ˜¯è¿™é‡Œçš„normalizeæ˜¯é™¤ä»¥255çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯æ ‡å‡†æ­£å¤ªåˆ†å¸ƒã€‚\n",
    "        #! é€šè¿‡ä»¥ä¸Šæ€»ç»“ï¼Œå¯çŸ¥ï¼Œæˆ‘ä»¬éœ€è¦1. ä¿®æ”¹è®­ç»ƒæ—¶çš„normalizeï¼›2. å°†ExtractBChannelæ“ä½œè¦å‡ºç°åœ¨è®­ç»ƒæ—¶çš„transformä»¥å¤–ï¼Œè¿˜éœ€è¦å°è£…æˆä¸€ä¸ªæ•°æ®é¢„å¤„ç†æ“ä½œã€‚åœ¨æ¨¡å‹æ¨ç†ä¹‹å‰å¯¹å›¾åƒè¿›è¡Œé¢å¤–çš„é¢„å¤„ç†æ“ä½œã€‚\n",
    "        \n",
    "        # è®°å½•ç»“æŸæ—¶é—´\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # è®¡ç®—è€—æ—¶\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢çª—å£\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # åŸå§‹å›¾åƒ\n",
    "        tmp0 = cv2.normalize(image, None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[0].imshow(tmp0)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # çƒ­å›¾\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! çƒ­åŠ›å›¾æ˜¯anomaly_mapä¸åŸå§‹å›¾åƒçš„åŠ æƒç»“åˆ\n",
    "        axs[3].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # å…³é—­åæ ‡è½´ \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # æ·»åŠ æ–‡æœ¬ä¿¡æ¯åˆ°å›¾å½¢çš„ä¸Šæ–¹ä¸­é—´ä½ç½®\n",
    "        fig_text_x = 0.1   # xåæ ‡åœ¨å›¾å½¢å®½åº¦çš„ä¸­å¿ƒä½ç½®\n",
    "        fig_text_y = 0.95  # yåæ ‡ç¨å¾®é è¿‘å›¾å½¢çš„é¡¶éƒ¨ï¼Œé¿å…ä¸å­å›¾é‡å \n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n' \n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # æ˜¾ç¤ºæ•´ä¸ªå›¾å½¢\n",
    "        plt.tight_layout()  # è°ƒæ•´å­å›¾é—´çš„é—´è·\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è½½openvinoæ¨¡å‹\\torchæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[input] shape[?,3,?,?] type: f32>\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(inferencer.input_blob)\n",
    "print(inferencer.input_blob.partial_shape[2].is_static)\n",
    "print(inferencer.input_blob.partial_shape[3].is_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      ExtractBChannel()\n",
       "      Resize(size=[448, 448], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰§è¡Œæ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['71__DA2951175 - å‰¯æœ¬ (2) (2).png', '71__DA2951175 - å‰¯æœ¬ (2).png', '69__DA2951225.png', '71__DA2951175 - å‰¯æœ¬.png', '69__DA2951225 - å‰¯æœ¬.png', '69__DA2951225 - å‰¯æœ¬ (2).png', '71__DA2951175.png', '71__DA2951175 - å‰¯æœ¬ (2) (2) 2.png']\n",
      "['11__DA2951215 (4).png', '17__DA2951215 (3).png', '59__DA1479053.png', '67__DA2951175.png', '59__DA2951175.png']\n",
      "['71__DA2951215.png', '69__DA2951225 (2).png', '5__DA2951225.png', '13__DA2951175.png', '17__DA2951225.png']\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - å‰¯æœ¬ (2) (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.5286 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - å‰¯æœ¬ (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.3104 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2618 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - å‰¯æœ¬.png;\n",
      "0.613671562128611 LabelName.ABNORMAL\n",
      "Prediction took 1.2537 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225 - å‰¯æœ¬.png;\n",
      "0.6118520193079384 LabelName.ABNORMAL\n",
      "Prediction took 1.2151 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/69__DA2951225 - å‰¯æœ¬ (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2653 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2493 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/71__DA2951175 - å‰¯æœ¬ (2) (2) 2.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.4610 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/11__DA2951215 (4).png;\n",
      "0.5029286703376449 LabelName.ABNORMAL\n",
      "Prediction took 1.2650 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/17__DA2951215 (3).png;\n",
      "0.4632706303388293 LabelName.NORMAL\n",
      "Prediction took 1.2026 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA1479053.png;\n",
      "0.48646007337364777 LabelName.NORMAL\n",
      "Prediction took 1.2223 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/67__DA2951175.png;\n",
      "0.4668515854114335 LabelName.NORMAL\n",
      "Prediction took 1.2731 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/normal/59__DA2951175.png;\n",
      "0.55547889839168 LabelName.ABNORMAL\n",
      "Prediction took 1.2192 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/71__DA2951215.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2436 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/69__DA2951225 (2).png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2172 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/5__DA2951225.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.2146 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/13__DA2951175.png;\n",
      "1.0 LabelName.ABNORMAL\n",
      "Prediction took 1.3101 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "\n",
      "===> /local_data/datasets/3-5-jing/abnormal/17__DA2951225.png;\n",
      "0.9743003814762914 LabelName.ABNORMAL\n",
      "Prediction took 1.2224 seconds.\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([3, 2048, 2448]); transform_image: torch.Size([3, 448, 448]); predictions.heat_map: (256, 256, 3);\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# å¾…æµ‹è¯•å›¾åƒ\n",
    "test_folder_path = r\"/home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122\"\n",
    "test_output_path = r\"/home/projects/anomalib/myprojects/anomalib_projects/datasets/test_1122/{}_output\".format(configs[\"model_name\"])\n",
    "\n",
    "test_png_files = [f for f in os.listdir(test_folder_path) if f.endswith('.png')]\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "\n",
    "print(test_png_files)\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# è¾“å‡ºè·¯å¾„ç¡®è®¤\n",
    "if os.path.exists(test_output_path):     shutil.rmtree(test_output_path)\n",
    "if os.path.exists(normal_ouput_path):    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(test_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# æ¨¡å‹æµ‹è¯•\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
