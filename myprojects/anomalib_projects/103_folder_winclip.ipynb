{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸºæœ¬é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from anomalib import TaskType\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image, to_image\n",
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "isLinux = True if os_name.lower() == 'linux' else False\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‡½æ•°ï¼šæ‰¹é‡å›¾åƒå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ•°æ®é›†ç›®å½•\\è¾“å‡ºç›®å½•\\æ¨¡å‹é€‰æ‹©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"/local_data/datasets/3-5-jing\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"/home/projects/myprojects/anomalib_projects/outputs\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"WinClip\",\n",
    "}\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path   = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "test_folder_path     = os.path.join(configs[\"dataset_root\"], \"test\")\n",
    "\n",
    "normal_ouput_path    = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")\n",
    "test_output_path     = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"test_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ•°æ®é¢„å¤„ç†æ“ä½œTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) çš„tensorç±»å‹\n",
    "    def forward(self, img):\n",
    "        \n",
    "        if not isinstance(img, torch.Tensor): img = torch.Tensor(img)\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]                      # æå– B é€šé“ï¼ˆå¼ é‡çš„ç¬¬ä¸‰ä¸ªé€šé“ï¼Œç´¢å¼•ä¸º2ï¼‰\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1                # ä¸èƒ½æ·»åŠ \n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),                                                # 0~1ä¹‹é—´\n",
    "        #ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((240, 240)),                                               # å¦‚æœresizeHWä¸ä¸€è‡´ï¼Œä¼šå¼•èµ·fastflowæŠ¥layernormé”™è¯¯\n",
    "        # RandomHorizontalFlip(p=0.3),                                    # æ— seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),     # onnx ä¸æ”¯æŒ grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),                              # Normalize expects float input\n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          \n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((240, 240)),\n",
    "        #RandomHorizontalFlip(p=0.3),  \n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ), \n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True), \n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ•°æ®é›†\n",
    "\n",
    "Folderçš„ç†è§£:\n",
    "\n",
    "1. è®­ç»ƒé›†ä¸­å…¨éƒ¨æ˜¯æ­£å¸¸æ ·æœ¬ï¼›\n",
    "2. æ­£å¸¸æ ·æœ¬é›†ä¸­çš„éƒ¨åˆ†(normal_split_ratio)æ ·æœ¬è¢«å‡ç­‰(val_split_ratio)æ”¾å…¥åˆ°éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­ã€‚\n",
    "3. test_split_ratioä¼¼ä¹å’Œnormal_split_ratioä¸€æ ·ã€‚\n",
    "4. ğŸ¯ éœ€è¦ç¡®ä¿è®­ç»ƒé›†ä¸å­˜åœ¨æ­£å¸¸æ ·æœ¬ï¼›æµ‹è¯•é›†ä¸å­˜åœ¨æ ·æœ¬ï¼›éªŒè¯é›†æ˜¯å…¨éƒ¨æ­£æ ·æœ¬å’Œå…¨éƒ¨è´Ÿæ ·æœ¬ã€‚\n",
    "\n",
    "æ¯”å¦‚ï¼šval_split_ratio=0.5æ—¶ï¼š82 | 27, 10 | 27, 10|; val_split_ratio=0.1æ—¶ï¼š82 | 5, 2 | 49, 18|;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ„å»ºè®­ç»ƒé›†ã€æµ‹è¯•é›†å’ŒéªŒè¯é›†\n",
    "\n",
    "**ç†å¿µæ˜¯ï¼šè®­ç»ƒé›†åªåŒ…å«æ­£å¸¸å›¾åƒï¼Œæµ‹è¯•é›†åŒ…å«å‡ ä¹æ‰€æœ‰çš„æ­£å¸¸å›¾åƒå’Œç¼ºé™·å›¾åƒã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.02\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.98\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.10\n"
     ]
    }
   ],
   "source": [
    "folder_datamoduleA = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 32, eval_batch_size = 32,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.02,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleB = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 32, eval_batch_size = 32,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.98,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.98,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleC = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"test\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,                                           #! in jupyter, need to be zero. and non-0 in python main;\n",
    "    train_batch_size = 32, eval_batch_size = 32,              #! è®¡ç®—çš„æ—¶å€™ä¼šä½¿ç”¨cudaï¼Œå› æ­¤éœ€è¦é™åˆ¶BSä¸é€‚ç”¨é»˜è®¤å€¼32ï¼›\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    #transform=train_transform, \n",
    "    seed = seed,\n",
    "    test_split_ratio=0.1,    #! æ§åˆ¶æ­£å¸¸æ ·æœ¬ï¼Œåœ¨éè®­ç»ƒé›†å’Œè®­ç»ƒé›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    "    val_split_ratio=0.1,     #! æ§åˆ¶å‰©ä½™æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸­çš„æ•°é‡æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "folder_datamoduleA.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleB.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "folder_datamoduleC.setup()    # è¿›è¡Œæ•°æ®é›†åˆ†å‰²\n",
    "\n",
    "train_loader = folder_datamoduleA.train_dataloader()\n",
    "val_loader   = folder_datamoduleB.val_dataloader()\n",
    "test_loader  = folder_datamoduleC.test_dataloader()\n",
    "\n",
    "# train_loader_lst  = [train_loader]\n",
    "# val_loader_lst    = [train_loader, val_loader, test_loader]\n",
    "# test_loader_lst   = [train_loader, val_loader, test_loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„æ•°æ®é‡ä»¥åŠç±»åˆ«åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana_dataloader(dataloader):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # ç»Ÿè®¡dataloaderä¸­æ ·æœ¬çš„ç±»åˆ«æ¯”ä¾‹\n",
    "    all_labels = []\n",
    "    all_image_paths = []\n",
    "    for data in dataloader:\n",
    "        image_paths: list = data[\"image_path\"]\n",
    "        labels: torch.tensor = data[\"label\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_image_paths.extend(image_paths)\n",
    "    label_frequency = Counter(all_labels)\n",
    "    print(\"labelé¢‘ç‡åˆ†å¸ƒï¼š{}\".format(label_frequency))\n",
    "    print(\"image_paths({})[:5]: \\n{}\".format(len(all_image_paths), all_image_paths[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([32, 3, 240, 240]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({0: 100})\n",
      "image_paths(100)[:5]: \n",
      "['F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\13__DA1479053.png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\17__DA2951215 (3).png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\5__DA2951215.png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\55__DA2951215.png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\71__DA1479053.png']\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(train_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([32, 3, 240, 240]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({0: 97, 1: 52})\n",
      "image_paths(149)[:5]: \n",
      "['F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\abnormal\\\\11__DA1479053 (2).png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\abnormal\\\\11__DA2951175 (2).png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\abnormal\\\\11__DA2951175.png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\abnormal\\\\11__DA2951225.png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\abnormal\\\\13__DA1479053 (2).png']\n"
     ]
    }
   ],
   "source": [
    "# Val images\n",
    "i, data = next(enumerate(val_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([32, 3, 240, 240]) <class 'list'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "labelé¢‘ç‡åˆ†å¸ƒï¼šCounter({1: 27, 0: 9})\n",
      "image_paths(36)[:5]: \n",
      "['F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\11__DA1479053.png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\3__DA2951175 (3).png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\53__DA1479053.png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\55__DA1479053.png', 'F:\\\\Projects\\\\anomalib\\\\notebooks\\\\datasets\\\\3-5 - jing\\\\normal\\\\55__DA2951225.png']\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "i, data = next(enumerate(test_loader))\n",
    "print(data.keys(), data[\"image\"].shape, type(data[\"image_path\"]), type(data[\"label\"]), type(data[\"image\"]))\n",
    "ana_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æï¼šå›¾åƒå†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 240, 240])\n",
      "image.0: -1.804444432258606, 1.9296391010284424, -1.5363496541976929;\n",
      "image.1: -1.804444432258606, 2.1801869869232178, -1.584612488746643;\n",
      "image.2: -1.804444432258606, 2.466968536376953, -1.4948996305465698;\n",
      "image.3: -1.804444432258606, 2.1296958923339844, -1.5176774263381958;\n",
      "image.4: -1.804444432258606, 2.1156301498413086, -1.5385373830795288;\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0])[:5]:\n",
    "    test_img = data[\"image\"][i]\n",
    "    print(\"image.{}: {}, {}, {};\".format(i, test_img.min(), test_img.max(), test_img.mean()))\n",
    "    total_images.append(test_img)\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æï¼šå›¾åƒç»è¿‡tranformåçš„æ•°æ®èŒƒå›´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å‰çš„æ•°æ® : 0.0, 0.9725490808486938, 0.06547369062900543;\n",
      "transform.0: 0.0, 0.9725490808486938, 0.04567107558250427;\n",
      "transform.1: 0.0, 0.9354147911071777, 0.06547418981790543;\n",
      "transform.2: -1.804444432258606, 2.517995834350586, -1.5134503841400146;\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "\n",
    "temp_path = r\"/local_data/datasets/3-5-jing/normal/1__DA2951175 (2).png\" if isLinux else r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\1__DA2951175 (2).png\"\n",
    "\n",
    "test_image = read_image(temp_path, as_tensor=True)\n",
    "print(\"å¤„ç†å‰çš„æ•°æ® : {}, {}, {};\".format(test_image.min(), test_image.max(), test_image.mean()))\n",
    "\n",
    "for index, trans in enumerate(train_transform.transforms):\n",
    "    tmp_image = trans(test_image)\n",
    "    print(\"transform.{}: {}, {}, {};\".format(index, tmp_image.min(), tmp_image.max(), tmp_image.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹ä¸ä¼˜åŒ–å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow, WinClip\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "\n",
    "# ['train_loss', 'train_loss_step', 'image_AUROC', 'train_loss_epoch', 'epoch', 'step']\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "graph_logger = GraphLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoModel\n",
    "# model = AutoModel.from_pretrained(\"immich-app/ViT-B-16-plus-240__laion400m_e31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ç¡®å®š] æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œcallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing WinClip model.\n",
      "INFO:root:Loaded ViT-B-16-plus-240 model config.\n",
      "INFO:root:Loading pretrained ViT-B-16-plus-240 weights (laion400m_e31).\n"
     ]
    }
   ],
   "source": [
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    callbacks = [ model_checkpoint, \n",
    "                 #early_stopping, \n",
    "                 graph_logger]\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, \n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    pixel_metrics=[\"F1Score\",\"AUROC\"], \n",
    "                    # callbacks= callbacks\n",
    "                    )\n",
    "elif configs[\"model_name\"] == \"WinClip\":\n",
    "    callbacks = [ model_checkpoint, \n",
    "                 #early_stopping, \n",
    "                 graph_logger]\n",
    "    \"\"\"\n",
    "    - Set the class name used in the prompt ensemble.\n",
    "    - Collect text embeddings for zero-shot inference.\n",
    "    - Collect reference images for few-shot inference.\n",
    "    \"\"\"\n",
    "    model = WinClip(class_name = \"yijiao\", few_shot_source=normal_folder_path, k_shot = len(os.listdir(normal_folder_path)))\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION,\n",
    "                    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "                    logger=False, callbacks= callbacks,\n",
    "                    accelerator=\"gpu\",                       # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "                    max_epochs=50,                            #! å¸Œæœ›èµ‹å€¼ç»™Lightning Trainerçš„å‚æ•°å¿…é¡»å…¨éƒ¨æ”¾åœ¨å·²æ ‡æ˜å‚æ•°çš„æœ€åé¢\n",
    "                    )\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "INFO:anomalib.models.image.winclip.lightning_model:Using class name from init args: yijiao\n",
      "INFO:anomalib.models.image.winclip.lightning_model:Loading reference images from F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9670a0a6b6bc4bee995795035ad5dc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f5ab9a91074e91a9f72e055d968d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.callbacks.timer:Testing took 143.72053003311157 seconds\n",
      "Throughput (batch_size=32) : 1.0367342784337916 FPS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">        image_AUROC        </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.7464313507080078     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       image_F1Score       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6338028311729431     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.7464313507080078    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6338028311729431    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Projects\\anomalib\\myprojects\\anomalib_projects\\results\\WinClip\\latest\n"
     ]
    }
   ],
   "source": [
    "engine.train(model=model, \n",
    "                train_dataloaders=train_loader, \n",
    "                val_dataloaders=val_loader, \n",
    "                test_dataloaders=val_loader)\n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹å¯¼å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯¼å‡ºopenvinoæ¨¡å‹\\ONNXæ¨¡å‹\\Torchæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to F:\\Projects\\anomalib\\myprojects\\anomalib_projects\\results\\WinClip\\latest\\weights\\openvino\\model.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to F:\\Projects\\anomalib\\myprojects\\anomalib_projects\\results\\WinClip\\latest\\weights\\onnx\\model.onnx\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'WinClipModel.encode_image.<locals>.get_feature_map.<locals>.hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m engine\u001b[38;5;241m.\u001b[39mexport(model\u001b[38;5;241m=\u001b[39mmodel, export_type\u001b[38;5;241m=\u001b[39mExportType\u001b[38;5;241m.\u001b[39mOPENVINO, transform\u001b[38;5;241m=\u001b[39meval_transform)  \u001b[38;5;66;03m# torch.onnx.export op=16\u001b[39;00m\n\u001b[0;32m      9\u001b[0m engine\u001b[38;5;241m.\u001b[39mexport(model\u001b[38;5;241m=\u001b[39mmodel, export_type\u001b[38;5;241m=\u001b[39mExportType\u001b[38;5;241m.\u001b[39mONNX, transform\u001b[38;5;241m=\u001b[39meval_transform)  \u001b[38;5;66;03m# torch.onnx.export op=16\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExportType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTORCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_transform\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# torch.onnx.export op=16\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel save to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mdefault_root_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[1;32mF:\\Projects\\anomalib\\src\\anomalib\\engine\\engine.py:952\u001b[0m, in \u001b[0;36mEngine.export\u001b[1;34m(self, model, export_type, export_root, input_size, transform, compression_type, datamodule, metric, ov_args, ckpt_path)\u001b[0m\n\u001b[0;32m    950\u001b[0m exported_model_path: Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m export_type \u001b[38;5;241m==\u001b[39m ExportType\u001b[38;5;241m.\u001b[39mTORCH:\n\u001b[1;32m--> 952\u001b[0m     exported_model_path \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m export_type \u001b[38;5;241m==\u001b[39m ExportType\u001b[38;5;241m.\u001b[39mONNX:\n\u001b[0;32m    958\u001b[0m     exported_model_path \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_onnx(\n\u001b[0;32m    959\u001b[0m         export_root\u001b[38;5;241m=\u001b[39mexport_root,\n\u001b[0;32m    960\u001b[0m         input_size\u001b[38;5;241m=\u001b[39minput_size,\n\u001b[0;32m    961\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[0;32m    962\u001b[0m         task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask,\n\u001b[0;32m    963\u001b[0m     )\n",
      "File \u001b[1;32mF:\\Projects\\anomalib\\src\\anomalib\\models\\components\\base\\export_mixin.py:89\u001b[0m, in \u001b[0;36mExportMixin.to_torch\u001b[1;34m(self, export_root, transform, task)\u001b[0m\n\u001b[0;32m     87\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata(task\u001b[38;5;241m=\u001b[39mtask)\n\u001b[0;32m     88\u001b[0m pt_model_path \u001b[38;5;241m=\u001b[39m export_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 89\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pt_model_path\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 850\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\serialization.py:1088\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m   1086\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[0;32m   1087\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[1;32m-> 1088\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1089\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m   1090\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'WinClipModel.encode_image.<locals>.get_feature_map.<locals>.hook'"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "\n",
    "#! ğŸ¯ æ¨¡å‹åœ¨å¯¼å‡ºæ—¶å¯ä»¥æŒ‡å®štransformï¼Œè€Œtransformå› ä¸ºç»§æ‰¿è‡ªtorch.nn.Moduleç±»å‹ï¼Œä¸”å®ç°åŸºäºtorchè‡ªèº«ç®—å­ï¼Œå› æ­¤å®ƒå¯ä»¥èå…¥åœ¨æ¨¡å‹æ–‡ä»¶ptæˆ–è€…onnxæ–‡ä»¶ä¸­ã€‚\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.OPENVINO, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.ONNX, transform=eval_transform)  # torch.onnx.export op=16\n",
    "engine.export(model=model, export_type=ExportType.TORCH, transform=eval_transform)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç”Ÿæˆå¯¼å‡ºæ¨¡å‹çš„ä¿å­˜è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "from anomalib.deploy import OpenVINOInferencer, TorchInferencer\n",
    "from pathlib import Path\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "#model_output_path = Path(r\"/home/projects/results/Fastflow/latest\")\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path       = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "ckpt_model_path     = model_output_path / \"weights\" / \"torch\" / \"model.pt\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists(), ckpt_model_path.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰§è¡Œæ¨¡å‹æ¨ç†ä¸å¯è§†åŒ–æ¨ç†ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer:OpenVINOInferencer, torch_inferencer: TorchInferencer, transform, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "        # è®°å½•å¼€å§‹æ—¶é—´\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # è¯»å–å›¾åƒ\n",
    "        image_path = os.path.join(input_path, file_name)\n",
    "        image = read_image(path=image_path)                      # HWC\n",
    "        CHW_image = read_image(path=image_path, as_tensor=True)  # CHW\n",
    "        print(\"\\n===> {};\".format(image_path))\n",
    "        \n",
    "        \n",
    "        # å›¾åƒtransform\n",
    "        filter_image: torch.tensor = ExtractBChannel()(CHW_image) \n",
    "        transform_image: torch.tensor = transform(CHW_image)\n",
    "        \n",
    "        \n",
    "        # å›¾åƒæ¨ç†\n",
    "        tmp = np.array(filter_image.permute(1,2,0), dtype=np.float32)\n",
    "        \n",
    "        tmp = cv2.resize(tmp, (256, 256))\n",
    "        predictions = inferencer.predict(image=tmp)        #! æ³¨ï¼šå¦‚æœä½¿ç”¨vinoï¼Œè¾“å…¥çš„imageå‚æ•°å¦‚æœä¸æ˜¯pathï¼Œé‚£ä¹ˆå…¶shapeåªèƒ½æ˜¯HWC\n",
    "        #predictions = torch_inferencer.predict(image=filter_image)\n",
    "        #predictions = torch_inferencer.predict(image=CHW_image)\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        #! ğŸ¯ inferencer.predictæ¥å—åŸå§‹å›¾åƒï¼Œ\n",
    "        #! å†…éƒ¨é€šè¿‡metadataå’Œmodelåœ¨forwardå‡½æ•°(épre_processå‡½æ•°)ä¸­è°ƒç”¨æ ‡å‡†åŒ–çš„transformå¯¹å›¾åƒå¤„ç†;\n",
    "        #! è§/home/projects/anomalib/docs/source/snippets/data/transforms/inference.txt\n",
    "        #! æ³¨: 1. æ¨¡å‹å¯¼å‡ºæ—¶ï¼Œç”Ÿæˆçš„bin/onnxã€jsonç­‰æ–‡ä»¶ä¸­å‡ä¸åŒ…å«è®­ç»ƒæ—¶ä½¿ç”¨åˆ°çš„transformæ“ä½œï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºç›¸å…³çš„æ“ä½œå’Œæ ‡å‡†åŒ–ç›¸å…³çš„æ“ä½œ\n",
    "        #! æ³¨: 2. è€Œpredictæ¨ç†æ—¶ï¼Œä¼šè¿›è¡Œçš„é¢„å¤„ç†æ“ä½œæ˜¯æ ‡å‡†åŒ–æ“ä½œï¼Œæ¯”å¦‚normalizeï¼Œä½†æ˜¯è¿™é‡Œçš„normalizeæ˜¯é™¤ä»¥255çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯æ ‡å‡†æ­£å¤ªåˆ†å¸ƒã€‚\n",
    "        #! é€šè¿‡ä»¥ä¸Šæ€»ç»“ï¼Œå¯çŸ¥ï¼Œæˆ‘ä»¬éœ€è¦1. ä¿®æ”¹è®­ç»ƒæ—¶çš„normalizeï¼›2. å°†ExtractBChannelæ“ä½œè¦å‡ºç°åœ¨è®­ç»ƒæ—¶çš„transformä»¥å¤–ï¼Œè¿˜éœ€è¦å°è£…æˆä¸€ä¸ªæ•°æ®é¢„å¤„ç†æ“ä½œã€‚åœ¨æ¨¡å‹æ¨ç†ä¹‹å‰å¯¹å›¾åƒè¿›è¡Œé¢å¤–çš„é¢„å¤„ç†æ“ä½œã€‚\n",
    "        \n",
    "        # è®°å½•ç»“æŸæ—¶é—´\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # è®¡ç®—è€—æ—¶\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        transform_image_show = transform_image.permute(1,2,0)    # CHW -> HWC\n",
    "        filter_image_show = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        # print(\"image: {}; filter_image: {}; transform_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, transform_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢çª—å£\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(18, 6))\n",
    "\n",
    "        # åŸå§‹å›¾åƒ\n",
    "        tmp0 = cv2.normalize(image, None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[0].imshow(tmp0)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp1 = cv2.normalize(filter_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[1].imshow(tmp1)\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # å…³é—­åæ ‡è½´\n",
    "        \n",
    "        # è®­ç»ƒç”¨å›¾åƒ\n",
    "        tmp2 = cv2.normalize(transform_image_show.numpy(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "        axs[2].imshow(tmp2)\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # çƒ­å›¾\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')         #! çƒ­åŠ›å›¾æ˜¯anomaly_mapä¸åŸå§‹å›¾åƒçš„åŠ æƒç»“åˆ\n",
    "        axs[3].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # å…³é—­åæ ‡è½´\n",
    "\n",
    "        # é¢„æµ‹æ©æ¨¡\n",
    "        axs[5].imshow(predictions.anomaly_map, cmap='gray', interpolation='nearest')\n",
    "        axs[5].set_title('Anomaly Map')      \n",
    "        axs[5].axis('off')  # å…³é—­åæ ‡è½´ \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # æ·»åŠ æ–‡æœ¬ä¿¡æ¯åˆ°å›¾å½¢çš„ä¸Šæ–¹ä¸­é—´ä½ç½®\n",
    "        fig_text_x = 0.1   # xåæ ‡åœ¨å›¾å½¢å®½åº¦çš„ä¸­å¿ƒä½ç½®\n",
    "        fig_text_y = 0.95  # yåæ ‡ç¨å¾®é è¿‘å›¾å½¢çš„é¡¶éƒ¨ï¼Œé¿å…ä¸å­å›¾é‡å \n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                f'Threshold: {0.5}\\n' \n",
    "                f'Score: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # æ˜¾ç¤ºæ•´ä¸ªå›¾å½¢\n",
    "        plt.tight_layout()  # è°ƒæ•´å­å›¾é—´çš„é—´è·\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è½½openvinoæ¨¡å‹\\torchæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")\n",
    "\n",
    "torch_inferencer = None\n",
    "# torch_inferencer = TorchInferencer(ckpt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[input] shape[?,3,?,?] type: f32>\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(inferencer.input_blob)\n",
    "print(inferencer.input_blob.partial_shape[2].is_static)\n",
    "print(inferencer.input_blob.partial_shape[3].is_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      ExtractBChannel()\n",
       "      Resize(size=[256, 256], interpolation=InterpolationMode.BILINEAR, antialias=False)\n",
       "      Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225], inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_inferencer.model.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰§è¡Œæ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11__DA1479053.png', '11__DA2951175.png', '11__DA2951215 (2).png', '11__DA2951215 (3).png', '11__DA2951215 (4).png']\n",
      "['11__DA1479053 (2).png', '11__DA2951175 (2).png', '11__DA2951175.png', '11__DA2951225.png', '13__DA1479053 (2).png']\n",
      "\n",
      "===> F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\normal\\11__DA1479053.png;\n",
      "0.6220808625221252 LabelName.ABNORMAL\n",
      "Prediction took 1.7649 seconds.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 26\u001b[0m\n\u001b[0;32m     21\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(abnormal_output_path)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# # æ¨¡å‹æµ‹è¯•\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mdraw_pic\u001b[49m\u001b[43m(\u001b[49m\u001b[43minferencer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_inferencer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal_png_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal_folder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal_ouput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)\n",
      "Cell \u001b[1;32mIn[44], line 75\u001b[0m, in \u001b[0;36mdraw_pic\u001b[1;34m(inferencer, torch_inferencer, transform, png_files, input_path, outpath)\u001b[0m\n\u001b[0;32m     72\u001b[0m axs[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# å…³é—­åæ ‡è½´\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# çƒ­å›¾\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m \u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheat_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m axs[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeat Map\u001b[39m\u001b[38;5;124m'\u001b[39m)         \u001b[38;5;66;03m#! çƒ­åŠ›å›¾æ˜¯anomaly_mapä¸åŸå§‹å›¾åƒçš„åŠ æƒç»“åˆ\u001b[39;00m\n\u001b[0;32m     77\u001b[0m axs[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# å…³é—­åæ ‡è½´\u001b[39;00m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\matplotlib\\__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[0;32m   1474\u001b[0m             ax,\n\u001b[0;32m   1475\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args),\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: sanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\matplotlib\\image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\matplotlib\\image.py:692\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    690\u001b[0m A \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39msafe_masked_invalid(A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    693\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted to float\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    }
   ],
   "source": [
    "# å¾…æµ‹è¯•å›¾åƒ\n",
    "test_folder_path = r\"/home/projects/myprojects/anomalib_projects/datasets/test_1122\"\n",
    "test_output_path = r\"/home/projects/myprojects/anomalib_projects/datasets/test_1122/{}_output\".format(configs[\"model_name\"])\n",
    "\n",
    "# test_png_files = [f for f in os.listdir(test_folder_path) if f.endswith('.png')]\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')][:5]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')][:5]\n",
    "\n",
    "# print(test_png_files)\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# è¾“å‡ºè·¯å¾„ç¡®è®¤\n",
    "# if os.path.exists(test_output_path):     shutil.rmtree(test_output_path)\n",
    "if os.path.exists(normal_ouput_path):    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): shutil.rmtree(abnormal_output_path)\n",
    "# os.makedirs(test_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# # æ¨¡å‹æµ‹è¯•\n",
    "# draw_pic(inferencer, torch_inferencer, train_transform, test_png_files, test_folder_path, test_output_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, torch_inferencer, train_transform, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
