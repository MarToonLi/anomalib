{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision.transforms.v2 import Resize\n",
    "from torchvision.transforms.v2.functional import to_pil_image\n",
    "\n",
    "from anomalib import TaskType\n",
    "import torch\n",
    "\n",
    "seed = 67\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.enabled = True\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "\n",
    "class ExtractBChannel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 可以在这里设置一些参数（如果需要的话），例如大小，填充值等\n",
    "        pass\n",
    "    \n",
    "    # RGB (N, 3, H, W) 的tensor类型\n",
    "    def forward(self, img):\n",
    "        \n",
    "        tmp_img = img.clone()\n",
    "        if len(img.shape) == 3: tmp_img = tmp_img.unsqueeze(0)\n",
    "        bs, channels, height, width = tmp_img.shape\n",
    "        \n",
    "        if channels == 1: tmp_img = tmp_img.repeat(1,3,1,1)\n",
    "        \n",
    "        b_channel = tmp_img[:, 2, :, :]     # 提取 B 通道（张量的第三个通道，索引为2）\n",
    "        b_channel[b_channel < 100/255] = 0\n",
    "        # b_channel[b_channel >= 100/255] = 1    # 不能添加\n",
    "        b_channel_3 = b_channel.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        out_img = b_channel_3\n",
    "        if len(img.shape) == 3: out_img = out_img.squeeze(0)\n",
    "        \n",
    "        #print(\"{} --> {} --> {} -- {};\".format(img.shape, tmp_img.shape, b_channel.shape, out_img.shape))\n",
    "        return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from anomalib.callbacks.checkpoint import ModelCheckpoint\n",
    "from anomalib.callbacks import GraphLogger\n",
    "from anomalib.loggers import AnomalibMLFlowLogger\n",
    "from torchvision.transforms.v2 import Resize, RandomHorizontalFlip, Compose, Normalize, ToDtype,RandomAffine,RandomPerspective, Grayscale, ToTensor, Transform, GaussianBlur\n",
    "from anomalib.data.image.folder import Folder, FolderDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read_image,会自动将数据类型转换为float32,但不会进行255归一化\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          # 0~1之间\n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),                # 如果resize的HW不一致，会引起fastflow模型报layernorm错误\n",
    "        # RandomHorizontalFlip(p=0.3),       # 无seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),  # onnx 不支持 grid_sampler.\n",
    "        RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_transform = Compose(\n",
    "    [\n",
    "        ExtractBChannel(),          # 0~1之间\n",
    "        # ToTensor(),\n",
    "        #ToDtype(torch.uint8, scale=True),\n",
    "        Resize((256, 256)),\n",
    "        #RandomHorizontalFlip(p=0.3),   # 无seed, 0.90 --> 0.95\n",
    "        #RandomAffine(degrees=(-5, 5), translate=(0.95, 0.95),scale=(0.95, 0.95), ),  # onnx 不支持 grid_sampler.\n",
    "        #RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "        #ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "        Normalize(mean=[0.406, 0.406, 0.406], std=[0.225, 0.225, 0.225]),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pic(inferencer, png_files, input_path, outpath):\n",
    "    from anomalib.data.utils import read_image\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for file_name in png_files:\n",
    "            \n",
    "        image = read_image(path=os.path.join(input_path, file_name))                     # HWC\n",
    "        CHW_image = read_image(path=os.path.join(input_path, file_name),as_tensor=True)  # CHW\n",
    "        \n",
    "\n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "        predictions = inferencer.predict(image=image)\n",
    "        filter_image = ExtractBChannel()(CHW_image)  # CHW -> CHW\n",
    "        filter_image = filter_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        train_image = train_transform(CHW_image)  # CHW -> CHW\n",
    "        train_image = train_image.permute(1,2,0)    # CHW -> HWC\n",
    "        \n",
    "        print(\"image: {}; filter_image: {}; train_image: {}; predictions.heat_map: {};\".format(image.shape, filter_image.shape, train_image.shape, predictions.heat_map.shape))\n",
    "        \n",
    "        \n",
    "        # 记录结束时间\n",
    "        end_time = time.time()\n",
    "\n",
    "        # 计算耗时\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Prediction took {elapsed_time:.4f} seconds.\")\n",
    "        print(predictions.pred_score, predictions.pred_label)\n",
    "        # 创建一个新的图形窗口\n",
    "        fig, axs = plt.subplots(1, 5, figsize=(18, 6))  # 创建一个1行3列的子图网格\n",
    "\n",
    "        # 原始图像\n",
    "        axs[0].imshow(image)\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[0].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        axs[1].imshow(filter_image.numpy())\n",
    "        axs[1].set_title('Filter Image')\n",
    "        axs[1].axis('off')  # 关闭坐标轴\n",
    "        \n",
    "        # 训练用图像\n",
    "        axs[2].imshow(train_image.numpy())\n",
    "        axs[2].set_title('Train Image')\n",
    "        axs[2].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 热图\n",
    "        axs[3].imshow(predictions.heat_map, cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('Heat Map')\n",
    "        axs[3].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 预测掩模\n",
    "        axs[4].imshow(predictions.pred_mask, cmap='gray', interpolation='nearest')\n",
    "        axs[4].set_title('Predicted Mask')\n",
    "        axs[4].axis('off')  # 关闭坐标轴\n",
    "\n",
    "\n",
    "        # 添加文本信息到图形的上方中间位置\n",
    "        fig_text_x = 0.1  # x坐标在图形宽度的中心位置\n",
    "        fig_text_y = 0.95  # y坐标稍微靠近图形的顶部，避免与子图重叠\n",
    "        fig.text(fig_text_x, fig_text_y,\n",
    "                f'Prediction Time: {elapsed_time:.4f} s\\n'\n",
    "                f'Predicted Class: {predictions.pred_label}\\n'\n",
    "                # f'Score: {predictions.pred_score:.4f}\\n'\n",
    "                f'Threshold: {predictions.pred_score:.4f}' if hasattr(predictions, 'pred_score') else '',\n",
    "                ha='left', va='center', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round\", fc=\"w\", ec=\"0.5\", alpha=0.5))  \n",
    "\n",
    "        # 显示整个图形\n",
    "        plt.tight_layout()  # 调整子图间的间距\n",
    "        #plt.show()\n",
    "        plt.savefig(os.path.join(outpath, file_name))\n",
    "        plt.close()\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def pplot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root: F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dataset_root\": r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\",\n",
    "    \"outputs_path\": r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5 - jing\\outputs\",\n",
    "    \"model_name\": \"Fastflow\",\n",
    "}\n",
    "\n",
    "dataset_root = configs[\"dataset_root\"]\n",
    "print(\"dataset_root: {}\".format(dataset_root))\n",
    "\n",
    "normal_folder_path = os.path.join(configs[\"dataset_root\"], \"normal\")\n",
    "abnormal_folder_path = os.path.join(configs[\"dataset_root\"], \"abnormal\")\n",
    "\n",
    "normal_ouput_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"normal_outputs\")\n",
    "abnormal_output_path = os.path.join(configs[\"outputs_path\"], configs[\"model_name\"] , \"abnormal_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom 数据集配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "folder_datamodule = Folder(\n",
    "    name=\"3-5\",\n",
    "    root=dataset_root,\n",
    "    normal_dir=\"normal\", abnormal_dir=\"abnormal\",\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    num_workers=0,          # in jupyter, need to be zero. and can be non-0 in python main.py;\n",
    "    # image_size=(256, 256),\n",
    "    train_batch_size = 16, eval_batch_size = 8,                     # 计算的时候会使用cuda，因此需要限制BS不适用默认值32；\n",
    "    train_transform=train_transform, eval_transform=eval_transform,\n",
    "    seed = seed,\n",
    ")\n",
    "\n",
    "folder_datamodule.setup()            #! 进行数据集分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> 检查训练集、验证集、测试集的数据量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image_path', 'label', 'image']) torch.Size([16, 3, 256, 306])\n"
     ]
    }
   ],
   "source": [
    "# Train images\n",
    "i, data = next(enumerate(folder_datamodule.train_dataloader()))\n",
    "print(data.keys(), data[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train images\n",
    "# i, data = next(enumerate(folder_datamodule.val_dataloader()))\n",
    "# print(data.keys(), data[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test images\n",
    "# i, data = next(enumerate(folder_datamodule.test_dataloader()))\n",
    "# print(data.keys(), data[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> 查看图像内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = to_pil_image(data[\"image\"][0].clone())\n",
    "# Image.fromarray((np.array(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前的数据:  tensor(0.) tensor(0.9725) tensor(0.0655)\n",
      "tensor(0.) tensor(0.9725) tensor(0.0457)\n",
      "tensor(0.) tensor(0.9362) tensor(0.0655)\n",
      "tensor(0.) tensor(0.9725) tensor(0.0655)\n",
      "tensor(0.) tensor(0.9725) tensor(0.0655)\n",
      "tensor(-1.8044) tensor(2.5180) tensor(-1.5135)\n"
     ]
    }
   ],
   "source": [
    "from anomalib.data.utils import read_image\n",
    "test_image = read_image(r\"F:\\Projects\\anomalib\\notebooks\\datasets\\3-5\\normal\\1__DA2951175 (2).png\", as_tensor=True)\n",
    "print(\"处理前的数据: \",test_image.min(), test_image.max(), test_image.mean())\n",
    "\n",
    "for trans in train_transform.transforms:\n",
    "    tmp_image = trans(test_image)\n",
    "    print(tmp_image.min(), tmp_image.max(), tmp_image.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 256, 306])\n",
      "tensor(-1.8044) tensor(1.6501)\n",
      "tensor(-1.8044) tensor(1.9774)\n",
      "tensor(-1.8044) tensor(2.4463)\n",
      "tensor(-1.8044) tensor(2.4919)\n",
      "tensor(-1.8044) tensor(2.4152)\n",
      "tensor(-1.8044) tensor(2.2764)\n",
      "tensor(-1.8044) tensor(1.7671)\n",
      "tensor(-1.8044) tensor(2.4152)\n",
      "tensor(-1.8044) tensor(2.2622)\n",
      "tensor(-1.8044) tensor(2.1572)\n",
      "tensor(-1.8044) tensor(2.4533)\n",
      "tensor(-1.8044) tensor(1.7715)\n",
      "tensor(-1.8044) tensor(2.0561)\n",
      "tensor(-1.8044) tensor(2.1324)\n",
      "tensor(-1.8044) tensor(1.6901)\n",
      "tensor(-1.8044) tensor(2.1258)\n"
     ]
    }
   ],
   "source": [
    "total_images = []\n",
    "print(data[\"image\"].shape)\n",
    "for i in range(data[\"image\"].shape[0]):\n",
    "    test_img = data[\"image\"][i]\n",
    "    total_images.append(test_img)\n",
    "    print(test_img.min(), test_img.max())\n",
    "\n",
    "pplot(total_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型选择和优化器配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomalib.models.components.base.anomaly_module:Initializing Fastflow model.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO:anomalib.data.base.datamodule:No normal test images found. Sampling from training set using a split ratio of 0.20\n",
      "WARNING:anomalib.metrics.f1_score:F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type                     | Params | Mode \n",
      "---------------------------------------------------------------------------\n",
      "0 | loss                  | FastflowLoss             | 0      | train\n",
      "1 | _transform            | Compose                  | 0      | train\n",
      "2 | normalization_metrics | MetricCollection         | 0      | train\n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train\n",
      "5 | image_metrics         | AnomalibMetricCollection | 0      | train\n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0      | train\n",
      "7 | model                 | FastflowModel            | 7.8 M  | train\n",
      "---------------------------------------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.366    Total estimated model params size (MB)\n",
      "261       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56a70d3990943ad9436b273986a7716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[64, 64, 76], expected input with shape [*, 64, 64, 76], but got input of size[16, 64, 64, 77]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 31\u001b[0m\n\u001b[0;32m     23\u001b[0m     model \u001b[38;5;241m=\u001b[39m Fastflow()\n\u001b[0;32m     24\u001b[0m     engine \u001b[38;5;241m=\u001b[39m Engine(\n\u001b[0;32m     25\u001b[0m     task\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mCLASSIFICATION,\n\u001b[0;32m     26\u001b[0m     image_metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1Score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUROC\u001b[39m\u001b[38;5;124m\"\u001b[39m], pixel_metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1Score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUROC\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,                            \u001b[38;5;66;03m#! 希望赋值给Lightning Trainer的参数必须全部放在已标明参数的最后面\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     )\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder_datamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     model \u001b[38;5;241m=\u001b[39m Patchcore()\n",
      "File \u001b[1;32mF:\\Projects\\anomalib\\src\\anomalib\\engine\\engine.py:871\u001b[0m, in \u001b[0;36mEngine.train\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, test_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mvalidate(model, val_dataloaders, \u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 871\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtest(model, test_dataloaders, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         closure()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\core\\module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1277\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1281\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \n\u001b[0;32m   1305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1306\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\optim\\adam.py:202\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 202\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    205\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     optimizer: Steppable,\n\u001b[0;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\Projects\\anomalib\\src\\anomalib\\models\\image\\fastflow\\lightning_model.py:84\u001b[0m, in \u001b[0;36mFastflow.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform the training step input and return the loss.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    STEP_OUTPUT: Dictionary containing the loss value.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m args, kwargs  \u001b[38;5;66;03m# These variables are not used.\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m hidden_variables, jacobians \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(hidden_variables, jacobians)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem(), on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mF:\\Projects\\anomalib\\src\\anomalib\\models\\image\\fastflow\\torch_model.py:192\u001b[0m, in \u001b[0;36mFastflowModel.forward\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m    190\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cait_features(input_tensor)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cnn_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Compute the hidden variable f: X -> Z and log-likelihood of the jacobian\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# (See Section 3.3 in the paper.)\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# NOTE: output variable has z, and jacobian tuple for each fast-flow blocks.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m hidden_variables: \u001b[38;5;28mlist\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mF:\\Projects\\anomalib\\src\\anomalib\\models\\image\\fastflow\\torch_model.py:221\u001b[0m, in \u001b[0;36mFastflowModel._get_cnn_features\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get CNN-based features.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    list[torch.Tensor]: List of features.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(input_tensor)\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms[i](feature) \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features)]\n",
      "File \u001b[1;32mF:\\Projects\\anomalib\\src\\anomalib\\models\\image\\fastflow\\torch_model.py:221\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get CNN-based features.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    list[torch.Tensor]: List of features.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(input_tensor)\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features)]\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Appsetup\\Anaconda\\envs\\anoma\\lib\\site-packages\\torch\\nn\\functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2892\u001b[0m         layer_norm,\n\u001b[0;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2899\u001b[0m     )\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given normalized_shape=[64, 64, 76], expected input with shape [*, 64, 64, 76], but got input of size[16, 64, 64, 77]"
     ]
    }
   ],
   "source": [
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Fastflow\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(mode=\"max\", monitor=\"image_F1Score\")\n",
    "early_stopping = EarlyStopping(monitor=\"image_F1Score\", mode=\"max\", patience=5)\n",
    "\n",
    "graph_logger = GraphLogger()\n",
    "callbacks = [\n",
    "    model_checkpoint,\n",
    "    #early_stopping,\n",
    "    graph_logger,\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if configs[\"model_name\"] == \"Patchcore\":\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"], callbacks= callbacks)\n",
    "    engine.train(datamodule=folder_datamodule, model=model)\n",
    "elif configs[\"model_name\"] == \"Fastflow\":\n",
    "    model = Fastflow()\n",
    "    engine = Engine(\n",
    "    task=TaskType.CLASSIFICATION,\n",
    "    image_metrics=[\"F1Score\",\"AUROC\"], pixel_metrics=[\"F1Score\",\"AUROC\"],\n",
    "    accelerator=\"auto\",  # \\<\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">,\n",
    "    logger=False, callbacks= callbacks,\n",
    "    max_epochs=10,                            #! 希望赋值给Lightning Trainer的参数必须全部放在已标明参数的最后面\n",
    "    )\n",
    "    engine.train(datamodule=folder_datamodule, model=model)\n",
    "else:\n",
    "    model = Patchcore()\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION)\n",
    "    engine.train(datamodule=folder_datamodule, model=model)\n",
    "    \n",
    "\n",
    "print(engine.trainer.default_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opset_versissssson: 14;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Exported model to F:\\Projects\\anomalib\\notebooks\\100_datamodules\\results\\Fastflow\\3-5\\latest\\weights\\onnx\\model.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save to F:\\Projects\\anomalib\\notebooks\\100_datamodules\\results\\Fastflow\\3-5\\latest).\n"
     ]
    }
   ],
   "source": [
    "# from anomalib.deploy import ExportType\n",
    "# engine.export(model=model, export_type=ExportType.OPENVINO)  # torch.onnx.export op=16\n",
    "# print(f\"Model save to {engine.trainer.default_root_dir}).\") \n",
    "\n",
    "from anomalib.deploy import ExportType\n",
    "engine.export(model=model, export_type=ExportType.ONNX)  # torch.onnx.export op=16\n",
    "print(f\"Model save to {engine.trainer.default_root_dir}).\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "# model_output_path=Path(engine.trainer.default_root_dir)\n",
    "# openvino_model_path = model_output_path / \"weights\" / \"openvino\" / \"model.bin\"\n",
    "# metadata_path = model_output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n",
    "# print(openvino_model_path.exists(), metadata_path.exists())\n",
    "\n",
    "model_output_path=Path(engine.trainer.default_root_dir)\n",
    "openvino_model_path = model_output_path / \"weights\" / \"onnx\" / \"model.onnx\"\n",
    "metadata_path = model_output_path / \"weights\" / \"onnx\" / \"metadata.json\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.deploy import OpenVINOInferencer\n",
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,    # Path to the OpenVINO IR model.\n",
    "    metadata=metadata_path,      # Path to the metadata file.\n",
    "    device=\"AUTO\",               # We would like to run it on an Intel CPU.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11__DA1479053.png', '11__DA2951175.png', '11__DA2951215 (2).png', '11__DA2951215 (3).png', '11__DA2951215 (4).png', '11__DA2951215.png', '11__DA2951225 (2).png', '11__DA2951225.png', '13__DA1479053.png', '13__DA2951175.png', '13__DA2951225.png', '15__DA1479053.png', '15__DA2951175.png', '15__DA2951215 (3).png', '15__DA2951225.png', '17__DA1479053 (2).png', '17__DA1479053.png', '17__DA2951175.png', '17__DA2951215 (2).png', '17__DA2951215 (3).png', '17__DA2951215.png', '17__DA2951225.png', '19__DA1479053 (2).png', '19__DA1479053.png', '19__DA2951175 (2).png', '19__DA2951175 (3).png', '19__DA2951175.png', '19__DA2951215 (2).png', '19__DA2951215 (3).png', '19__DA2951215.png', '19__DA2951225.png', '1__DA2951175 (2).png', '1__DA2951175 (3).png', '1__DA2951175.png', '1__DA2951215 (2).png', '1__DA2951225.png', '3__DA2951175 (2).png', '3__DA2951175 (3).png', '3__DA2951175.png', '3__DA2951215 (2).png', '3__DA2951215.png', '3__DA2951225.png', '53__DA1479053.png', '53__DA2951175.png', '53__DA2951215.png', '53__DA2951225.png', '55__DA1479053.png', '55__DA2951175.png', '55__DA2951215.png', '55__DA2951225.png', '57__DA1479053.png', '57__DA2951215.png', '59__DA1479053.png', '59__DA2951175.png', '59__DA2951215.png', '59__DA2951225.png', '5__DA1479053 (2).png', '5__DA1479053 (3).png', '5__DA1479053.png', '5__DA2951175 (2).png', '5__DA2951175 (3).png', '5__DA2951175.png', '5__DA2951215 (2).png', '5__DA2951215 (3).png', '5__DA2951215.png', '5__DA2951225.png', '61__DA1479053.png', '61__DA2951175.png', '61__DA2951225.png', '63__DA2951175.png', '63__DA2951215.png', '63__DA2951225.png', '65__DA2951175.png', '65__DA2951215.png', '65__DA2951225.png', '67__DA1479053.png', '67__DA2951175.png', '67__DA2951225.png', '69__DA1479053.png', '69__DA2951215 (2).png', '69__DA2951215.png', '71__DA1479053.png', '71__DA2951215.png', '71__DA2951225.png', '73__DA1479053.png', '73__DA2951225.png', '7__DA1479053 (2).png', '7__DA1479053 (3).png', '7__DA1479053.png', '7__DA2951215 (2).png', '7__DA2951215 (3).png', '7__DA2951215.png', '7__DA2951225 (2).png', '7__DA2951225.png', '9__DA1479053 (2).png', '9__DA1479053 (3).png', '9__DA1479053.png', '9__DA2951175 (2).png', '9__DA2951175 (3).png', '9__DA2951175.png', '9__DA2951215 (2).png', '9__DA2951215.png']\n",
      "['11__DA1479053 (2).png', '11__DA2951175 (2).png', '11__DA2951175.png', '11__DA2951225.png', '13__DA1479053 (2).png', '13__DA1479053 (3).png', '13__DA1479053.png', '13__DA2951175 (2).png', '13__DA2951175.png', '13__DA2951225.png', '15__DA1479053 (2).png', '15__DA1479053 (3).png', '15__DA1479053.png', '15__DA2951215.png', '15__DA2951225 (2).png', '15__DA2951225.png', '17__DA1479053 (2).png', '17__DA1479053.png', '17__DA2951175 (2).png', '17__DA2951175.png', '17__DA2951225 (2).png', '17__DA2951225.png', '19__DA2951225 (2).png', '19__DA2951225.png', '1__DA1479053 (2).png', '1__DA1479053 (3).png', '1__DA1479053.png', '1__DA2951225 (2).png', '1__DA2951225.png', '3__DA1479053 (4).png', '3__DA2951225.png', '57__DA2951225.png', '5__DA2951225 (2).png', '5__DA2951225.png', '69__DA1479053.png', '69__DA2951175 (2).png', '69__DA2951175.png', '69__DA2951225 (2).png', '71__DA1479053 (2).png', '71__DA2951175 (2).png', '71__DA2951215.png', '71__DA2951225.png', '73__DA2951175.png', '73__DA2951215.png', '75__DA1479053.png', '75__DA2951175.png', '75__DA2951215.png', '75__DA2951225.png', '7__DA2951175 (2).png', '7__DA2951175.png', '9__DA1479053.png', '9__DA2951175.png', '9__DA2951215 (3).png', '9__DA2951215.png']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8991438].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3400 seconds.\n",
      "0.6780486021857608 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.4402523].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2961 seconds.\n",
      "0.38345081955164834 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.879812].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3488 seconds.\n",
      "0.394619484267805 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.1615112].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3380 seconds.\n",
      "0.5618409050499321 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.0553207].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3001 seconds.\n",
      "0.3747610244323575 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.1615112].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3373 seconds.\n",
      "0.5618409050499321 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6129792].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3001 seconds.\n",
      "0.8374240717509531 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8147168].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2991 seconds.\n",
      "0.6535684561773256 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.346743].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3246 seconds.\n",
      "0.6781048975127653 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.4230688].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2981 seconds.\n",
      "0.8201988932264483 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8076477].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3538 seconds.\n",
      "0.7987100761847473 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.088412].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3112 seconds.\n",
      "0.6194800144019933 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.5065491].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3081 seconds.\n",
      "0.5830086902736882 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.1463242].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3081 seconds.\n",
      "0.6502307800755758 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8892226].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3179 seconds.\n",
      "0.8340406679044021 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.324986].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3266 seconds.\n",
      "0.6852108445075504 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.9789428].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3256 seconds.\n",
      "0.6034808512138631 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.4405787].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3158 seconds.\n",
      "0.4423144401664619 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6084601].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3379 seconds.\n",
      "0.5104054547866923 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6884772].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3081 seconds.\n",
      "0.7090704136461279 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6084601].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2943 seconds.\n",
      "0.5104054547866923 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8655376].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3145 seconds.\n",
      "0.5493947153591945 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.1534925].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3164 seconds.\n",
      "0.7644973627680541 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.13815].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3243 seconds.\n",
      "0.8384186941505845 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.4537294].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2990 seconds.\n",
      "0.4747753961488195 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.422894].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3053 seconds.\n",
      "0.3788864053768483 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.4537294].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3325 seconds.\n",
      "0.4747753961488195 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.1119082].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3007 seconds.\n",
      "0.41067267690579434 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.1119082].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3265 seconds.\n",
      "0.41067267690579434 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8547895].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3300 seconds.\n",
      "0.7288820323316252 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.0540655].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3160 seconds.\n",
      "0.7037619713639032 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.341703].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3027 seconds.\n",
      "0.4308624236066113 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.274687].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3047 seconds.\n",
      "0.6000296468537686 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.3444371].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3216 seconds.\n",
      "0.4308624236066113 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6167672].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3197 seconds.\n",
      "0.5755365306520194 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.7666352].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3380 seconds.\n",
      "0.5199293661633538 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.197791].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3387 seconds.\n",
      "0.739574784713592 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.183551].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3281 seconds.\n",
      "0.4804739990353419 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.2032826].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3384 seconds.\n",
      "0.739574784713592 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.7698634].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3205 seconds.\n",
      "0.6561765979720107 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.7698634].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3250 seconds.\n",
      "0.6561765979720107 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.583858].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3092 seconds.\n",
      "0.5702498095369108 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.4229014].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2973 seconds.\n",
      "0.6783226561197899 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.490523].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2861 seconds.\n",
      "0.4476941794450662 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.195283].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2898 seconds.\n",
      "0.43673725704144517 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.9460317].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3339 seconds.\n",
      "0.5458280293983886 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.1428177].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3346 seconds.\n",
      "0.5735346672610687 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.5582197].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2955 seconds.\n",
      "0.29460382334538104 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.076696].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3299 seconds.\n",
      "0.3752349052904862 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.1008632].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3070 seconds.\n",
      "0.7295621470768159 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.229113].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3068 seconds.\n",
      "0.718268585649741 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.5991274].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3040 seconds.\n",
      "0.6268200499459876 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.9388158].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3115 seconds.\n",
      "0.7457150383091946 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.222027].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3254 seconds.\n",
      "0.4482902614016472 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.296798].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3064 seconds.\n",
      "0.7787795184825499 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.7604783].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3424 seconds.\n",
      "0.6727980960381285 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8448458].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3146 seconds.\n",
      "0.8316588208228836 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.2253582].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3307 seconds.\n",
      "0.8837333461081924 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8464215].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3127 seconds.\n",
      "0.8316588208228836 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.3565743].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3064 seconds.\n",
      "0.4152988948330725 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.2869349].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3351 seconds.\n",
      "0.4383029390819418 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.3565743].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3221 seconds.\n",
      "0.4152988948330725 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.596603].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3537 seconds.\n",
      "0.7142768133231232 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.596603].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3338 seconds.\n",
      "0.7142768133231232 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6598923].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2946 seconds.\n",
      "0.5214271890900359 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.568462].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2886 seconds.\n",
      "0.6114937741362918 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.3738167].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2996 seconds.\n",
      "0.5175457024734345 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.496421].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2910 seconds.\n",
      "0.4132870303292051 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6324077].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3367 seconds.\n",
      "0.7782690319884717 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.47182].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3206 seconds.\n",
      "0.40810926665130365 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.9123034].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3440 seconds.\n",
      "0.607304128227941 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.3581505].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3016 seconds.\n",
      "0.772833700586115 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.253738].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3036 seconds.\n",
      "0.6294880812176316 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.0792959].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3539 seconds.\n",
      "0.4337716535936787 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.9612495].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3006 seconds.\n",
      "0.6251201342180607 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.161572].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3420 seconds.\n",
      "0.715125462447701 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.4837623].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2976 seconds.\n",
      "0.46975893948354897 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.5410023].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3412 seconds.\n",
      "0.9087000697001391 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.2520044].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3070 seconds.\n",
      "0.5781294754430926 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6273768].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3048 seconds.\n",
      "0.4417987577816887 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6548882].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3121 seconds.\n",
      "0.3199318762082949 LabelName.NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.4383388].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3124 seconds.\n",
      "0.8325915808697657 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8976568].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3234 seconds.\n",
      "0.712986044687297 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6502888].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3424 seconds.\n",
      "0.693004289881672 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.268084].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3299 seconds.\n",
      "0.5192579242582708 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8071476].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3337 seconds.\n",
      "0.760275076508758 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.7484471].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3040 seconds.\n",
      "0.8443747866278675 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.9120404].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3032 seconds.\n",
      "0.8443747866278675 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.885373].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3183 seconds.\n",
      "0.7087650281005391 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6680857].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3050 seconds.\n",
      "0.7577996252610337 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.065564].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3285 seconds.\n",
      "0.7028182725997683 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.6738665].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3285 seconds.\n",
      "0.7577996252610337 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.0791748].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3395 seconds.\n",
      "0.6848767838960205 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.8254163].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3123 seconds.\n",
      "0.7098365145222947 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.058779].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3084 seconds.\n",
      "0.8499610723283123 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.058779].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3174 seconds.\n",
      "0.8499610723283123 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.9350843].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3070 seconds.\n",
      "0.8153229600110286 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.3145814].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3241 seconds.\n",
      "0.7205302826422921 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.5719643].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3062 seconds.\n",
      "0.5625756352476918 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.3145814].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3411 seconds.\n",
      "0.7205302826422921 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.3205323].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3086 seconds.\n",
      "0.8145323251547376 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..1.5929528].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2975 seconds.\n",
      "0.41440848324869745 LabelName.NORMAL\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.636978].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3159 seconds.\n",
      "0.9709021965872081 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.637405].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3387 seconds.\n",
      "1.0 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6337333].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3317 seconds.\n",
      "0.8507061161737676 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6353848].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3270 seconds.\n",
      "0.896558054482723 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6201212].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3374 seconds.\n",
      "0.7856617296426687 LabelName.ABNORMAL\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3507 seconds.\n",
      "0.7856617296426687 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.619905].\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6024988].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3026 seconds.\n",
      "0.7856617296426687 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6357944].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2938 seconds.\n",
      "0.8553799175210952 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.635619].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3003 seconds.\n",
      "0.8553799175210952 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6321185].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3197 seconds.\n",
      "0.8198120533041738 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6372185].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3249 seconds.\n",
      "0.8521617859721793 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6372185].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3231 seconds.\n",
      "0.8521617859721793 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6361682].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3044 seconds.\n",
      "0.8521617859721793 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6276493].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3436 seconds.\n",
      "0.6806275219786421 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.636472].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3060 seconds.\n",
      "0.8745280053380939 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6354].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3485 seconds.\n",
      "0.8745280053380939 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6370008].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3062 seconds.\n",
      "0.8669264367846403 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6365736].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3347 seconds.\n",
      "0.8669264367846403 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6359649].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3068 seconds.\n",
      "0.8773343137158671 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6359649].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3116 seconds.\n",
      "0.8773343137158671 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6380439].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3302 seconds.\n",
      "0.7711018281770711 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.636633].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3085 seconds.\n",
      "0.7711018281770711 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6363316].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3337 seconds.\n",
      "0.8166055636757625 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6363316].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3124 seconds.\n",
      "0.8166055636757625 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6369128].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3429 seconds.\n",
      "0.8895291473108751 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6363802].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3272 seconds.\n",
      "0.8895291473108751 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6362271].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3299 seconds.\n",
      "0.8895291473108751 LabelName.ABNORMAL\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3224 seconds.\n",
      "0.8135704212397463 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.636111].\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6366775].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2948 seconds.\n",
      "0.8135704212397463 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.1090229].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3065 seconds.\n",
      "0.7412476271163182 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.5492632].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3061 seconds.\n",
      "0.7534492192759146 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6226377].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3290 seconds.\n",
      "0.7391445805907796 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6371517].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3241 seconds.\n",
      "0.848127626116207 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6371517].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3335 seconds.\n",
      "0.848127626116207 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.63801].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3045 seconds.\n",
      "0.9745661687645808 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6360104].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3215 seconds.\n",
      "0.8932840964089706 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.5953462].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3028 seconds.\n",
      "0.8370145535238027 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.637395].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3006 seconds.\n",
      "0.9176672003686348 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6372406].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3271 seconds.\n",
      "0.898931521725489 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6360528].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3345 seconds.\n",
      "1.0 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6374123].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3299 seconds.\n",
      "1.0 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.2457993].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2993 seconds.\n",
      "0.8325217879470084 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.577903].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3312 seconds.\n",
      "0.8765123472480165 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6370351].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3036 seconds.\n",
      "1.0 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6352396].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3126 seconds.\n",
      "0.7722261720432659 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6378891].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3439 seconds.\n",
      "1.0 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6364326].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3422 seconds.\n",
      "0.9630444569490912 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.636198].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3270 seconds.\n",
      "0.8754052123280711 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6360252].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3343 seconds.\n",
      "0.9241986498399895 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6360252].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3157 seconds.\n",
      "0.9241986498399895 LabelName.ABNORMAL\n",
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3468 seconds.\n",
      "0.8833104279524073 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6366987].\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6374652].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2918 seconds.\n",
      "0.8656593817251441 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6235116].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.2974 seconds.\n",
      "0.8986751650733009 LabelName.ABNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8044444..2.6240547].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (2048, 2448, 3); filter_image: torch.Size([2048, 2448, 3]); train_image: torch.Size([256, 256, 3]); predictions.heat_map: (2048, 2448, 3);\n",
      "Prediction took 0.3007 seconds.\n",
      "0.8986751650733009 LabelName.ABNORMAL\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 待测试图像\n",
    "normal_png_files = [f for f in os.listdir(normal_folder_path) if f.endswith('.png')]\n",
    "abnormal_png_files = [f for f in os.listdir(abnormal_folder_path) if f.endswith('.png')]\n",
    "print(normal_png_files)\n",
    "print(abnormal_png_files)\n",
    "\n",
    "\n",
    "import shutil\n",
    "# 输出路径确认\n",
    "if os.path.exists(normal_ouput_path): \n",
    "    shutil.rmtree(normal_ouput_path)\n",
    "if os.path.exists(abnormal_output_path): \n",
    "    shutil.rmtree(abnormal_output_path)\n",
    "os.makedirs(normal_ouput_path)\n",
    "os.makedirs(abnormal_output_path)\n",
    "\n",
    "\n",
    "# 模型测试\n",
    "draw_pic(inferencer, normal_png_files, normal_folder_path, normal_ouput_path)\n",
    "draw_pic(inferencer, abnormal_png_files, abnormal_folder_path, abnormal_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Folder` data module offers much more flexibility cater all different sorts of needs. Please refer to the documentation for more details.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some transforms that will be applied to the images using torchvision. Let's add a transform that resizes the \n",
    "input image to 256x256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256, 256)\n",
    "transform = Resize(image_size, antialias=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m folder_dataset_classification_train \u001b[38;5;241m=\u001b[39m FolderDataset(\n\u001b[0;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3-5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m----> 3\u001b[0m     normal_dir\u001b[38;5;241m=\u001b[39m\u001b[43mdataset_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m     abnormal_dir\u001b[38;5;241m=\u001b[39mdataset_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[0;32m      7\u001b[0m     task\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mCLASSIFICATION,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m folder_dataset_classification_train\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "folder_dataset_classification_train = FolderDataset(\n",
    "    name=\"3-5\",\n",
    "    normal_dir=dataset_root / \"normal\",\n",
    "    abnormal_dir=dataset_root / \"abnormal\",\n",
    "    split=\"train\",\n",
    "    transform=transform,\n",
    "    task=TaskType.CLASSIFICATION,\n",
    ")\n",
    "folder_dataset_classification_train.samples.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first sample in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = folder_dataset_classification_train[0]\n",
    "print(data.keys(), data[\"image\"].shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data[\"image\"][2])   # RGB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, when we choose `classification` task and `train` split, the dataset only returns `image`. This is mainly because training only requires normal images and no labels. Now let's try `test` split for the `classification` task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Classification Test Set\n",
    "folder_dataset_classification_test = FolderDataset(\n",
    "    name=\"3-5\",\n",
    "    normal_dir=dataset_root / \"normal\",\n",
    "    abnormal_dir=dataset_root / \"abnormal\",\n",
    "    split=\"test\",\n",
    "    transform=transform,\n",
    "    task=TaskType.CLASSIFICATION,\n",
    ")\n",
    "folder_dataset_classification_test.samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = folder_dataset_classification_test[0]\n",
    "print(data.keys(), data[\"image\"].shape, data[\"image_path\"], data[\"label\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
